"""Multi-round fine-tuning pipeline that orchestrates LLaMA-Factory training."""

from __future__ import annotations

import copy
import json
import math
import logging
import os
import random
import re
import subprocess
import gc
import time
import sys
import subprocess
import tempfile
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass  # type: ignore[reportMissingImports]
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import numpy as np
import torch
import yaml  # type: ignore[reportMissingImports]
from PIL import Image
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from transformers import ViTImageProcessor, ViTModel

from ..config import AdaptiveConfig
from ..processors.gemini_generator import GeminiHardProblemGenerator

logger = logging.getLogger(__name__)


@dataclass
class ProblemRecord:
    """Canonical representation of a WeMath problem."""

    pid: str
    category_id: int
    category_name: str
    question: str
    answer: str
    image_path: str
    source: str = "original"
    last_prediction: Optional[str] = None
    image_code: Optional[str] = None
    idx: Optional[int] = None
    model_answer: Optional[str] = None  # COT answer generated by LLM


class MultiRoundFineTuner:
    """Coordinate multi-round fine-tuning with adaptive data refresh."""

    def __init__(self, config: AdaptiveConfig) -> None:
        self.config = config
        self.random_seed = config.random_seed
        self.rng = random.Random(self.random_seed)

        self.problem_map: Dict[str, ProblemRecord] = {}
        self.available_problem_ids: List[str] = []
        self.prev_wrong_ids: List[str] = []
        self.generated_bank: List[ProblemRecord] = []
        self.generated_counter: int = 0
        self.history: List[Dict[str, object]] = []
        self.current_model_path: Optional[str] = config.model_path
        
        # 用于分配新的唯一ID（从原始数据最大ID+1开始）
        self.next_generated_id: int = 5844  # 原始数据最大ID是5843

        self.gemini_generator = GeminiHardProblemGenerator(
            api_key=config.gemini_api_key,
            model=config.gemini_model,
            base_url=config.gemini_base_url,
            max_tokens=config.gemini_max_tokens,
            max_output_tokens=config.gemini_max_output_tokens,
        )

        self._load_problems()

    # ------------------------------------------------------------------
    # Lifecycle
    # ------------------------------------------------------------------
    def run(self) -> None:
        start_round = self.config.start_round
        logger.info("Starting multi-round fine-tuning pipeline – rounds=%s, starting from round %s", 
                    self.config.max_rounds, start_round)

        for round_index in range(start_round, self.config.max_rounds + 1):
            round_dir = self.config.get_round_dir(round_index)
            dataset_dir = round_dir / "dataset"
            dataset_dir.mkdir(parents=True, exist_ok=True)

            logger.info("\n%s\n[Round %s] Preparing datasets\n%s", "=" * 70, round_index, "=" * 70)
            train_records, eval_records = self._prepare_round_datasets(round_index)

            # 写入数据文件（第一轮包含处理后的原始数据，后续轮包含上一轮处理好的生成题目）
            train_file, eval_file = self._write_dataset_files(dataset_dir, train_records, eval_records)
            self._write_dataset_info(dataset_dir, train_file.name, eval_file.name)

            train_yaml = self._write_train_yaml(round_index, dataset_dir)
            eval_yaml = self._write_eval_yaml(round_index, dataset_dir)

            # ------------------------------------------------------------------
            # Training
            # ------------------------------------------------------------------
            logger.info("[Round %s] Launching training via LLaMA-Factory", round_index)
            success = self._run_llamafactory_command(
                self.config.train_command_prefix + ["llamafactory-cli", "train", str(train_yaml)]
            )
            if not success:
                logger.error("Training failed for round %s – aborting multi-round pipeline.", round_index)
                break

            round_model_dir = self._get_round_model_dir(round_index)
            self.current_model_path = str(round_model_dir)

            # ------------------------------------------------------------------
            # Evaluation (使用 Apptainer + vLLM，解决兼容性问题)
            # ------------------------------------------------------------------
            logger.info("[Round %s] Running evaluation with vLLM (Apptainer)", round_index)
            
            # 等待训练进程释放 GPU 内存并清理 GPU 缓存
            logger.info("Waiting for training process to release GPU memory...")
            time.sleep(15)  # 增加等待时间到 15 秒
            torch.cuda.empty_cache()
            logger.info("Cleared CUDA cache")
            
            # 使用 vLLM 进行评估（强制使用 Apptainer 容器）
            eval_output_dir = round_model_dir / "eval"
            eval_output_dir.mkdir(parents=True, exist_ok=True)
            predictions_path = eval_output_dir / "generated_predictions.json"
            
            # 准备评估数据集文件路径（使用实际写入的文件名）
            eval_dataset_file = dataset_dir / "eval.json"
            if not eval_dataset_file.exists():
                logger.error("Evaluation dataset not found: %s", eval_dataset_file)
                logger.error("Evaluation failed for round %s – aborting pipeline.", round_index)
                break
            
            # 构建 vLLM 评估命令（强制使用 Apptainer）
            llm_generator_script = Path(__file__).parent.parent / "processors" / "llmasgenerator.py"
            
            # 获取 Apptainer 镜像路径（从环境变量或配置）
            apptainer_image = os.getenv("APPTAINER_IMAGE", self.config.apptainer_image if hasattr(self.config, 'apptainer_image') else "")
            
            if not apptainer_image:
                logger.error("APPTAINER_IMAGE not set. Please set it in environment or config.")
                logger.error("Evaluation failed for round %s – aborting pipeline.", round_index)
                break
            
            # 使用 Apptainer 容器（参考质量检测的配置）
            vllm_cmd = [
                "apptainer", "exec", "--nv",
                "--cleanenv",
                "--bind", "/share:/share,/mnt:/mnt",
                "--env", "TRITON_CACHE_DIR=/tmp/triton_cache",
                "--env", "PATH=/opt/py312/bin:/usr/local/cuda/bin:/usr/bin:/bin",
                "--env", "CC=/usr/bin/gcc",
                "--env", "CXX=/usr/bin/g++",
                "--env", "LD_LIBRARY_PATH=/usr/local/cuda/lib64:/.singularity.d/libs",
                "--env", "CUDA_HOME=/usr/local/cuda",
                "--env", f"PYTHONPATH={Path(__file__).parent.parent.parent}",
                apptainer_image,
                "python", str(llm_generator_script),
                "--model-path", str(round_model_dir),
                "--input", str(eval_dataset_file),
                "--output", str(predictions_path),
                "--question-key", "problem",
                "--answer-key", "predict",  # 输出到 predict 字段（与 Judge 期望一致）
                "--image-key", "image_path",
                "--temperature", str(self.config.eval_temperature),
                "--top-p", str(self.config.eval_top_p),
                "--max-tokens", str(self.config.eval_max_tokens),
                "--tensor-parallel-size", str(self.config.eval_tensor_parallel_size),
            ]
            
            logger.info("Running vLLM evaluation: %s", " ".join(map(str, vllm_cmd)))
            
            try:
                result = subprocess.run(
                    vllm_cmd,
                    check=True,
                    stdout=None,
                    stderr=None,
                    text=True,
                )
                logger.info("✅ vLLM evaluation completed successfully")
            except subprocess.CalledProcessError as exc:
                logger.error("❌ vLLM evaluation failed with exit code %d", exc.returncode)
                logger.error("Evaluation failed for round %s – aborting pipeline.", round_index)
                break
            
            # 验证输出文件
            if not predictions_path.exists():
                logger.error("Prediction file not found: %s", predictions_path)
                logger.error("Evaluation failed for round %s – aborting pipeline.", round_index)
                break

            # 评估完成后，强制释放 GPU 显存，为 LLM Judge 腾出空间
            logger.info("Evaluation complete.")
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            gc.collect()
            logger.info("GPU memory released. Waiting 20 seconds for GPU to fully release memory...")  
            time.sleep(20)  # 增加等待时间到 30 秒
            logger.info("Ready to start LLM Judge.")

            # LLM Judge
            wrong_records, category_stats = self._analyse_predictions(predictions_path, eval_records)

            # 生成新题
            hard_generated = self._generate_hard_problems(category_stats, wrong_records, round_index)
            refresh_generated = self._generate_refresh_problems(category_stats, wrong_records, eval_records, round_index)
            all_generated = hard_generated + refresh_generated
            
            # 为新生成的题目生成图片和COT答案
            if all_generated:
                
                # 1. 生成图片（会更新 record.image_path）
                logger.info("Generating images for %d newly generated problems.", len(all_generated))
                failed_pids = self._materialise_problem_images(dataset_dir, all_generated)
                
                # 2. 过滤掉图片生成失败的问题
                if failed_pids:
                    failed_set = set(failed_pids)
                    all_generated = [r for r in all_generated if r.pid not in failed_set]
                    logger.info("Filtered out %d problems with failed image generation", len(failed_pids))
                    logger.info("Remaining: %d generated problems", len(all_generated))
                
                # 3. 生成 COT 答案（model_answer）
                logger.info("Generating COT answers.")
                self._generate_model_answers(dataset_dir, all_generated)
                
                # 4. 新题质量检测：使用当前轮训练的模型测试新题
                if self.config.enable_quality_check and all_generated:
                    logger.info("\n%s\n[Round %s] Quality check for newly generated problems\n%s", 
                               "=" * 70, round_index, "=" * 70)
                    logger.info("Testing newly generated problems with current model.")
                    
                    qualified_problems = self._quality_check_generated_problems(
                        dataset_dir, all_generated, round_model_dir, round_index
                    )
                    
                    filtered_count = len(all_generated) - len(qualified_problems)
                    if filtered_count > 0:
                        logger.info("Quality check filtered out %d problems (too easy or too hard)", filtered_count)
                        logger.info("Remaining: %d qualified problems", len(qualified_problems))
                        all_generated = qualified_problems
                    else:
                        logger.info("All %d problems passed quality check", len(all_generated))
                
                logger.info("Successfully processed %d newly generated problems", len(all_generated))
            else:
                logger.info("No new problems generated in this round")

            self._update_state_after_round(
                round_index,
                train_records,
                eval_records,
                wrong_records,
                all_generated,
            )

            self._write_round_summary(
                round_dir,
                train_records,
                eval_records,
                wrong_records,
                category_stats,
                all_generated,
            )

        self._write_overall_summary()

    # ------------------------------------------------------------------
    # Data loading helpers
    # ------------------------------------------------------------------
    def _load_problems(self) -> None:
        logger.info("Loading WeMath standard dataset for multi-round pipeline")

        wemath_records = self._load_wemath_standard(Path(self.config.dataset_path))
        
        # 检查数据集是否自带分类信息
        has_builtin_categories = False
        if wemath_records:
            first_record = wemath_records[0]
            has_builtin_categories = "category" in first_record and "category_name" in first_record
        
        # 如果数据集没有自带分类信息，则从外部文件加载
        classification_map: Dict[str, int] = {}
        if not has_builtin_categories:
            logger.info("Dataset does not contain built-in category info, loading from classification file")
            classification_map = self._load_classification_map(Path(self.config.classification_file))
            logger.info("Loaded %s category mappings from external file", len(classification_map))
        else:
            logger.info("Dataset contains built-in category info, will use it directly")

        skipped = 0
        for idx, record in enumerate(wemath_records):
            pid = str(record.get("id", ""))
            if idx < 3:  # 打印前3条记录用于调试
                logger.info("Record %d: raw_id=%s, pid=%s, type=%s", idx, record.get("id"), pid, type(record.get("id")))
            if not pid:
                skipped += 1
                continue

            # 优先使用数据集自带的分类信息
            if has_builtin_categories:
                category_id = record.get("category", 0)
                category_name = record.get("category_name", "Unknown")
            else:
                # 回退到外部分类文件
                category_id = classification_map.get(pid, 0)
                category_name = self.config.categories.get(category_id, "未知类别")

            # 读取 COT 答案：优先使用 model_answer（详细推理），如果没有则使用 answer
            answer_text = record.get("model_answer", "")
            if not answer_text or not isinstance(answer_text, str):
                answer_text = record.get("answer", "")
            answer_text = str(answer_text).strip()

            problem = ProblemRecord(
                pid=pid,
                category_id=category_id,
                category_name=category_name,
                question=record.get("problem", ""),
                answer=answer_text,  # COT 格式的答案（来自 model_answer 或 answer）
                image_path=record.get("image_path", ""),
                source="original",
                image_code=None,  # wemath_standard_30b 使用真实图片，不需要生成
                idx=record.get("idx"),
                model_answer=None,  # 不使用 model_answer 字段
            )
            self.problem_map[pid] = problem

        if skipped > 0:
            logger.warning("Skipped %s records with empty ID", skipped)
            
        self.available_problem_ids = list(self.problem_map.keys())
        self.rng.shuffle(self.available_problem_ids)
        logger.info("Loaded %s problems from dataset", len(self.problem_map))

    @staticmethod
    def _load_classification_map(path: Path) -> Dict[str, int]:
        mapping: Dict[str, int] = {}
        if not path.exists():
            logger.warning("Classification file not found at %s", path)
            return mapping

        with path.open("r", encoding="utf-8") as fin:
            for line in fin:
                line = line.strip()
                if not line:
                    continue
                try:
                    pid, label = line.split("\t", 1)
                    cat_id = int(label.split("-", 1)[0].strip())
                    mapping[str(pid).strip()] = cat_id
                except ValueError:
                    continue
        return mapping

    @staticmethod
    def _load_wemath_standard(path: Path) -> List[Dict[str, object]]:
        """加载 wemath_standard_30b.json (JSON 数组格式)"""
        logger.info("Loading from path: %s", path)
        logger.info("Path exists: %s", path.exists())
        
        if not path.exists():
            logger.error("WeMath standard dataset not found at %s", path)
            return []

        with path.open("r", encoding="utf-8") as fin:
            try:
                data = json.load(fin)
            except json.JSONDecodeError as exc:
                logger.error("Failed to parse WeMath standard dataset: %s", exc)
                return []

        if not isinstance(data, list):
            logger.error("WeMath standard dataset should be a JSON array")
            return []

        logger.info("Loaded %s records from %s", len(data), path)
        if data:
            logger.info("First record keys: %s", list(data[0].keys()))
            logger.info("First record id: %s (type: %s)", data[0].get("id"), type(data[0].get("id")))
        return data

    # ------------------------------------------------------------------
    # Round preparation
    # ------------------------------------------------------------------
    def _prepare_round_datasets(self, round_index: int) -> Tuple[List[ProblemRecord], List[ProblemRecord]]:
        """
        准备每轮的训练和验证数据集
        1. 第一轮：使用原始数据集，按比例划分
        2. 后续轮：上一轮验证集的全部错题 + 生成新题
        """
        if round_index == 1:
            return self._prepare_first_round_datasets()
        else:
            return self._prepare_adaptive_round_datasets(round_index)
    
    def _prepare_first_round_datasets(self) -> Tuple[List[ProblemRecord], List[ProblemRecord]]:
        """第一轮：使用原始数据集，按比例划分"""
        if not self.available_problem_ids:
            raise RuntimeError("No data available for the first round.")
        
        # 获取所有原始数据
        all_records = [self.problem_map[pid] for pid in self.available_problem_ids]
        
        # 限制数据量
        if self.config.round_total_samples > 0:
            all_records = all_records[:self.config.round_total_samples]
        
        logger.info("Round 1 total data: %d samples", len(all_records))
        
        # 随机打乱
        self.rng.shuffle(all_records)
        
        # 按比例划分
        train_size = max(1, int(len(all_records) * self.config.train_ratio))
        train_records = all_records[:train_size]
        eval_records = all_records[train_size:]
        
        logger.info(
            "Round 1 dataset split – train=%d (%.1f%%), eval=%d (%.1f%%)",
            len(train_records),
            len(train_records) / len(all_records) * 100,
            len(eval_records),
            len(eval_records) / len(all_records) * 100,
        )
        
        # 清空 available_problem_ids（已全部使用）
        self.available_problem_ids = []
        
        return train_records, eval_records
    
    def _prepare_adaptive_round_datasets(self, round_index: int) -> Tuple[List[ProblemRecord], List[ProblemRecord]]:
        """后续轮：使用上轮错题 + 生成的新题"""
        
        # 1. 收集上一轮验证集的全部错题作为基础数据
        base_records = [self.problem_map[pid] for pid in self.prev_wrong_ids if pid in self.problem_map]
        
        if not base_records:
            logger.warning("No wrong problems from previous round, cannot continue.")
            raise RuntimeError("No wrong problems available for adaptive training.")
        
        logger.info("Round %d base data: %d wrong problems from previous round", round_index, len(base_records))
        
        # 2. 添加 Gemini 生成的新题
        all_records = list(self.generated_bank) + base_records
        
        logger.info("Round %d total data: %d samples (generated=%d, wrong=%d)",
                   round_index, len(all_records), len(self.generated_bank), len(base_records))
        
        # 3. 随机打乱
        self.rng.shuffle(all_records)
        
        # 4. 按比例划分
        train_size = max(1, int(len(all_records) * self.config.train_ratio))
        train_records = all_records[:train_size]
        eval_records = all_records[train_size:]
        
        logger.info(
            "Round %d dataset split – train=%d (%.1f%%), eval=%d (%.1f%%)",
            round_index,
            len(train_records),
            len(train_records) / len(all_records) * 100,
            len(eval_records),
            len(eval_records) / len(all_records) * 100,
        )
        
        return train_records, eval_records

    def _pop_available_ids(self, count: int) -> List[str]:
        count = max(0, min(count, len(self.available_problem_ids)))
        selected = self.available_problem_ids[:count]
        self.available_problem_ids = self.available_problem_ids[count:]
        return selected

    # ------------------------------------------------------------------
    # File writing helpers
    # ------------------------------------------------------------------
    def _write_dataset_files(
        self,
        dataset_dir: Path,
        train_records: List[ProblemRecord],
        eval_records: List[ProblemRecord],
    ) -> Tuple[Path, Path]:
        """直接写入 wemath_standard_30b 格式的数据"""
        train_path = dataset_dir / "train.json"
        eval_path = dataset_dir / "eval.json"

        self._dump_alpaca_format(train_path, train_records)
        self._dump_alpaca_format(eval_path, eval_records)

        return train_path, eval_path

    def _dump_alpaca_format(self, path: Path, records: List[ProblemRecord]) -> None:
        """写入 alpaca 格式的数据（与 wemath_standard_30b.json 格式一致）"""
        serialisable = []
        fixed_count = 0
        
        for problem in records:
            # 统一将 id 转换为整数
            if problem.pid.isdigit():
                id_value = int(problem.pid)
            elif "_" in problem.pid:
                # 对于 "gen_123" 格式，提取数字部分
                parts = problem.pid.split("_")
                id_value = int(parts[-1]) if parts[-1].isdigit() else hash(problem.pid) % (10**9)
            else:
                # 其他情况使用 hash 转换为整数
                id_value = hash(problem.pid) % (10**9)
            
            # LLaMA-Factory 要求 images 字段必须是列表格式
            image_path_list = [problem.image_path] if problem.image_path else []
            
            # 修复：确保 image_path 和 <image> 标签匹配
            question_text = problem.question
            if image_path_list and "<image>" not in question_text:
                # 情况1: 有图片但缺少 <image> 标签 -> 在开头插入标签
                question_text = "<image> " + question_text
                fixed_count += 1
                logger.debug("Auto-fixed: added <image> token for problem %s", problem.pid)
            elif not image_path_list and "<image>" in question_text:
                # 情况2: 没有图片但有 <image> 标签 -> 移除标签
                question_text = question_text.replace("<image>", "").strip()
                fixed_count += 1
                logger.debug("Auto-fixed: removed <image> token from problem %s", problem.pid)
            
            entry = {
                "id": id_value,
                "problem": question_text,
                "answer": problem.answer,  # COT 格式的答案（原始数据或 LLM 生成）
                "image_path": image_path_list,
                "category": problem.category_id,      # 与原始数据格式一致：顶层字段
                "category_name": problem.category_name,  # 与原始数据格式一致：顶层字段
                "system": (
                    "Solve the following problem carefully and thoroughly. Use a long, detailed chain of thought to reason step-by-step. "
                    "Make sure to: "
                    "1. Explicitly break down the problem into smaller parts. "
                    "2. Explore alternative approaches when relevant. "
                    "3. Verify intermediate steps for correctness. "
                    "4. Clearly summarize the reasoning before giving the final answer. "
                    "5. Enclose the final answer in \\boxed{}."
                ),
            }
            
            # 添加可选字段
            if problem.idx is not None:
                entry["idx"] = problem.idx
            
            # 添加元数据（用于追踪额外信息）
            entry["_metadata"] = {
                "source": problem.source,  # 标记数据来源（original/generated_hard/generated_refresh）
            }
            
            if problem.image_code:
                entry["_metadata"]["image_code"] = problem.image_code

            serialisable.append(entry)

        if fixed_count > 0:
            logger.info("Auto-fixed %d problems with missing <image> tokens in %s", fixed_count, path.name)

        with path.open("w", encoding="utf-8") as fout:
            json.dump(serialisable, fout, ensure_ascii=False, indent=2)


    @staticmethod
    def _write_dataset_info(dataset_dir: Path, train_filename: str, eval_filename: str) -> None:
        """写入 dataset_info.json，配置为 alpaca 格式（与 wemath_standard_30b 一致）"""
        dataset_info = {
            "round_train": {
                "file_name": train_filename,
                "formatting": "alpaca",
                "columns": {
                    "prompt": "problem",
                    "response": "answer",  # 使用 answer 字段（原始数据的 COT 或新生成的 COT）
                    "images": "image_path",
                    "system": "system"  # 添加 system 字段映射
                },
            },
            "round_eval": {
                "file_name": eval_filename,
                "formatting": "alpaca",
                "columns": {
                    "prompt": "problem",
                    "response": "answer",  # 使用 answer 字段
                    "images": "image_path",
                    "system": "system"  # 添加 system 字段映射
                },
            },
        }

        info_path = dataset_dir / "dataset_info.json"
        with info_path.open("w", encoding="utf-8") as fout:
            json.dump(dataset_info, fout, ensure_ascii=False, indent=2)

    def _write_train_yaml(self, round_index: int, dataset_dir: Path) -> Path:
        with open(self.config.train_yaml_template, "r", encoding="utf-8") as fin:
            config = yaml.safe_load(fin)

        output_dir = self._get_round_model_dir(round_index)
        output_dir.mkdir(parents=True, exist_ok=True)

        config["model_name_or_path"] = self.current_model_path
        config["dataset_dir"] = str(dataset_dir)
        config["dataset"] = "round_train"
        config.pop("eval_dataset", None)
        config["output_dir"] = str(output_dir)
        
        # 如果 round_total_samples <= 0，不限制 max_samples（使用全部数据）
        if self.config.round_total_samples <= 0:
            config.pop("max_samples", None)  
        else:
            config["max_samples"] = self.config.round_total_samples

        yaml_path = dataset_dir.parent / f"round_{round_index:02d}_train.yaml"
        with yaml_path.open("w", encoding="utf-8") as fout:
            yaml.safe_dump(config, fout, sort_keys=False, allow_unicode=True)
        return yaml_path

    def _write_eval_yaml(self, round_index: int, dataset_dir: Path) -> Path:
        with open(self.config.eval_yaml_template, "r", encoding="utf-8") as fin:
            config = yaml.safe_load(fin)

        eval_output = self._get_round_model_dir(round_index) / "eval"
        eval_output.mkdir(parents=True, exist_ok=True)

        config["model_name_or_path"] = str(self._get_round_model_dir(round_index))
        config["dataset_dir"] = str(dataset_dir)
        config.pop("dataset", None)
        config.pop("dataset_mixer", None)
        config["eval_dataset"] = ["round_eval"]
        config["output_dir"] = str(eval_output)
        config["stage"] = "sft"
        config["do_train"] = False
        config["do_eval"] = False
        config["do_predict"] = True
        config["predict_with_generate"] = True
        config.pop("deepspeed", None)

        yaml_path = dataset_dir.parent / f"round_{round_index:02d}_eval.yaml"
        with yaml_path.open("w", encoding="utf-8") as fout:
            yaml.safe_dump(config, fout, sort_keys=False, allow_unicode=True)
        return yaml_path

    def _get_round_model_dir(self, round_index: int) -> Path:
        return self.config.workspace_dir / "models" / f"round_{round_index:02d}"

    # ------------------------------------------------------------------
    # Command execution
    # ------------------------------------------------------------------
    def _run_llamafactory_command(self, command: List[str]) -> bool:
        logger.info("Executing command: %s", " ".join(command))
        try:
            subprocess.run(command, check=True)
            return True
        except subprocess.CalledProcessError as exc:
            logger.error("Command failed with exit code %s: %s", exc.returncode, command)
            return False

    # ------------------------------------------------------------------
    # Evaluation analysis
    # ------------------------------------------------------------------
    def _analyse_predictions(
        self,
        predictions_path: Path,
        eval_records: List[ProblemRecord],
    ) -> Tuple[List[ProblemRecord], Dict[int, Dict[str, float]]]:
        """分析预测结果，根据配置选择使用规则匹配或 LLM Judge"""
        
        if self.config.use_llm_judge:
            logger.info("Using LLM-as-Judge for evaluation")
            return self._analyse_predictions_with_llm_judge(predictions_path, eval_records)
        else:
            logger.info("Using rule-based matching for evaluation")
            return self._analyse_predictions_with_rules(predictions_path, eval_records)
    
    def _analyse_predictions_with_llm_judge(
        self,
        predictions_path: Path,
        eval_records: List[ProblemRecord],
    ) -> Tuple[List[ProblemRecord], Dict[int, Dict[str, float]]]:

        eval_records_dict = []
        for record in eval_records:
            eval_records_dict.append({
                'pid': record.pid,
                'question': record.question,
                'answer': record.answer,
                'category_id': record.category_id,
                'category_name': record.category_name,
            })
        
        # 创建临时文件保存 eval_records
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            eval_records_file = Path(f.name)
            json.dump(eval_records_dict, f, ensure_ascii=False, indent=2)
        
        # 创建临时文件接收 Judge 输出
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            judge_output = Path(f.name)
        
        # 使用独立的 LLM Judge 脚本（避免在代码中动态生成）
        judge_script = Path(__file__).parent.parent / "processors" / "run_llm_judge.py"
        
        # 准备 Judge 配置参数
        judge_config = {
            'model_path': self.config.judge_model_path,
            'tensor_parallel_size': self.config.judge_tensor_parallel_size,
            'gpu_memory_utilization': self.config.judge_gpu_memory_utilization,
            'temperature': self.config.judge_temperature,
            'max_tokens': self.config.judge_max_tokens,
            'max_model_len': self.config.judge_max_model_len,  # 添加 max_model_len 配置
        }
        judge_config_json = json.dumps(judge_config)
        
        logger.info("Running LLM Judge in separate process to avoid CUDA conflicts...")
        logger.info("Judge script: %s", judge_script)
        logger.info("Predictions: %s", predictions_path)
        logger.info("Eval records: %d items", len(eval_records))
        
        # 在独立的 Python 进程中运行（会完全重新初始化 CUDA）
        # 使用 Popen 和实时输出，以便监控进度
        try:
            import subprocess
            import select
            import time
            
            process = subprocess.Popen(
                [
                    sys.executable, 
                    str(judge_script),
                    str(eval_records_file),
                    str(predictions_path),
                    str(judge_output),
                    judge_config_json,
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,  # 行缓冲
            )
            
            # 实时输出子进程日志（带超时检测）
            logger.info("=== LLM Judge subprocess output ===")
            last_output_time = time.time()
            no_output_timeout = 300  # 5分钟没有输出就认为可能卡死
            graceful_termination = False  # 标记是否是正常完成后主动终止的
            
            while True:
                # 检查进程是否结束
                if process.poll() is not None:
                    # 进程已结束，读取剩余输出
                    remaining = process.stdout.read()
                    if remaining:
                        for line in remaining.splitlines():
                            logger.info("  [Judge] %s", line.rstrip())
                    break
                
                # 非阻塞读取（使用 readline with timeout）
                import threading
                line_container = []
                
                def read_line():
                    try:
                        line = process.stdout.readline()
                        if line:
                            line_container.append(line)
                    except:
                        pass
                
                reader_thread = threading.Thread(target=read_line)
                reader_thread.daemon = True
                reader_thread.start()
                reader_thread.join(timeout=1.0)  # 1秒超时
                
                if line_container:
                    line = line_container[0]
                    logger.info("  [Judge] %s", line.rstrip())
                    last_output_time = time.time()
                else:
                    # 检查是否超时无输出
                    if time.time() - last_output_time > no_output_timeout:
                        logger.warning("⚠️  Judge subprocess has no output for %d seconds, but process is still running", no_output_timeout)
                        logger.warning("⚠️  This might indicate the process completed but stdout wasn't closed properly")
                        logger.warning("⚠️  Checking if output file exists...")
                        
                        # 检查输出文件是否已生成（可能进程已完成但 stdout 未关闭）
                        if judge_output.exists():
                            logger.info("✅ Output file exists! Judge likely completed. Terminating subprocess.")
                            graceful_termination = True  # 标记为正常完成
                            process.terminate()
                            time.sleep(2)
                            if process.poll() is None:
                                process.kill()
                            break
                        
                        # 重置超时计时器
                        last_output_time = time.time()
                    
                    time.sleep(0.5)  # 短暂休眠避免 CPU 占用
            
            # 等待进程结束（如果还没结束）
            return_code = process.wait(timeout=10)
            logger.info("=== LLM Judge subprocess finished with code %d ===", return_code)
            
            # 如果是正常完成后主动终止，不报错
            if return_code != 0 and not graceful_termination:
                raise RuntimeError(f"LLM Judge failed with exit code {return_code}")
                
        except subprocess.TimeoutExpired:
            logger.error("LLM Judge timed out after 1 hour")
            process.kill()
            raise RuntimeError("LLM Judge timed out")
        except Exception as e:
            logger.error("LLM Judge failed: %s", e)
            raise
        finally:
            # 清理临时文件
            eval_records_file.unlink(missing_ok=True)
        
        # 读取结果
        if not judge_output.exists():
            raise RuntimeError("LLM Judge did not produce output file")
        
        with open(judge_output, 'r') as f:
            results = json.load(f)
        
        eval_data = results['eval_data']
        judge_stats = results['judge_stats']
        
        # 清理输出文件
        judge_output.unlink(missing_ok=True)
        
        # 输出 Judge 统计信息
        logger.info("=" * 70)
        logger.info("LLM Judge Statistics:")
        logger.info("  Total: %d", judge_stats.get('total', 0))
        logger.info("  Correct: %d", judge_stats.get('correct', 0))
        logger.info("  Wrong: %d", judge_stats.get('wrong', 0))
        logger.info("  Unknown: %d", judge_stats.get('unknown', 0))
        logger.info("  Accuracy: %.2f%%", judge_stats.get('accuracy', 0.0) * 100)
        logger.info("=" * 70)
        
        # 构建结果（按类别统计）
        wrong_records: List[ProblemRecord] = []
        stats: Dict[int, Dict[str, float]] = {}
        
        for record, eval_item in zip(eval_records, eval_data):
            # 更新预测结果
            record.last_prediction = eval_item.get('model_prediction', '')
            
            # 统计
            total = stats.setdefault(record.category_id, {"total": 0, "wrong": 0})
            total["total"] += 1
            
            # 判断是否错误
            matched = eval_item.get('matched')
            if matched != True:  # False 或 None 都算错
                total["wrong"] += 1
                wrong_records.append(record)
        
        # 计算每个类别的错误率
        for category_id, value in stats.items():
            total = value.get("total", 1)
            wrong = value.get("wrong", 0)
            value["ratio"] = wrong / total if total else 0.0
            value["category_name"] = self.config.categories.get(category_id, "未知类别")
        
        logger.info(
            "LLM Judge evaluation summary: %s wrong out of %s examples (%.2f%%)",
            len(wrong_records),
            len(eval_records),
            (len(wrong_records) / len(eval_records)) * 100 if eval_records else 0,
        )
        
        # 保存详细的 Judge 结果和统计信息
        judge_results_path = predictions_path.parent / "judge_results.json"
        judge_summary_path = predictions_path.parent / "judge_summary.json"
        
        with judge_results_path.open("w", encoding="utf-8") as fout:
            json.dump(eval_data, fout, ensure_ascii=False, indent=2)
        logger.info("Saved detailed LLM Judge results to %s", judge_results_path)
        
        # 保存完整的统计信息
        summary_data = {
            "overall": judge_stats,
            "by_category": stats,
        }
        with judge_summary_path.open("w", encoding="utf-8") as fout:
            json.dump(summary_data, fout, ensure_ascii=False, indent=2)
        logger.info("Saved LLM Judge summary to %s", judge_summary_path)
        
        return wrong_records, stats
    
    def _analyse_predictions_with_rules(
        self,
        predictions_path: Path,
        eval_records: List[ProblemRecord],
    ) -> Tuple[List[ProblemRecord], Dict[int, Dict[str, float]]]:
        """使用规则匹配分析预测结果（原有逻辑）"""
        predictions: List[Dict[str, str]] = []
        
        # 读取 JSON 数组格式的预测结果
        try:
            with predictions_path.open("r", encoding="utf-8") as fin:
                predictions = json.load(fin)
                logger.debug("Loaded %d predictions from JSON file", len(predictions))
        except Exception as exc:
            logger.error("Failed to read predictions from %s: %s", predictions_path, exc)
            logger.warning("Returning empty predictions list")

        if len(predictions) != len(eval_records):
            logger.warning(
                "Mismatch between predictions (%s) and evaluation records (%s)",
                len(predictions),
                len(eval_records),
            )

        wrong_records: List[ProblemRecord] = []
        stats: Dict[int, Dict[str, float]] = {}

        for record, pred in zip(eval_records, predictions):
            predicted_answer = pred.get("predict", "")
            label_answer = pred.get("label", "")

            prediction_text = str(predicted_answer) if predicted_answer is not None else ""
            label_text = str(label_answer) if label_answer is not None else ""

            record.last_prediction = prediction_text

            total = stats.setdefault(record.category_id, {"total": 0, "wrong": 0})
            total["total"] += 1

            if not self._answers_match(label_text, prediction_text):
                total["wrong"] += 1
                wrong_records.append(record)

        for category_id, value in stats.items():
            total = value.get("total", 1)
            wrong = value.get("wrong", 0)
            value["ratio"] = wrong / total if total else 0.0
            value["category_name"] = self.config.categories.get(category_id, "未知类别")

        # 计算总体准确率
        total_count = len(eval_records)
        correct_count = total_count - len(wrong_records)
        accuracy = correct_count / total_count if total_count > 0 else 0.0
        
        logger.info("=" * 70)
        logger.info("Rule-based Evaluation Statistics:")
        logger.info("  Total: %d", total_count)
        logger.info("  Correct: %d", correct_count)
        logger.info("  Wrong: %d", len(wrong_records))
        logger.info("  Accuracy: %.2f%%", accuracy * 100)
        logger.info("=" * 70)

        logger.info(
            "Rule-based evaluation summary: %s wrong out of %s examples (%.2f%%)",
            len(wrong_records),
            len(eval_records),
            (len(wrong_records) / len(eval_records)) * 100 if eval_records else 0,
        )
        
        # 保存统计信息
        rule_summary_path = predictions_path.parent / "rule_summary.json"
        summary_data = {
            "overall": {
                "total": total_count,
                "correct": correct_count,
                "wrong": len(wrong_records),
                "accuracy": accuracy,
            },
            "by_category": stats,
        }
        with rule_summary_path.open("w", encoding="utf-8") as fout:
            json.dump(summary_data, fout, ensure_ascii=False, indent=2)
        logger.info("Saved rule-based evaluation summary to %s", rule_summary_path)

        return wrong_records, stats

    @staticmethod
    def _extract_boxed_answer(text: str) -> str:
        """从文本中提取 \\boxed{} 里的内容
        
        支持的格式:
        - \\boxed{answer}
        - \\boxed{ answer }
        - 嵌套的大括号: \\boxed{\\frac{a}{b}}
        
        如果找不到 \\boxed{}，返回原文本
        """
        if not text:
            return ""
        
        # 处理转义的反斜杠，匹配 \boxed{...} 或 \\boxed{...}
        pattern = r'\\\\?boxed\{'
        match = re.search(pattern, text)
        
        if not match:
            return text  # 没找到 boxed，返回原文
        
        # 找到 \boxed{ 的位置
        start_pos = match.end()
        
        # 手动匹配括号（处理嵌套情况）
        brace_count = 1
        end_pos = start_pos
        
        while end_pos < len(text) and brace_count > 0:
            if text[end_pos] == '{':
                brace_count += 1
            elif text[end_pos] == '}':
                brace_count -= 1
            end_pos += 1
        
        if brace_count == 0:
            # 成功匹配
            boxed_content = text[start_pos:end_pos-1]
            return boxed_content.strip()
        else:
            # 括号不匹配，返回原文
            return text

    @staticmethod
    def _answers_match(ground_truth: str, prediction: str) -> bool:
        # 提取 \boxed{} 中的内容（如果存在）
        truth = MultiRoundFineTuner._extract_boxed_answer(ground_truth)
        pred = MultiRoundFineTuner._extract_boxed_answer(prediction)
        
        # 标准化答案
        truth = MultiRoundFineTuner._normalise_answer(truth)
        pred = MultiRoundFineTuner._normalise_answer(pred)
        
        if not truth or not pred:
            return False
        if MultiRoundFineTuner._is_number(truth) and MultiRoundFineTuner._is_number(pred):
            try:
                return math.isclose(float(truth), float(pred), rel_tol=1e-6, abs_tol=1e-6)
            except ValueError:
                pass
        return truth == pred

    @staticmethod
    def _normalise_answer(answer: str) -> str:
        text = answer or ""
        text = text.strip().lower()
        
        # 移除常见的 LaTeX 命令和符号
        latex_removals = [
            ('\\left', ''),
            ('\\right', ''),
            ('\\,', ''),
            ('\\;', ''),
            ('\\quad', ''),
            ('\\qquad', ''),
            ('\\frac', 'frac'),
            ('\\text', ''),
            ('\\mathrm', ''),
            ('\\mathbf', ''),
            ('\\ln', 'ln'),
            ('\\log', 'log'),
            ('\\sin', 'sin'),
            ('\\cos', 'cos'),
            ('\\tan', 'tan'),
            ('\\sqrt', 'sqrt'),
            ('$', ''),  # 移除 $
        ]
        
        for pattern, replacement in latex_removals:
            text = text.replace(pattern, replacement)
        
        for marker in ("final answer:", "答案:"):
            if marker in text:
                text = text.split(marker, 1)[1]
        
        text = text.strip()
        text = text.replace("\n", " ")
        
        # 只保留字母、数字和特定符号
        text = "".join(ch for ch in text if ch.isalnum() or ch in ".-/")
        return text

    @staticmethod
    def _is_number(value: str) -> bool:
        try:
            float(value)
            return True
        except (TypeError, ValueError):
            return False

    # ------------------------------------------------------------------
    # Hard problem generation
    # ------------------------------------------------------------------
    def _generate_hard_problems(
        self,
        category_stats: Dict[int, Dict[str, float]],
        wrong_records: List[ProblemRecord],
        round_index: int,
    ) -> List[ProblemRecord]:
        """
        根据错题生成新题
        - 对每道错题都生成 2 道新题
        - 第1道题：提升思维难度（增加推理步骤、更复杂的逻辑）
        - 第2道题：提升视觉难度（更复杂的图形、更多视觉元素）
        """
        new_problems: List[ProblemRecord] = []
        all_generation_records = []  # 收集所有类别的生成记录
        
        if not wrong_records:
            logger.info("No wrong problems to generate from")
            return new_problems

        # 按类别分组错题
        wrong_by_category: Dict[int, List[ProblemRecord]] = {}
        for record in wrong_records:
            wrong_by_category.setdefault(record.category_id, []).append(record)
        
        # 对每个类别的错题逐个生成
        for category_id, category_wrong_records in wrong_by_category.items():
            category_name = self.config.categories.get(category_id, "Unknown")
            stats = category_stats.get(category_id, {})
            ratio = stats.get("ratio", 0.0)
            wrong = int(stats.get("wrong", 0))
            total = int(stats.get("total", 1))
            
            logger.info(
                "Category %s (%s): error_rate=%.1f%% (%d/%d) - will generate 2 problems per wrong problem",
                category_id, category_name, ratio * 100, wrong, total
            )
            
            # 并发生成：对每道错题生成2道新题（1道思维难度提升 + 1道视觉难度提升）
            problems_generated_count = 0
            generation_records = []  # 记录每道错题及其生成的新题
            
            logger.info(
                "Starting concurrent generation for %d wrong problems in category '%s' (max_workers=%d)...",
                len(category_wrong_records), category_name, min(50, len(category_wrong_records))
            )
            
            # 准备所有任务
            def generate_for_one_problem(idx_and_record):
                idx, wrong_record = idx_and_record
                example = {
                    "question": wrong_record.question,
                    "answer": wrong_record.answer,
                    "prediction": wrong_record.last_prediction or "",
                    "image_code": wrong_record.image_code or "",
                }
                
                logger.info(
                    "🤖 [Thread-%d] Calling Gemini API for wrong problem %d/%d in category '%s'...",
                    idx + 1, idx + 1, len(category_wrong_records), category_name
                )
                
                # 调用 Gemini 生成（传入错题示例和图片，生成2道题）
                generated = self.gemini_generator.generate_two_difficulty_variants(
                    category_id=category_id,
                    category_name=category_name,
                    category_description=self.config.category_descriptions.get(category_id, ""),
                    wrong_example=example,
                    image_path=wrong_record.image_path if wrong_record.image_path else None,
                )
                
                logger.info(
                    "✅ [Thread-%d] Generated %d problem(s) from wrong problem %d/%d",
                    idx + 1, len(generated), idx + 1, len(category_wrong_records)
                )
                
                return (idx, wrong_record, generated)
            
            # 使用线程池并发执行
            max_workers = min(50, len(category_wrong_records))  
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # 提交所有任务
                futures = {
                    executor.submit(generate_for_one_problem, (idx, record)): idx
                    for idx, record in enumerate(category_wrong_records)
                }
                
                # 按完成顺序处理结果
                for future in as_completed(futures):
                    try:
                        idx, wrong_record, generated = future.result()
                        
                        # 记录此错题及其生成的新题
                        generation_record = {
                            "original_problem": {
                                "pid": wrong_record.pid,
                                "category_id": category_id,
                                "category_name": category_name,
                                "question": wrong_record.question,
                                "answer": wrong_record.answer,
                                "prediction": wrong_record.last_prediction or "",
                                "image_code": wrong_record.image_code or "",
                                "source": wrong_record.source,
                            },
                            "generated_problems": []
                        }
                        
                        # 处理生成的题目
                        for item in generated:
                            # 分配新的唯一ID（纯数字）
                            generated_id = self.next_generated_id
                            self.next_generated_id += 1
                            
                            pid = str(generated_id)  # 使用纯数字作为pid
                            question = item.get("question", "").strip()
                            answer = item.get("answer", "").strip()
                            image_code = item.get("image_code", "")
                            difficulty_type = item.get("difficulty_type", "unknown")  # reasoning 或 visual
                            
                            if isinstance(image_code, str):
                                image_code = image_code.strip()
                            else:
                                image_code = ""
                            
                            problem = ProblemRecord(
                                pid=pid,
                                category_id=category_id,
                                category_name=category_name,
                                question=question,
                                answer=answer,
                                image_path="",  # 生成图片后会更新
                                source=f"generated_hard_{difficulty_type}", 
                                image_code=image_code if image_code else None,
                            )
                            new_problems.append(problem)
                            problems_generated_count += 1
                            
                            # 记录生成的新题
                            generation_record["generated_problems"].append({
                                "pid": pid,
                                "difficulty_type": difficulty_type,
                                "question": question,
                                "answer": answer,
                                "image_code": image_code,
                            })
                        
                        # 添加到记录列表
                        generation_records.append(generation_record)
                    
                    except Exception as e:
                        logger.error(
                            "❌ Failed to generate problems for wrong problem in category '%s': %s",
                            category_name, str(e)
                        )

            logger.info(
                "Generated %d problems for category %s (%s) - %d reasoning-hard + %d visual-hard",
                problems_generated_count, category_id, category_name,
                problems_generated_count // 2, problems_generated_count // 2
            )
            
            # 将此类别的记录添加到总记录中
            if generation_records:
                all_generation_records.extend([{
                    "category_id": category_id,
                    "category_name": category_name,
                    **record
                } for record in generation_records])

        if new_problems:
            logger.info(
                "Generated %s new problems across %s categories (2 per wrong problem: 1 reasoning-hard + 1 visual-hard)",
                len(new_problems),
                len({p.category_id for p in new_problems})
            )
        else:
            logger.info("No new problems generated")
        
        # 统一保存所有错题生成记录到一个文件
        if all_generation_records:
            record_file = self.config.get_round_dir(round_index) / "generation_records" / "hard_problems_all.json"
            record_file.parent.mkdir(parents=True, exist_ok=True)
            with open(record_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "round": round_index,
                    "generation_type": "hard_problems",
                    "total_wrong_problems": len(wrong_records),
                    "total_generated_problems": len(new_problems),
                    "records": all_generation_records
                }, f, indent=2, ensure_ascii=False)
            logger.info(
                "📝 Saved all hard problem generation records to %s (%d records)",
                record_file, len(all_generation_records)
            )
        
        return new_problems

    def _generate_refresh_problems(
        self,
        category_stats: Dict[int, Dict[str, float]],
        wrong_records: List[ProblemRecord],
        eval_records: List[ProblemRecord],
        round_index: int,
    ) -> List[ProblemRecord]:
        """
        根据答对的题目生成新题（使用聚类方法选择有代表性的题目）
        规则（聚类 + 逐题生成）：
        - 使用聚类算法从每个类别的正确题目中选择最具代表性的题目
        - 对每道选中的代表性题目生成 2 道新题：
          - 第1道题：提升思维难度（增加推理步骤、更复杂的逻辑）
          - 第2道题：提升视觉难度（更复杂的图形、更多视觉元素）
        """

        wrong_ids = {record.pid for record in wrong_records}
        correct_records = [record for record in eval_records if record.pid not in wrong_ids]
        if not correct_records:
            return []

        logger.info("="*70)
        logger.info("Generating refresh problems using clustering-based selection")
        logger.info("="*70)
        logger.info("Total correct problems: %d", len(correct_records))

        # 按类别分组正确的题目
        correct_by_category: Dict[int, List[ProblemRecord]] = {}
        for record in correct_records:
            correct_by_category.setdefault(record.category_id, []).append(record)

        logger.info("Correct problems distributed across %d categories", len(correct_by_category))

        # 准备聚类输入：将 ProblemRecord 转换为字典格式，包含 image_path
        problems_by_category_for_clustering = {}
        for category_id, records in correct_by_category.items():
            problems_by_category_for_clustering[category_id] = [
                {
                    'pid': r.pid,
                    'category_id': r.category_id,
                    'category_name': r.category_name,
                    'question': r.question,
                    'answer': r.answer,
                    'image_path': r.image_path  # 添加图片路径用于多模态聚类
                }
                for r in records
            ]

        # 使用聚类选择代表性题目（启用多模态聚类）
        logger.info("="*70)
        logger.info("Applying multimodal clustering (text + image) to select representative problems...")
        logger.info("="*70)
        
        try:
            from ..processors.problem_clustering import ProblemClusterer
            
            clusterer = ProblemClusterer(
                embedding_model="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                min_cluster_size=2,
                max_cluster_size=10,
                selection_strategy="centroid",
                use_image_embeddings=True, 
                image_model="facebook/dinov2-small" 
            )
            
            # 执行聚类并选择代表性题目
            representatives_by_category = clusterer.select_representative_problems(
                problems_by_category_for_clustering
            )
            
            # 获取聚类统计信息
            clustering_stats = clusterer.get_cluster_statistics(
                problems_by_category_for_clustering
            )
            
            # 打印聚类统计
            logger.info("\nClustering statistics:")
            total_representatives = 0
            for category_id in sorted(clustering_stats.keys()):
                stat = clustering_stats[category_id]
                category_name = self.config.categories.get(category_id, "未知类别")
                
                if 'error' in stat:
                    logger.warning(
                        "  Category %d (%s): Clustering failed - %s",
                        category_id, category_name, stat['error']
                    )
                else:
                    n_reps = len(representatives_by_category.get(category_id, []))
                    total_representatives += n_reps
                    logger.info(
                        "  Category %d (%s): %d problems -> %d clusters -> %d representatives",
                        category_id,
                        category_name,
                        stat['n_problems'],
                        stat['n_clusters'],
                        n_reps
                    )
            
            logger.info(
                "\n✅ Selected %d representative problems from %d correct problems (%.1f%% reduction)",
                total_representatives,
                len(correct_records),
                (1 - total_representatives / len(correct_records)) * 100 if len(correct_records) > 0 else 0
            )
            
        except ImportError as e:
            logger.error(
                "Failed to import clustering module: %s\n"
                "Please install dependencies: pip install sentence-transformers scikit-learn numpy\n"
                "Falling back to random selection...",
                e
            )
            # Fallback: 随机选择
            representatives_by_category = {}
            clustering_stats = {}
            for category_id, records_list in correct_by_category.items():
                n_select = max(1, min(len(records_list) // 3, 10))
                selected = self.rng.sample(records_list, n_select)
                representatives_by_category[category_id] = [
                    {
                        'pid': r.pid,
                        'category_id': r.category_id,
                        'category_name': r.category_name,
                        'question': r.question,
                        'answer': r.answer
                    }
                    for r in selected
                ]
        except Exception as e:
            logger.error("Clustering failed: %s\nFalling back to random selection...", e)
            # Fallback: 随机选择
            representatives_by_category = {}
            clustering_stats = {}
            for category_id, records_list in correct_by_category.items():
                n_select = max(1, min(len(records_list) // 3, 10))
                selected = self.rng.sample(records_list, n_select)
                representatives_by_category[category_id] = [
                    {
                        'pid': r.pid,
                        'category_id': r.category_id,
                        'category_name': r.category_name,
                        'question': r.question,
                        'answer': r.answer
                    }
                    for r in selected
                ]

        # 将选中的代表性题目（字典格式）转回 ProblemRecord
        selected_records_by_category = {}
        for category_id, repr_dicts in representatives_by_category.items():
            selected_records_by_category[category_id] = []
            for repr_dict in repr_dicts:
                # 从原始记录中找到对应的 ProblemRecord
                for record in correct_by_category[category_id]:
                    if record.pid == repr_dict['pid']:
                        selected_records_by_category[category_id].append(record)
                        break

        if not selected_records_by_category:
            logger.warning("No representative problems selected, skipping refresh problem generation")
            return []

        new_problems: List[ProblemRecord] = []
        all_generation_records = []  # 收集所有类别的生成记录
        problems_selected = 0

        # 为每个类别的代表性题目生成新题
        logger.info("="*70)
        logger.info("Generating new problems from representatives...")
        logger.info("="*70)
        
        for category_id in sorted(selected_records_by_category.keys()):
            selected_records = selected_records_by_category[category_id]
            if not selected_records:
                continue

            category_name = self.config.categories.get(category_id, "未知类别")
            category_desc = self.config.category_descriptions.get(category_id, "")
            
            logger.info(
                "Category %s (%s): will generate 2 problems per representative (%d representatives)",
                category_id, category_name, len(selected_records)
            )
            
            # 并发生成：对每道代表性题目生成2道新题（1道思维难度提升 + 1道视觉难度提升）
            problems_generated_count = 0
            generation_records = []  # 记录每道代表性题及其生成的新题
            
            logger.info(
                "🚀 Starting concurrent generation for %d correct problems in category '%s' (max_workers=%d)...",
                len(selected_records), category_name, min(50, len(selected_records))
            )
            
            # 准备所有任务
            def generate_for_one_correct_problem(idx_and_record):
                idx, correct_record = idx_and_record
                example = {
                    "question": correct_record.question,
                    "answer": correct_record.answer,
                    "prediction": correct_record.last_prediction or correct_record.answer,
                    "image_code": correct_record.image_code or "",
                }
                
                logger.info(
                    "🤖 [Thread-%d] Calling Gemini API for correct problem %d/%d in category '%s'...",
                    idx + 1, idx + 1, len(selected_records), category_name
                )
                
                # 调用 Gemini 生成（传入正确题目示例和图片，生成2道题）
                generated = self.gemini_generator.generate_two_difficulty_variants_from_correct(
                    category_id=category_id,
                    category_name=category_name,
                    category_description=category_desc,
                    correct_example=example,
                    image_path=correct_record.image_path if correct_record.image_path else None,
                )
                
                logger.info(
                    "✅ [Thread-%d] Generated %d problem(s) from correct problem %d/%d",
                    idx + 1, len(generated), idx + 1, len(selected_records)
                )
                
                return (idx, correct_record, generated)
            
            # 使用线程池并发执行
            max_workers = min(50, len(selected_records))
            with ThreadPoolExecutor(max_workers=max_workers) as executor:

                futures = {
                    executor.submit(generate_for_one_correct_problem, (idx, record)): idx
                    for idx, record in enumerate(selected_records)
                }
                
                # 按完成顺序处理结果
                for future in as_completed(futures):
                    try:
                        idx, correct_record, generated = future.result()
                        
                        # 记录此正确题及其生成的新题
                        generation_record = {
                            "original_problem": {
                                "pid": correct_record.pid,
                                "category_id": category_id,
                                "category_name": category_name,
                                "question": correct_record.question,
                                "answer": correct_record.answer,
                                "prediction": correct_record.last_prediction or correct_record.answer,
                                "image_code": correct_record.image_code or "",
                                "source": correct_record.source,
                            },
                            "generated_problems": []
                        }
                        
                        # 处理生成的题目
                        for item in generated:
                            generated_id = self.next_generated_id
                            self.next_generated_id += 1
                            
                            pid = str(generated_id) 
                            question = item.get("question", "").strip()
                            answer = item.get("answer", "").strip()
                            image_code = item.get("image_code", "")
                            difficulty_type = item.get("difficulty_type", "unknown")  # reasoning 或 visual
                            
                            if isinstance(image_code, str):
                                image_code = image_code.strip()
                            else:
                                image_code = ""
                            
                            problem = ProblemRecord(
                                pid=pid,
                                category_id=category_id,
                                category_name=category_name,
                                question=question,
                                answer=answer,
                                image_path="",  # 生成图片后会更新
                                source=f"generated_refresh_{difficulty_type}",  # 标记难度类型
                                image_code=image_code if image_code else None,
                            )
                            new_problems.append(problem)
                            problems_generated_count += 1
                            
                            # 记录生成的新题
                            generation_record["generated_problems"].append({
                                "pid": pid,
                                "difficulty_type": difficulty_type,
                                "question": question,
                                "answer": answer,
                                "image_code": image_code,
                            })
                        
                        # 添加到记录列表
                        generation_records.append(generation_record)
                    
                    except Exception as e:
                        logger.error(
                            "❌ Failed to generate problems for correct problem in category '%s': %s",
                            category_name, str(e)
                        )

            logger.info(
                "Generated %d refresh problems for category %s (%s) - %d reasoning-hard + %d visual-hard",
                problems_generated_count, category_id, category_name,
                problems_generated_count // 2, problems_generated_count // 2
            )
            
            # 将此类别的记录添加到总记录中
            if generation_records:
                all_generation_records.extend([{
                    "category_id": category_id,
                    "category_name": category_name,
                    **record
                } for record in generation_records])
            
            problems_selected += len(selected_records)

        if new_problems:
            logger.info(
                "\n✅ Successfully generated %d refresh problems from %d representatives across %d categories",
                len(new_problems),
                problems_selected,
                len(selected_records_by_category)
            )
            logger.info(
                "   (2 per representative: %d reasoning-hard + %d visual-hard)",
                len(new_problems) // 2, len(new_problems) // 2
            )
        
        # 统一保存所有正确题生成记录到一个文件
        if all_generation_records:
            record_file = self.config.get_round_dir(round_index) / "generation_records" / "refresh_problems_all.json"
            record_file.parent.mkdir(parents=True, exist_ok=True)
            
            # 构建聚类统计摘要
            clustering_summary = {}
            if clustering_stats:
                for cat_id, stat in clustering_stats.items():
                    if 'error' not in stat:
                        # 将 NumPy 类型转换为 Python 原生类型
                        n_problems = int(stat.get('n_problems', 0))
                        n_clusters = int(stat.get('n_clusters', 0))
                        cluster_sizes = stat.get('cluster_sizes', [])
                        # 转换 cluster_sizes 中的 NumPy int64 为 Python int
                        cluster_sizes = [int(x) for x in cluster_sizes] if cluster_sizes else []
                        
                        clustering_summary[cat_id] = {
                            'category_name': self.config.categories.get(cat_id, "未知类别"),
                            'n_problems': n_problems,
                            'n_clusters': n_clusters,
                            'n_representatives': len(representatives_by_category.get(cat_id, [])),
                            'cluster_sizes': cluster_sizes
                        }
            
            with open(record_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "round": round_index,
                    "generation_type": "refresh_problems_with_clustering",
                    "total_correct_problems": len(correct_records),
                    "total_representatives_selected": problems_selected,
                    "total_generated_problems": len(new_problems),
                    "clustering_enabled": bool(clustering_stats),
                    "clustering_statistics": clustering_summary,
                    "records": all_generation_records
                }, f, indent=2, ensure_ascii=False)
            logger.info(
                "📝 Saved all refresh problem generation records to %s (%d records)",
                record_file, len(all_generation_records)
            )
        
        return new_problems

    def _calculate_generation_quota(self, error_ratio: float) -> int:
        span = max(self.config.max_hard_problems - self.config.base_hard_problems, 0)
        scaled = self.config.base_hard_problems + span * (error_ratio - self.config.min_error_rate_threshold) / (1 - self.config.min_error_rate_threshold)
        return max(self.config.base_hard_problems, int(round(min(self.config.max_hard_problems, scaled))))

    # ------------------------------------------------------------------
    # State updates & summaries
    # ------------------------------------------------------------------
    def _update_state_after_round(
        self,
        round_index: int,
        train_records: List[ProblemRecord],
        eval_records: List[ProblemRecord],
        wrong_records: List[ProblemRecord],
        generated_records: List[ProblemRecord],
    ) -> None:
        # 保留所有错题的ID（包括新生成的题目）
        self.prev_wrong_ids = [record.pid for record in wrong_records]
        
        # 将新生成的题目添加到 problem_map 中，以便下一轮可以使用
        for record in generated_records:
            if record.pid not in self.problem_map:
                self.problem_map[record.pid] = record
        
        # 同时也要将本轮的错题添加到 problem_map（如果是新生成的）
        for record in wrong_records:
            if record.pid not in self.problem_map:
                self.problem_map[record.pid] = record
        
        self.generated_bank = list(generated_records)

        summary_entry = {
            "round": round_index,
            "train_size": len(train_records),
            "eval_size": len(eval_records),
            "wrong_count": len(wrong_records),
            "generated_count": len(generated_records),
        }
        if generated_records:
            summary_entry["generated_by_source"] = {
                source: sum(1 for record in generated_records if record.source == source)
                for source in {record.source for record in generated_records}
            }
        self.history.append(summary_entry)

    def _write_round_summary(
        self,
        round_dir: Path,
        train_records: List[ProblemRecord],
        eval_records: List[ProblemRecord],
        wrong_records: List[ProblemRecord],
        category_stats: Dict[int, Dict[str, float]],
        generated_records: List[ProblemRecord],
    ) -> None:
        summary = {
            "train_ids": [p.pid for p in train_records],
            "eval_ids": [p.pid for p in eval_records],
            "wrong_ids": [p.pid for p in wrong_records],
            "category_stats": category_stats,
            "generated_ids": [p.pid for p in generated_records],
        }
        if generated_records:
            summary["generated_by_source"] = {
                source: [p.pid for p in generated_records if p.source == source]
                for source in {p.source for p in generated_records}
            }
        with (round_dir / "summary.json").open("w", encoding="utf-8") as fout:
            json.dump(summary, fout, ensure_ascii=False, indent=2)

    def _write_overall_summary(self) -> None:
        summary_path = self.config.workspace_dir / "multi_round_history.json"
        with summary_path.open("w", encoding="utf-8") as fout:
            json.dump(self.history, fout, ensure_ascii=False, indent=2)
        logger.info("Wrote multi-round training summary to %s", summary_path)

    def _materialise_problem_images(self, dataset_dir: Path, records: List[ProblemRecord]) -> List[str]:
        """执行 image_code 生成图片，保存到固定位置，使用纯数字命名
        
        如果代码执行失败，会调用 Gemini 重新修复代码并重试
        
        Returns:
            需要删除的问题 ID 列表（图片生成失败的问题）
        """
        image_records = [record for record in records if record.image_code]
        if not image_records:
            return []

        try:
            import matplotlib
            import matplotlib.pyplot as plt  # type: ignore[import]
            from matplotlib.figure import Figure  # type: ignore[import]
            import numpy as np  # type: ignore[import]
            
            # 设置中文字体，避免警告和方框
            matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']
            matplotlib.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题
        except Exception as exc:  # pragma: no cover - optional dependency
            logger.error("matplotlib/numpy not available for image generation: %s", exc)
            return []

        # 使用固定的绝对路径（与原始数据一致）
        image_dir = Path("/mnt/dhwfile/raise/user/zhuyun/We-Math2.0-Standard/images_generated")
        image_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("Generating images for %s problems with image_code", len(image_records))

        max_retries = 2  # 最多重试2次
        failed_pids: List[str] = []  # 记录失败的问题ID
        failed_problems: List[Dict] = []  # 记录失败问题的详细信息
        
        for record in image_records:
            # 直接使用 pid 作为图片名称（pid 已经是纯数字了）
            output_path = image_dir / f"{record.pid}.png"
            
            current_code = record.image_code
            success = False
            error_history = []  # 记录所有尝试的错误
            
            for attempt in range(max_retries + 1):
                namespace: Dict[str, object] = {"plt": plt, "np": np}
                original_savefig = plt.savefig
                original_fig_savefig = Figure.savefig

                def plt_savefig_override(path: object, *args: object, **kwargs: object) -> object:
                    return original_savefig(output_path, *args, **kwargs)

                def fig_savefig_override(self: Figure, path: object, *args: object, **kwargs: object) -> object:
                    return original_fig_savefig(self, output_path, *args, **kwargs)

                plt.savefig = plt_savefig_override  # type: ignore[assignment]
                Figure.savefig = fig_savefig_override  # type: ignore[assignment]

                try:
                    exec(current_code, namespace, {})  # pylint: disable=exec-used
                    if output_path.exists():
                        # 更新 image_path 为绝对路径（与原始数据格式一致）
                        record.image_path = str(output_path)
                        logger.debug("Generated image for problem %s at %s", record.pid, output_path)
                        success = True
                    else:
                        error_msg = "Code executed but no image file created"
                        error_history.append({
                            "attempt": attempt + 1,
                            "error": error_msg,
                            "code": current_code
                        })
                        logger.warning("Image generation did not create file for problem %s (attempt %d)", 
                                     record.pid, attempt + 1)
                except Exception as exc:  # pragma: no cover - execution dependent
                    error_message = f"{type(exc).__name__}: {str(exc)}"
                    error_history.append({
                        "attempt": attempt + 1,
                        "error": error_message,
                        "code": current_code
                    })
                    logger.warning("Failed to render image for problem %s (attempt %d/%d): %s", 
                                 record.pid, attempt + 1, max_retries + 1, error_message)
                    
                    if output_path.exists():
                        output_path.unlink(missing_ok=True)
                    
                    # 如果还有重试机会，调用 Gemini 修复代码
                    if attempt < max_retries:
                        logger.info("Asking Gemini to fix the code for problem %s", record.pid)
                        fixed_code = self.gemini_generator.fix_image_code(
                            question=record.question,
                            answer=record.answer,
                            original_code=current_code,
                            error_message=error_message,
                        )
                        
                        if fixed_code:
                            logger.info("Got fixed code from Gemini for problem %s, retrying...", record.pid)
                            current_code = fixed_code
                            # 更新记录中的代码
                            record.image_code = fixed_code
                        else:
                            logger.warning("Gemini could not fix the code for problem %s", record.pid)
                            break  # Gemini 无法修复，放弃重试
                finally:
                    plt.savefig = original_savefig  # type: ignore[assignment]
                    Figure.savefig = original_fig_savefig  # type: ignore[assignment]
                    plt.close("all")
                
                if success:
                    break
            
            # 如果所有尝试都失败了，标记为需要删除并记录详细信息
            if not success:
                logger.error("All attempts failed for problem %s, will remove this problem from dataset", record.pid)
                failed_pids.append(record.pid)
                
                # 记录失败问题的完整信息
                failed_problem = {
                    "id": record.pid,
                    "category_id": record.category_id,
                    "category_name": record.category_name,
                    "question": record.question,
                    "answer": record.answer,
                    "source": record.source,
                    "original_image_code": record.image_code,
                    "error_history": error_history,
                    "total_attempts": len(error_history)
                }
                failed_problems.append(failed_problem)

        if failed_pids:
            logger.warning("Image generation failed for %d problems, they will be removed: %s", 
                         len(failed_pids), ", ".join(failed_pids))
            
            # 保存失败问题的记录
            self._save_failed_problems(dataset_dir, failed_problems)
        
        logger.info("Image generation completed: %d succeeded, %d failed", 
                   len(image_records) - len(failed_pids), len(failed_pids))
        
        return failed_pids
    
    def _save_failed_problems(self, dataset_dir: Path, failed_problems: List[Dict]) -> None:
        """保存失败的问题到文件"""
        if not failed_problems:
            return
        
        failed_file = dataset_dir / "failed_problems.json"
        
        # 如果文件已存在，追加而不是覆盖
        existing_failed = []
        if failed_file.exists():
            try:
                with failed_file.open("r", encoding="utf-8") as f:
                    existing_failed = json.load(f)
            except Exception as exc:
                logger.warning("Could not load existing failed problems: %s", exc)
        
        # 合并新旧失败记录
        all_failed = existing_failed + failed_problems
        
        # 保存
        try:
            with failed_file.open("w", encoding="utf-8") as f:
                json.dump(all_failed, f, ensure_ascii=False, indent=2)
            logger.info("Saved %d failed problems to %s", len(failed_problems), failed_file)
        except Exception as exc:
            logger.error("Failed to save failed problems: %s", exc)

    def _quality_check_generated_problems(
        self,
        dataset_dir: Path,
        records: List[ProblemRecord],
        model_dir: Path,
        round_index: int,
    ) -> List[ProblemRecord]:
        """
        使用当前轮训练的模型测试新生成的题目，每题测5次，过滤掉太简单或太难的题目
        
        过滤规则：
        - 5次全对（太简单）-> 过滤
        - 5次全错（太难）-> 过滤
        - 1-4次正确（难度合适）-> 保留
        
        注意：直接使用训练后的模型进行测试，通过简单的字符串匹配判断正确性
        
        Args:
            dataset_dir: 数据集目录
            records: 新生成的问题记录列表
            model_dir: 当前轮训练的模型目录
            round_index: 当前轮次
            
        Returns:
            通过质量检测的问题列表
        """
        if not records:
            return []
        
        logger.info("Starting quality check for %d newly generated problems", len(records))
        
        # 1. 创建测试数据集（每题重复5次）
        quality_check_dir = dataset_dir / "quality_check"
        quality_check_dir.mkdir(exist_ok=True)
        
        test_data = []
        pid_to_record = {}
        
        for record in records:
            pid_to_record[record.pid] = record
            
            # 每题重复5次，每次使用不同的 ID 后缀
            for attempt in range(1, 6):
                test_id = f"{record.pid}_attempt{attempt}"
                
                # 确保 image_path 是列表格式（LLaMA-Factory 要求）
                image_path_list = [record.image_path] if record.image_path else []
                
                # 统一将 test_id 转换为整数（用于 Alpaca 格式的 id 字段）
                if record.pid.isdigit():
                    id_value = int(record.pid) * 10 + attempt  # pid=123, attempt=1 -> 1231
                else:
                    # 对于非纯数字 pid，使用 hash
                    id_value = (hash(record.pid) % (10**8)) * 10 + attempt
                
                # 修复：确保 image_path 和 <image> 标签匹配
                question_text = record.question
                if image_path_list and "<image>" not in question_text:
                    # 有图片但缺少 <image> 标签 -> 在开头插入标签
                    question_text = "<image> " + question_text
                elif not image_path_list and "<image>" in question_text:
                    # 没有图片但有 <image> 标签 -> 移除标签
                    question_text = question_text.replace("<image>", "").strip()
                
                # ✅ 使用 Alpaca 格式（与训练数据一致）
                test_data.append({
                    "id": id_value,
                    "problem_id": record.pid,  # 添加 problem_id 字段
                    "problem": question_text,
                    "answer": record.answer,
                    "image_path": image_path_list,
                    "category": record.category_id,
                    "category_name": record.category_name,
                    # 注意：不添加 system 字段，因为 llmasgenerator.py 已经有 PROMPT_TEMPLATE
                    # 元数据字段用于后续分析
                    "_metadata": {
                        "test_id": test_id,
                        "original_pid": record.pid,
                        "attempt": attempt,
                    }
                })
        
        # 保存测试数据集
        test_file = quality_check_dir / "quality_check_test.json"
        with test_file.open("w", encoding="utf-8") as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        
        logger.info("Created quality check dataset with %d test instances (%d problems × 5 attempts)", 
                   len(test_data), len(records))
        
        # 2. 创建 dataset_info.json（使用 Alpaca 格式）
        dataset_info = {
            "quality_check_test": {
                "file_name": test_file.name,
                "formatting": "alpaca",
                "columns": {
                    "prompt": "problem",
                    "response": "answer",
                    "images": "image_path",
                    "system": "system"
                },
            }
        }
        
        dataset_info_file = quality_check_dir / "dataset_info.json"
        with dataset_info_file.open("w", encoding="utf-8") as f:
            json.dump(dataset_info, f, ensure_ascii=False, indent=2)
        
        # 3. 创建评估配置 YAML
        eval_yaml_path = quality_check_dir / f"quality_check_round_{round_index}.yaml"
        eval_config = {
            "model_name_or_path": str(model_dir),
            "stage": "sft",
            "do_predict": True,
            "finetuning_type": "lora",
            "dataset_dir": str(quality_check_dir),
            "dataset": "quality_check_test",
            "eval_dataset": "quality_check_test",  # 添加 eval_dataset 参数（LLaMA-Factory 要求）
            "template": "qwen2_vl",
            "cutoff_len": 32000,
            "max_samples": len(test_data),
            "overwrite_cache": True,
            "preprocessing_num_workers": 16,
            "output_dir": str(quality_check_dir / "eval_output"),
            "per_device_eval_batch_size": 1,
            "predict_with_generate": True,
            "temperature": 0.6,
            "top_p": 0.9,
            "max_new_tokens": 8192,
            "bf16": True,
            "ddp_timeout": 180000000,
            "val_size": 0.0,
            "eval_num_beams": 1,
            "vllm_maxlen": 32000,
        }
        
        with eval_yaml_path.open("w", encoding="utf-8") as f:
            yaml.dump(eval_config, f, allow_unicode=True, default_flow_style=False)
        
        logger.info("Created quality check evaluation config: %s", eval_yaml_path)
        
        # 4. 运行评估（使用 vLLM 代替 LLaMA Factory，速度快10-20倍）
        logger.info("Running quality check evaluation with vLLM (this may take 2-5 minutes)...")
        
        # 使用 vLLM 进行批量生成（类似 _generate_model_answers）
        predictions_path = quality_check_dir / "eval_output" / "generated_predictions.json"
        predictions_path.parent.mkdir(parents=True, exist_ok=True)
        
        llm_generator_script = Path(__file__).parent.parent / "processors" / "llmasgenerator.py"
        
        # 检查是否使用 Apptainer 容器
        use_apptainer = os.getenv("USE_APPTAINER_FOR_VLLM", "false").lower() == "true"
        apptainer_image = os.getenv("APPTAINER_IMAGE", "")
        
        if use_apptainer and apptainer_image:
            vllm_cmd = [
                "apptainer", "exec", "--nv",
                "--cleanenv",
                "--bind", "/share:/share,/mnt:/mnt",
                "--env", "TRITON_CACHE_DIR=/tmp/triton_cache",
                "--env", "PATH=/opt/py312/bin:/usr/local/cuda/bin:/usr/bin:/bin",
                "--env", "CC=/usr/bin/gcc",
                "--env", "CXX=/usr/bin/g++",
                "--env", "LD_LIBRARY_PATH=/usr/local/cuda/lib64:/.singularity.d/libs",
                "--env", "CUDA_HOME=/usr/local/cuda",
                "--env", f"PYTHONPATH={Path(__file__).parent.parent.parent}",
                apptainer_image,
                "python", str(llm_generator_script),
                "--input", str(test_file),
                "--output", str(predictions_path),
                "--model-path", str(model_dir),
                "--question-key", "problem",
                "--answer-key", "predict",  # 输出到 predict 字段（兼容后续处理）
                "--image-key", "image_path",
                "--batch-size", "8",  # 批量处理，提速
                "--temperature", "0.6",
                "--top-p", "0.9",
                "--max-tokens", "16384",  # 增加到 16K 以避免截断
            ]
        else:
            vllm_cmd = [
                "python", str(llm_generator_script),
                "--input", str(test_file),
                "--output", str(predictions_path),
                "--model-path", str(model_dir),
                "--question-key", "problem",
                "--answer-key", "predict",  # 输出到 predict 字段（兼容后续处理）
                "--image-key", "image_path",
                "--batch-size", "8",  # 批量处理，提速
                "--temperature", "0.6",
                "--top-p", "0.9",
                "--max-tokens", "16384",  # 增加到 16K 以避免截断
            ]
        
        if hasattr(self.config, 'quality_check_tensor_parallel_size'):
            vllm_cmd.extend(["--tensor-parallel-size", str(self.config.quality_check_tensor_parallel_size)])
        elif hasattr(self.config, 'eval_tensor_parallel_size'):
            vllm_cmd.extend(["--tensor-parallel-size", str(self.config.eval_tensor_parallel_size)])
        
        logger.info("Running vLLM quality check: %s", " ".join(map(str, vllm_cmd)))
        
        try:
            result = subprocess.run(
                vllm_cmd,
                check=True,
                stdout=None,
                stderr=None,
                text=True,
            )
            logger.info("✅ vLLM quality check completed successfully")
        except subprocess.CalledProcessError as exc:
            logger.error("❌ vLLM quality check failed with exit code %d", exc.returncode)
            logger.warning("Quality check evaluation failed, keeping all problems")
            return records
        
        # 5. 分析预测结果
        if not predictions_path.exists():
            logger.warning("Quality check predictions not found, keeping all problems")
            return records
        
        # 读取预测结果（JSON 数组格式）
        predictions = []
        try:
            with predictions_path.open("r", encoding="utf-8") as f:
                predictions = json.load(f)
                logger.debug("Loaded %d predictions from JSON file", len(predictions))
        except Exception as exc:
            logger.error("Failed to read quality check predictions: %s", exc)
            # 如果失败，尝试跳过质量检测，保留所有题目
            logger.warning("Quality check predictions read failed, keeping all problems")
            return records
        
        # 6. 使用 LLM Judge 评估预测结果（与正式评测相同）
        logger.info("Evaluating predictions with LLM Judge...")
        
        # 准备评估数据
        eval_records_for_judge = []
        for record in records:
            eval_records_for_judge.append({
                "pid": record.pid,
                "question": record.question,
                "answer": record.answer,
                "category_id": record.category_id,
                "category_name": record.category_name,
            })
        
        # 保存评估记录供 Judge 使用
        eval_records_file = quality_check_dir / "eval_records.json"
        with eval_records_file.open("w", encoding="utf-8") as f:
            json.dump(eval_records_for_judge, f, ensure_ascii=False, indent=2)
        
        # 调用 LLM Judge（使用独立进程，与正式评测相同）
        judge_output_file = quality_check_dir / "judge_results.json"
        judge_config = {
            "model_path": self.config.judge_model_path,
            "tensor_parallel_size": self.config.judge_tensor_parallel_size,
            "gpu_memory_utilization": self.config.judge_gpu_memory_utilization,
            "temperature": self.config.judge_temperature,
            "max_tokens": self.config.judge_max_tokens,
            "max_model_len": self.config.judge_max_model_len,  # 添加 max_model_len 配置
        }
        
        judge_cmd = [
            "python",
            str(Path(__file__).parent.parent / "processors" / "run_llm_judge.py"),
            str(eval_records_file),
            str(predictions_path),
            str(judge_output_file),
            json.dumps(judge_config),
        ]
        
        logger.info("Running LLM Judge for quality check (this may take 5-15 minutes)...")
        judge_success = self._run_llamafactory_command(judge_cmd)
        
        if not judge_success or not judge_output_file.exists():
            logger.warning("LLM Judge failed for quality check, keeping all problems")
            return records
        
        # 读取 Judge 结果
        try:
            with judge_output_file.open("r", encoding="utf-8") as f:
                judge_results = json.load(f)
            eval_data = judge_results["eval_data"]
            judge_stats = judge_results["judge_stats"]
            
            logger.info("LLM Judge stats: %s", judge_stats)
        except Exception as exc:
            logger.error("Failed to read Judge results: %s", exc)
            return records
        
        # 建立 pid 到 record 的映射
        pid_to_test_items = {}
        for test_item in test_data:
            original_pid = test_item["_metadata"]["original_pid"]
            attempt = test_item["_metadata"]["attempt"]
            if original_pid not in pid_to_test_items:
                pid_to_test_items[original_pid] = {}
            pid_to_test_items[original_pid][attempt] = test_item
        
        # 按原始 pid 分组统计（使用 Judge 的评估结果）
        pid_attempts = {}
        
        for eval_item in eval_data:
            # 从 eval_item 中获取 pid 和 attempt（llmasjudge 会保留这些字段）
            original_pid = eval_item.get("pid", "")
            attempt = eval_item.get("attempt", 1)
            
            if not original_pid:
                logger.warning("Eval item missing pid field")
                continue
            
            if original_pid not in pid_to_record:
                logger.warning("Original pid %s not found in records", original_pid)
                continue
            
            record = pid_to_record[original_pid]
            
            # 使用 Judge 的评估结果（字段名是 'matched'，不是 'judge_result'）
            is_correct = eval_item.get("matched", None)
            
            if is_correct is None:
                logger.warning("Problem %s (attempt %d) has no judge result (matched field)", original_pid, attempt)
                is_correct = False
            
            if original_pid not in pid_attempts:
                pid_attempts[original_pid] = []
            
            pid_attempts[original_pid].append({
                "attempt": attempt,
                "prediction": eval_item.get("model_prediction", ""),
                "label": record.answer,
                "correct": is_correct,
                "judge_analysis": eval_item.get("match_analysis", ""),
            })
        
        # 7. 根据正确率过滤题目，并对太简单的题目进行难度升级
        qualified_records = []
        filtered_too_easy = []  # 记录原始的太简单题目
        filtered_too_hard = []
        upgraded_problems = []  # 记录难度升级后的新题目
        
        for record in records:
            attempts = pid_attempts.get(record.pid, [])
            
            if len(attempts) < 5:
                logger.warning("Problem %s has only %d attempts, keeping it", record.pid, len(attempts))
                qualified_records.append(record)
                continue
            
            correct_count = sum(1 for a in attempts if a["correct"])
            
            if correct_count == 0:
                # 5次全错，太难
                filtered_too_hard.append(record.pid)
                logger.info("❌ Filtered (too hard): %s - 0/5 correct", record.pid)
            elif correct_count == 5:
                # 5次全对，太简单 → 需要难度升级
                filtered_too_easy.append(record.pid)
                logger.info("⚠️  Too easy: %s - 5/5 correct, will upgrade difficulty", record.pid)
                
                # 记录需要升级的题目
                upgraded_problems.append(record)
            else:
                # 1-4次正确，难度合适
                qualified_records.append(record)
                logger.info("✅ Qualified: %s - %d/5 correct", record.pid, correct_count)
        
        # 8. 对太简单的题目进行难度升级（迭代升级直到难度合适）
        max_upgrade_iterations = 3  # 最多升级 3 次，避免无限循环
        upgrade_iteration = 0
        
        while upgraded_problems and upgrade_iteration < max_upgrade_iterations:
            upgrade_iteration += 1
            logger.info("\n🔄 Upgrade iteration %d: Processing %d problems that were too easy...", 
                       upgrade_iteration, len(upgraded_problems))
            
            # 升级难度
            upgraded_records = self._upgrade_problem_difficulty(
                upgraded_problems, dataset_dir, round_index
            )
            
            if not upgraded_records:
                logger.warning("Failed to upgrade any problems in iteration %d", upgrade_iteration)
                break
            
            logger.info("✅ Successfully upgraded %d problems", len(upgraded_records))
            
            # 为升级后的题目生成图片
            logger.info("Generating images for upgraded problems...")
            failed_pids = self._materialise_problem_images(dataset_dir, upgraded_records)
            if failed_pids:
                logger.warning("%d upgraded problems failed image generation", len(failed_pids))
                # 移除图片生成失败的题目
                upgraded_records = [r for r in upgraded_records if r.pid not in failed_pids]
            
            if not upgraded_records:
                logger.warning("All upgraded problems failed image generation")
                break
            
            # 为升级后的题目生成 COT 答案
            logger.info("Generating COT answers for upgraded problems...")
            self._generate_model_answers(dataset_dir, upgraded_records)
            
            # 如果这不是最后一次迭代，测试升级后的题目
            if upgrade_iteration < max_upgrade_iterations:
                logger.info("Testing upgraded problems to check if they are still too easy...")
                
                # 重新测试升级后的题目（使用相同的质量检测流程）
                still_too_easy = self._test_upgraded_problems(
                    dataset_dir, upgraded_records, model_dir, round_index, quality_check_dir
                )
                
                if still_too_easy:
                    logger.info("⚠️  %d upgraded problems are still too easy, will upgrade again", 
                               len(still_too_easy))
                    # 准备下一轮升级
                    upgraded_problems = still_too_easy
                    # 将已经合适的题目添加到合格列表
                    qualified_upgraded = [r for r in upgraded_records if r not in still_too_easy]
                    if qualified_upgraded:
                        qualified_records.extend(qualified_upgraded)
                        logger.info("Added %d qualified upgraded problems to training set", 
                                   len(qualified_upgraded))
                else:
                    logger.info("✅ All upgraded problems now have appropriate difficulty")
                    qualified_records.extend(upgraded_records)
                    logger.info("Added %d upgraded problems to qualified list", len(upgraded_records))
                    break
            else:
                # 最后一次迭代，不再测试，直接加入
                logger.info("Reached max upgrade iterations (%d), accepting all upgraded problems", 
                           max_upgrade_iterations)
                qualified_records.extend(upgraded_records)
                logger.info("Added %d upgraded problems to qualified list", len(upgraded_records))
                break
        
        # 8. 保存质量检测报告（包含 Judge 统计信息）
        quality_report = {
            "round": round_index,
            "total_problems": len(records),
            "qualified_problems": len(qualified_records),
            "filtered_too_easy": len(filtered_too_easy),
            "filtered_too_hard": len(filtered_too_hard),
            "filtered_too_easy_pids": filtered_too_easy,
            "filtered_too_hard_pids": filtered_too_hard,
            "judge_stats": judge_stats if 'judge_stats' in locals() else {},
            "details": {
                pid: {
                    "attempts": attempts,
                    "correct_count": sum(1 for a in attempts if a["correct"]),
                    "qualified": pid in [r.pid for r in qualified_records],
                }
                for pid, attempts in pid_attempts.items()
            },
        }
        
        report_file = quality_check_dir / f"quality_report_round_{round_index}.json"
        with report_file.open("w", encoding="utf-8") as f:
            json.dump(quality_report, f, ensure_ascii=False, indent=2)
        
        logger.info("Quality check report saved to: %s", report_file)
        logger.info("Quality check summary:")
        logger.info("  Total: %d problems", len(records))
        logger.info("  Qualified: %d problems (1-4/5 correct)", len(qualified_records))
        logger.info("  Filtered (too easy): %d problems (5/5 correct)", len(filtered_too_easy))
        logger.info("  Filtered (too hard): %d problems (0/5 correct)", len(filtered_too_hard))
        
        return qualified_records
    
    def _test_upgraded_problems(
        self,
        dataset_dir: Path,
        upgraded_records: List[ProblemRecord],
        model_dir: Path,
        round_index: int,
        quality_check_dir: Path,
    ) -> List[ProblemRecord]:
        """
        测试升级后的题目是否仍然太简单（使用 vLLM）
        
        Args:
            dataset_dir: 数据集目录
            upgraded_records: 升级后的题目列表
            model_dir: 当前轮训练后的模型目录
            round_index: 当前轮次
            quality_check_dir: 质量检测目录
            
        Returns:
            仍然太简单的题目列表（5/5 正确）
        """
        logger.info("Testing %d upgraded problems with current model (5 attempts each) using vLLM...", 
                   len(upgraded_records))
        
        # 1. 创建测试数据集（每题 5 次，不需要 system 字段，因为 vLLM 推理）
        test_data = []
        pid_to_record = {record.pid: record for record in upgraded_records}
        
        for record in upgraded_records:
            for attempt in range(1, 6):  # 1-5
                test_id = f"{record.pid}_attempt{attempt}"
                
                # 处理图片路径
                image_path_list = []
                if record.image_path:
                    img_path = Path(record.image_path)
                    if not img_path.is_absolute():
                        img_path = dataset_dir / record.image_path
                    if img_path.exists():
                        image_path_list = [str(img_path)]
                
                # 确保 <image> 标签正确
                question_text = record.question
                if image_path_list and "<image>" not in question_text:
                    question_text = "<image> " + question_text
                elif not image_path_list and "<image>" in question_text:
                    question_text = question_text.replace("<image>", "").strip()
                
                test_data.append({
                    "id": hash(test_id) % (10**8),
                    "problem_id": record.pid,  # 添加 problem_id 字段
                    "problem": question_text,
                    "answer": record.answer,
                    "image_path": image_path_list,
                    "category": record.category_id,
                    "category_name": record.category_name,
                    "_metadata": {
                        "test_id": test_id,
                        "original_pid": record.pid,
                        "attempt": attempt,
                    }
                })
        
        # 2. 保存测试数据集
        test_file = quality_check_dir / f"upgraded_test_iteration.json"
        with test_file.open("w", encoding="utf-8") as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        
        logger.info("Saved upgraded test data: %s (%d instances)", test_file, len(test_data))
        
        # 3. 使用 vLLM 进行批量生成
        predictions_path = quality_check_dir / "upgraded_eval_output" / "generated_predictions.json"
        predictions_path.parent.mkdir(parents=True, exist_ok=True)
        
        llm_generator_script = Path(__file__).parent.parent / "processors" / "llmasgenerator.py"
        
        # 检查是否使用 Apptainer 容器
        use_apptainer = os.getenv("USE_APPTAINER_FOR_VLLM", "false").lower() == "true"
        apptainer_image = os.getenv("APPTAINER_IMAGE", "")
        
        if use_apptainer and apptainer_image:
            vllm_cmd = [
                "apptainer", "exec", "--nv",
                "--cleanenv",
                "--bind", "/share:/share,/mnt:/mnt",
                "--env", "TRITON_CACHE_DIR=/tmp/triton_cache",
                "--env", "PATH=/opt/py312/bin:/usr/local/cuda/bin:/usr/bin:/bin",
                "--env", "CC=/usr/bin/gcc",
                "--env", "CXX=/usr/bin/g++",
                "--env", "LD_LIBRARY_PATH=/usr/local/cuda/lib64:/.singularity.d/libs",
                "--env", "CUDA_HOME=/usr/local/cuda",
                "--env", f"PYTHONPATH={Path(__file__).parent.parent.parent}",
                apptainer_image,
                "python", str(llm_generator_script),
                "--input", str(test_file),
                "--output", str(predictions_path),
                "--model-path", str(model_dir),
                "--question-key", "problem",
                "--answer-key", "predict",
                "--image-key", "image_path",
                "--batch-size", "8",
                "--temperature", "0.6",
                "--top-p", "0.9",
                "--max-tokens", "16384",  # 增加到 16K 以避免截断
            ]
        else:
            vllm_cmd = [
                "python", str(llm_generator_script),
                "--input", str(test_file),
                "--output", str(predictions_path),
                "--model-path", str(model_dir),
                "--question-key", "problem",
                "--answer-key", "predict",
                "--image-key", "image_path",
                "--batch-size", "8",
                "--temperature", "0.6",
                "--top-p", "0.9",
                "--max-tokens", "16384",  # 增加到 16K 以避免截断
            ]
        
        if hasattr(self.config, 'quality_check_tensor_parallel_size'):
            vllm_cmd.extend(["--tensor-parallel-size", str(self.config.quality_check_tensor_parallel_size)])
        elif hasattr(self.config, 'eval_tensor_parallel_size'):
            vllm_cmd.extend(["--tensor-parallel-size", str(self.config.eval_tensor_parallel_size)])
        
        logger.info("Running vLLM for upgraded test: %s", " ".join(map(str, vllm_cmd)))
        
        try:
            result = subprocess.run(
                vllm_cmd,
                check=True,
                stdout=None,
                stderr=None,
                text=True,
            )
            logger.info("✅ vLLM upgraded test completed successfully")
        except subprocess.CalledProcessError as exc:
            logger.error("❌ vLLM upgraded test failed with exit code %d", exc.returncode)
            logger.warning("Upgraded problems evaluation failed, assuming all are qualified")
            return []
        
        # 4. 验证输出文件
        if not predictions_path.exists():
            logger.warning("Upgraded test predictions not found, assuming all are qualified")
            return []
        
        # 5. 使用 LLM Judge 评估
        eval_records_for_judge = []
        for record in upgraded_records:
            eval_records_for_judge.append({
                "pid": record.pid,
                "question": record.question,
                "answer": record.answer,
                "category_id": record.category_id,
                "category_name": record.category_name,
            })
        
        eval_records_file = quality_check_dir / "upgraded_eval_records.json"
        with eval_records_file.open("w", encoding="utf-8") as f:
            json.dump(eval_records_for_judge, f, ensure_ascii=False, indent=2)
        
        judge_output_file = quality_check_dir / "upgraded_judge_results.json"
        judge_config = {
            "model_path": self.config.judge_model_path,
            "tensor_parallel_size": self.config.judge_tensor_parallel_size,
            "gpu_memory_utilization": self.config.judge_gpu_memory_utilization,
            "temperature": self.config.judge_temperature,
            "max_tokens": self.config.judge_max_tokens,
            "max_model_len": self.config.judge_max_model_len,  # 添加 max_model_len 配置
        }
        
        judge_cmd = [
            "python",
            str(Path(__file__).parent.parent / "processors" / "run_llm_judge.py"),
            str(eval_records_file),
            str(predictions_path),
            str(judge_output_file),
            json.dumps(judge_config),
        ]
        
        logger.info("Running LLM Judge on upgraded problems...")
        judge_success = self._run_llamafactory_command(judge_cmd)
        
        if not judge_success or not judge_output_file.exists():
            logger.warning("LLM Judge failed for upgraded test, assuming all are qualified")
            return []
        
        # 6. 分析结果（与质量检测相同的逻辑）
        try:
            with judge_output_file.open("r", encoding="utf-8") as f:
                judge_results = json.load(f)
            eval_data = judge_results["eval_data"]
        except Exception as exc:
            logger.error("Failed to read upgraded Judge results: %s", exc)
            return []
        
        # 建立索引映射
        index_to_info = {}
        for idx, test_item in enumerate(test_data):
            index_to_info[idx] = {
                "pid": test_item["_metadata"]["original_pid"],
                "attempt": test_item["_metadata"]["attempt"],
            }
        
        # 按 pid 统计正确率
        pid_correct_counts = {}
        
        for idx, eval_item in enumerate(eval_data):
            if idx not in index_to_info:
                continue
            
            info = index_to_info[idx]
            original_pid = info["pid"]
            
            # 修复：检查 matched 字段（布尔值），而不是 judge_result 字段
            is_correct = eval_item.get("matched", False)
            
            if original_pid not in pid_correct_counts:
                pid_correct_counts[original_pid] = 0
            
            if is_correct:
                pid_correct_counts[original_pid] += 1
        
        # 找出仍然太简单的题目（5/5 正确）
        still_too_easy = []
        for record in upgraded_records:
            correct_count = pid_correct_counts.get(record.pid, 0)
            if correct_count == 5:
                logger.info("⚠️  Upgraded problem %s is still too easy: 5/5 correct", record.pid)
                still_too_easy.append(record)
            else:
                logger.info("✅ Upgraded problem %s has appropriate difficulty: %d/5 correct", 
                          record.pid, correct_count)
        
        return still_too_easy
    
    def _upgrade_problem_difficulty(
        self,
        easy_problems: List[ProblemRecord],
        dataset_dir: Path,
        round_index: int,
    ) -> List[ProblemRecord]:
        """
        对太简单的题目进行难度升级
        
        根据题目的 source 判断类型（reasoning 或 visual），
        调用 Gemini 生成难度升级版本
        
        Args:
            easy_problems: 太简单的题目列表
            dataset_dir: 数据集目录
            round_index: 当前轮次
            
        Returns:
            升级后的题目列表
        """
        upgraded_records = []
        upgrade_details = []
        
        logger.info("Starting difficulty upgrade for %d problems", len(easy_problems))
        
        for record in easy_problems:
            try:
                # 判断题目类型（reasoning 或 visual）
                if "reasoning" in record.source.lower():
                    difficulty_aspect = "reasoning"
                elif "visual" in record.source.lower():
                    difficulty_aspect = "visual"
                else:
                    # 默认为 reasoning
                    difficulty_aspect = "reasoning"
                    logger.warning("Cannot determine difficulty type for %s (source: %s), defaulting to reasoning",
                                 record.pid, record.source)
                
                logger.info("Upgrading problem %s (%s difficulty)", record.pid, difficulty_aspect)
                
                # 调用 Gemini 升级难度
                upgraded = self.gemini_generator.upgrade_problem_difficulty(
                    problem=record.question,
                    answer=record.answer,
                    image_path=record.image_path,
                    category_id=record.category_id,
                    category_name=record.category_name,
                    difficulty_aspect=difficulty_aspect,  # "reasoning" 或 "visual"
                )
                
                if not upgraded:
                    logger.warning("Failed to upgrade problem %s", record.pid)
                    continue
                
                # 分配新的 ID
                new_id = self.next_generated_id
                self.next_generated_id += 1
                new_pid = str(new_id)
                
                # 创建新的记录
                new_record = ProblemRecord(
                    pid=new_pid,
                    category_id=record.category_id,
                    category_name=record.category_name,
                    question=upgraded.get("question", "").strip(),
                    answer=upgraded.get("answer", "").strip(),
                    image_path="",  # 稍后生成图片
                    source=f"{record.source}_upgraded",  # 标记为升级版本
                    image_code=upgraded.get("image_code", ""),
                )
                
                upgraded_records.append(new_record)
                
                # 记录升级详情（完整保存，不截断）
                upgrade_details.append({
                    "original_pid": record.pid,
                    "new_pid": new_pid,
                    "difficulty_aspect": difficulty_aspect,
                    "original_question": record.question,  # 保存完整问题
                    "new_question": new_record.question,    # 保存完整问题
                    "original_answer": record.answer,       # 保存完整答案
                    "new_answer": new_record.answer,        # 保存完整答案
                })
                
                logger.info("✅ Upgraded problem %s → %s (%s)", 
                          record.pid, new_pid, difficulty_aspect)
                
            except Exception as exc:
                logger.error("Failed to upgrade problem %s: %s", record.pid, exc)
        
        # 保存升级记录
        if upgrade_details:
            upgrade_file = dataset_dir / f"upgraded_problems_round_{round_index}.json"
            try:
                with upgrade_file.open("w", encoding="utf-8") as f:
                    json.dump({
                        "round": round_index,
                        "total_upgraded": len(upgrade_details),
                        "upgrades": upgrade_details
                    }, f, ensure_ascii=False, indent=2)
                logger.info("Saved upgrade details to: %s", upgrade_file)
            except Exception as exc:
                logger.error("Failed to save upgrade details: %s", exc)
        
        return upgraded_records
    
    def _check_answer_correctness(self, prediction: str, ground_truth: str) -> bool:
        """
        检查预测答案是否正确（简单的字符串匹配）
        
        从预测的 COT 答案中提取最终答案，与标准答案对比
        
        Args:
            prediction: 模型的预测结果（COT格式）
            ground_truth: 标准答案
            
        Returns:
            是否正确
        """
        import re
        
        # 标准化答案：去除空格、转小写
        def normalize(text):
            if not text:
                return ""
            # 去除多余空格
            text = " ".join(text.split())
            # 转小写
            text = text.lower()
            return text
        
        pred_normalized = normalize(prediction)
        gt_normalized = normalize(ground_truth)
        
        # 方法1: 直接包含检查（最宽松）
        if gt_normalized in pred_normalized:
            return True
        
        # 方法2: 提取"答案是"、"answer is"等模式后的内容
        answer_patterns = [
            r"答案是[：:]\s*(.+?)(?:\n|$|。)",
            r"答案是\s*(.+?)(?:\n|$|。)",
            r"answer\s*is[：:]\s*(.+?)(?:\n|$|\.|,)",
            r"answer[：:]\s*(.+?)(?:\n|$|\.|,)",
            r"因此[，,]\s*(.+?)(?:\n|$|。)",
            r"所以[，,]\s*(.+?)(?:\n|$|。)",
            r"最终答案[为是][：:]\s*(.+?)(?:\n|$|。)",
        ]
        
        for pattern in answer_patterns:
            match = re.search(pattern, pred_normalized, re.IGNORECASE)
            if match:
                extracted_answer = normalize(match.group(1).strip())
                # 检查提取的答案是否包含标准答案
                if gt_normalized in extracted_answer or extracted_answer in gt_normalized:
                    return True
                # 精确匹配
                if extracted_answer == gt_normalized:
                    return True
        
        # 方法3: 提取数字进行比较（适用于数值答案）
        def extract_numbers(text):
            # 提取所有数字（包括小数）
            numbers = re.findall(r'-?\d+\.?\d*', text)
            return [float(n) for n in numbers if n]
        
        pred_numbers = extract_numbers(pred_normalized)
        gt_numbers = extract_numbers(gt_normalized)
        
        if pred_numbers and gt_numbers:
            # 如果预测中包含标准答案的数字
            for gt_num in gt_numbers:
                if any(abs(pred_num - gt_num) < 1e-6 for pred_num in pred_numbers):
                    return True
        
        # 方法4: 如果都失败，返回 False
        return False

    def _check_answer_consistency(self, cot_answer: str, gemini_answer: str) -> bool:
        """
        检查 COT 答案和 Gemini 答案是否一致
        
        使用 llmasjudge 的提取函数来提取和比较答案：
        1. 从 COT 答案提取最终答案（\boxed{}, <answer>, "answer is" 等）
        2. 从 Gemini 答案提取最终答案（支持 Gemini 特有的格式）
        3. 比较两者是否一致
        
        Args:
            cot_answer: Qwen3-VL-30B 生成的 COT 答案（包含 \boxed{}）
            gemini_answer: Gemini 生成的简短答案
            
        Returns:
            True 如果答案一致，False 如果不一致
        """
        import re
        
        # 导入 llmasjudge 的提取函数
        try:
            from ..processors.llmasjudge import extract_solution_text
        except ImportError:
            # 如果导入失败，回退到原来的方法
            logger.warning("Failed to import extract_solution_text from llmasjudge, using fallback method")
            return self._check_answer_correctness(cot_answer, gemini_answer)
        
        # 标准化函数
        def normalize(text):
            if not text:
                return ""
            text = " ".join(text.split())
            text = text.lower()
            # 移除 markdown 格式
            text = re.sub(r'\*\*', '', text)
            return text
        
        # 1. 使用 llmasjudge 的函数提取 COT 答案
        cot_extracted = extract_solution_text(cot_answer)
        cot_normalized = normalize(cot_extracted) if cot_extracted else ""
        
        # 2. 使用 llmasjudge 的函数提取 Gemini 答案
        gemini_extracted = extract_solution_text(gemini_answer)
        gemini_normalized = normalize(gemini_extracted) if gemini_extracted else ""
        
        # 3. 如果 llmasjudge 提取失败（返回完整文本），尝试用 Gemini 特有的模式提取
        # 检测是否提取失败：如果提取结果太长（>100字符），说明没有成功提取
        if len(gemini_normalized) > 100:
            # Gemini 常见模式："Therefore, only X", "Thus, X is", "答案为 X" 等
            gemini_specific_patterns = [
                r'therefore[,\s]+(?:only\s+)?(?:shape\s+|option\s+)?([a-z0-9]+)\s+is',
                r'thus[,\s]+(?:only\s+)?(?:shape\s+|option\s+)?([a-z0-9]+)\s+(?:is|corresponds)',
                r'only\s+(?:shape\s+|option\s+)?([a-z0-9]+)\s+(?:is|can)',
                r'(?:shape|option)\s+([a-z0-9]+)\s+is\s+(?:the\s+)?correct',
                r'答案(?:是|为)[：:\s]*([a-z0-9]+)',
                r'选项?\s*([a-z0-9]+)\s*(?:是|为)(?:正确|答案)',
            ]
            
            for pattern in gemini_specific_patterns:
                match = re.search(pattern, gemini_normalized, re.IGNORECASE)
                if match:
                    gemini_normalized = match.group(1).lower()
                    logger.debug("Extracted from Gemini using specific pattern: '%s'", gemini_normalized)
                    break
        
        logger.debug("Extracted answers - COT: '%s', Gemini: '%s'", 
                    cot_normalized[:50], gemini_normalized[:50])
        
        # 4. 比较提取的答案
        if cot_normalized and gemini_normalized:
            # 精确匹配
            if cot_normalized == gemini_normalized:
                return True
            
            # 包含关系（处理 "A" vs "shape A" 这种情况）
            if cot_normalized in gemini_normalized or gemini_normalized in cot_normalized:
                return True
            
            # 对于单字符/短答案（如选择题），做额外的模糊匹配
            if len(cot_normalized) <= 5 and len(gemini_normalized) <= 20:
                # 查找 "shape X", "option X" 等模式
                pattern = rf'\b{re.escape(cot_normalized)}\b'
                if re.search(pattern, gemini_normalized, re.IGNORECASE):
                    return True
        
        # 5. 如果一个提取成功但另一个失败，尝试在完整文本中查找
        if cot_normalized and len(cot_normalized) <= 10:
            # COT 提取成功（且是短答案），在 Gemini 完整文本中查找
            gemini_full_normalized = normalize(gemini_answer)
            pattern = rf'\b{re.escape(cot_normalized)}\b'
            if re.search(pattern, gemini_full_normalized, re.IGNORECASE):
                return True
        
        # 6. 回退到原来的方法
        return self._check_answer_correctness(cot_answer, gemini_answer)
    
    def _save_inconsistent_answers(self, dataset_dir: Path, inconsistent_details: List[Dict]) -> None:
        """保存答案不一致的题目详情到文件
        
        Args:
            dataset_dir: 数据集目录
            inconsistent_details: 不一致答案的详细信息列表
        """
        if not inconsistent_details:
            return
        
        inconsistent_file = dataset_dir / "inconsistent_answers.json"
        
        # 如果文件已存在，追加而不是覆盖
        existing_inconsistent = []
        if inconsistent_file.exists():
            try:
                with inconsistent_file.open("r", encoding="utf-8") as f:
                    existing_inconsistent = json.load(f)
            except Exception as exc:
                logger.warning("Could not load existing inconsistent answers: %s", exc)
        
        # 合并新旧记录
        all_inconsistent = existing_inconsistent + inconsistent_details
        
        # 保存
        try:
            with inconsistent_file.open("w", encoding="utf-8") as f:
                json.dump(all_inconsistent, f, ensure_ascii=False, indent=2)
            logger.info("Saved %d inconsistent answers to %s", 
                       len(inconsistent_details), inconsistent_file)
        except Exception as exc:
            logger.error("Failed to save inconsistent answers: %s", exc)

    def _generate_model_answers(self, dataset_dir: Path, records: List[ProblemRecord]) -> None:
        """使用 Qwen3-VL-30B-A3B-Thinking 为新生成的问题生成 COT 答案，替换原来的简短答案
        
        Args:
            dataset_dir: 数据集目录
            records: 问题记录列表
        """
        # 筛选出需要生成 COT 答案的记录（新生成的问题）
        generated_records = [
            r for r in records 
            if (r.source.startswith("generated_hard") or r.source.startswith("generated_refresh")) and r.image_path
        ]
        
        logger.info("Generating COT answers for %d newly generated problems using Qwen3-VL-30B-A3B-Thinking", 
                   len(generated_records))
        
        # 创建临时 JSON 文件作为 llmasgenerator 的输入
        temp_input_path = dataset_dir / "new_problems_wo_cot.json"
        
        # 将记录转换为 JSON 格式
        temp_data = []
        for record in generated_records:
            temp_data.append({
                "id": record.pid,
                "problem": record.question,
                "answer": record.answer,  # 保留原始简短答案用于参考
                "image_path": record.image_path,
                "category_id": record.category_id,
            })
        
        # 保存临时输入文件
        with temp_input_path.open("w", encoding="utf-8") as f:
            json.dump(temp_data, f, ensure_ascii=False, indent=2)
        
        logger.info("Saved temporary input file for LLM generation: %s", temp_input_path)
        
        output_path = dataset_dir / "new_problems_with_cot.json"
        
        # 构建命令
        llm_generator_script = Path(__file__).parent.parent / "processors" / "llmasgenerator.py"
        
        # 检查是否使用 Apptainer 容器
        use_apptainer = os.getenv("USE_APPTAINER_FOR_VLLM", "false").lower() == "true"
        apptainer_image = os.getenv("APPTAINER_IMAGE", "")
        if use_apptainer and apptainer_image:
            
            cmd = [
                "apptainer", "exec", "--nv",
                "--cleanenv", 
                "--bind", "/share:/share,/mnt:/mnt",
                "--env", "TRITON_CACHE_DIR=/tmp/triton_cache", 
                "--env", "PATH=/opt/py312/bin:/usr/local/cuda/bin:/usr/bin:/bin",
                "--env", "CC=/usr/bin/gcc",
                "--env", "CXX=/usr/bin/g++",
                "--env", "LD_LIBRARY_PATH=/usr/local/cuda/lib64:/.singularity.d/libs",
                "--env", "CUDA_HOME=/usr/local/cuda",
                "--env", "PYTHONPATH=/mnt/petrelfs/shangxiaoran/math_generation",
                apptainer_image,
                "python", str(llm_generator_script),
                "--input", str(temp_input_path),
                "--output", str(output_path),
                "--model-path", self.config.llm_generator_model_path,
                "--question-key", "problem",
                "--answer-key", "model_answer",  # 生成到 model_answer 字段
                "--image-key", "image_path",
                "--batch-size", "1",
                "--temperature", "0.6",
                "--top-p", "0.9",
                "--max-tokens", "32000",
            ]
        else:
            cmd = [
                "python", str(llm_generator_script),
                "--input", str(temp_input_path),
                "--output", str(output_path),
                "--model-path", self.config.llm_generator_model_path,
                "--question-key", "problem",
                "--answer-key", "model_answer",  # 生成到 model_answer 字段
                "--image-key", "image_path",
                "--batch-size", "1",
                "--temperature", "0.6",
                "--top-p", "0.9",
                "--max-tokens", "32000",
            ]
        
        if hasattr(self.config, 'llm_generator_tensor_parallel_size') and self.config.llm_generator_tensor_parallel_size:
            cmd.extend(["--tensor-parallel-size", str(self.config.llm_generator_tensor_parallel_size)])
        
        logger.info("Running LLM generator command: %s", " ".join(cmd))
        
        try:
            result = subprocess.run(
                cmd,
                check=True,
                stdout=None,  
                stderr=None, 
                text=True,
            )
            logger.info("✅ LLM generator completed successfully")
        except subprocess.CalledProcessError as exc:
            logger.error("❌ LLM generator failed with exit code %d", exc.returncode)
            temp_input_path.unlink(missing_ok=True)
            output_path.unlink(missing_ok=True)
            raise RuntimeError("LLM generator failed") from exc
        
        # 读取生成的 COT 答案并替换原来的 answer 字段
        try:
            with output_path.open("r", encoding="utf-8") as f:
                generated_data = json.load(f)
            
            # 创建 pid -> COT answer 的映射
            pid_to_cot_answer = {
                item["id"]: item.get("model_answer", "")
                for item in generated_data
            }
            
            # 用 COT 答案替换原来的简短答案，并对比答案一致性
            updated_count = 0
            inconsistent_pids = []  # 记录答案不一致的题目 ID
            inconsistent_details = []  # 记录详细信息
            
            for record in generated_records:
                if record.pid in pid_to_cot_answer and pid_to_cot_answer[record.pid]:
                    cot_answer = pid_to_cot_answer[record.pid]
                    gemini_answer = record.answer  # Gemini 的原始答案
                    
                    # 检查答案一致性
                    is_consistent = self._check_answer_consistency(cot_answer, gemini_answer)
                    
                    if not is_consistent:
                        logger.warning("Answer inconsistency detected for problem %s", record.pid)
                        logger.warning("  Gemini answer: %s", gemini_answer[:100])
                        logger.warning("  COT answer: %s", cot_answer[:100])
                        inconsistent_pids.append(record.pid)
                        inconsistent_details.append({
                            "pid": record.pid,
                            "question": record.question[:200],  # 前200字符
                            "gemini_answer": gemini_answer,
                            "cot_answer": cot_answer,
                            "category_id": record.category_id,
                            "category_name": record.category_name,
                        })
                        # 标记为需要删除（但暂时保留在 records 中，等待调用者处理）
                        continue
                    
                    # 答案一致，替换为 COT 答案
                    record.answer = cot_answer
                    record.model_answer = cot_answer
                    updated_count += 1
                    logger.debug("Replaced answer for problem %s with COT answer (answers consistent)", record.pid)
            
            logger.info("Updated %d records: replaced short answers with COT answers", updated_count)
            
            if inconsistent_pids:
                logger.warning("Found %d problems with inconsistent answers between Gemini and COT", 
                             len(inconsistent_pids))
                logger.warning("These problems will be filtered out: %s", ", ".join(inconsistent_pids))
                
                # 保存不一致答案的详细记录
                self._save_inconsistent_answers(dataset_dir, inconsistent_details)
                
                # 从 records 列表中移除不一致的题目
                original_count = len(records)
                records[:] = [r for r in records if r.pid not in inconsistent_pids]
                logger.info("Removed %d problems with inconsistent answers from dataset", 
                          original_count - len(records))
            
        except Exception as exc:
            logger.error("Failed to read or process generated COT answers: %s", exc)
            raise
        finally:
            # 清理临时文件
            temp_input_path.unlink(missing_ok=True)
            # output_path.unlink(missing_ok=True)


