"""
Multi-round Adaptive Training Pipeline with DPO
"""

import gc
import json
import logging
import os
import re
import subprocess
import time
import signal
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union

from adaptive_training.processors.gemini_generator import REASONING_DIFFICULTY_LEVELS as DEFAULT_REASONING_LEVELS

# æ–°æ¨¡å—
from .sft_trainer import SFTTrainer
from .problem_validator import ProblemValidator
from .error_analyzer import ErrorAnalyzer
from .hes_scorer import HESScorer
from ..processors.sft_data_builder import SFTDataBuilder


logger = logging.getLogger(__name__)


# è¶…æ—¶å¼‚å¸¸ç±»
class TimeoutError(Exception):
    pass


def timeout_handler(signum, frame):
    """ä¿¡å·å¤„ç†å‡½æ•°ï¼šè¶…æ—¶æ—¶æŠ›å‡ºå¼‚å¸¸"""
    raise TimeoutError("Code execution timed out")


class MultiRoundPipelineDPO:
    """
    ä¸»æµç¨‹åè°ƒå™¨ - DPO ç‰ˆæœ¬
    """
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # å»¶è¿Ÿå¯¼å…¥åŸé¡¹ç›®çš„å¤„ç†å™¨ï¼ˆé¿å…å¯¼å…¥æ—¶ä¾èµ–é—®é¢˜ï¼‰
        import sys
        sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "math_generation"))
        
        from adaptive_training.processors.gemini_generator import GeminiHardProblemGenerator
        # LLM Generator å’Œ Judge åœ¨éœ€è¦æ—¶å†å¯¼å…¥
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.sft_builder = SFTDataBuilder(config)
        self.sft_trainer = SFTTrainer(config)
        # ProblemValidator å’Œ ErrorAnalyzer å°†åœ¨ _init_llm_components ä¹‹ååˆå§‹åŒ–
        self.problem_validator = None
        self.error_analyzer = None
        
        # åˆå§‹åŒ– LLM Generator å’Œ Judgeï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å… torch ä¾èµ–é—®é¢˜ï¼‰
        self.llm_generator = None
        self.llm_judge = None
        self._init_llm_components()
        
        # çŠ¶æ€
        self.initial_model_path = config.model_path  # ä¿å­˜åˆå§‹æ¨¡å‹è·¯å¾„
        self.current_model_path = config.model_path  # å½“å‰ä½¿ç”¨çš„æ¨¡å‹è·¯å¾„
        self.round_history = []
        self.sft_data_files = []  # ä¿å­˜æ¯ä¸€è½®çš„SFTæ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºç´¯ç§¯è®­ç»ƒ
        
        # éš¾åº¦æ§åˆ¶å‚æ•°ï¼ˆæ”¯æŒè‡ªå®šä¹‰ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤åˆ»åº¦ï¼‰
        self.min_reasoning_level = float(getattr(config, "min_reasoning_level", 1.0))
        self.max_reasoning_level = float(getattr(config, "max_reasoning_level", 10.0))
        self.reasoning_level_step = float(getattr(config, "reasoning_level_step", 0.5)) or 0.5
        self.reasoning_level_increment = float(getattr(config, "reasoning_level_increment", 0.5))
        reasoning_level_values = getattr(config, "reasoning_level_values", None)
        self.reasoning_scale = self._build_reasoning_scale(reasoning_level_values)
        self.default_reasoning_level = self._clamp_reasoning_level(
            float(getattr(config, "default_reasoning_level", 1.5))
        )
        
        self.min_visual_level = int(getattr(config, "min_visual_level", 1))
        self.max_visual_level = int(getattr(config, "max_visual_level", 7))
        self.visual_level_step = int(getattr(config, "visual_level_step", 1)) or 1
        self.visual_level_increment = int(getattr(config, "visual_level_increment", 1))
        self.default_visual_level = self._clamp_visual_level(
            int(getattr(config, "default_visual_level", 1))
        )
        
        # ç”¨äºåˆ†é…æ–°çš„å”¯ä¸€IDï¼ˆä»åŸå§‹æ•°æ®æœ€å¤§ID+1å¼€å§‹ï¼‰
        # éœ€è¦ä»åˆå§‹æ•°æ®é›†ä¸­è¯»å–æœ€å¤§ ID
        self.next_generated_id: int = self._get_max_id_from_initial_dataset() + 1
        self.logger.info(f"ğŸ“ Next generated problem ID will start from: {self.next_generated_id}")
        
        # åˆå§‹åŒ– Gemini Generatorï¼ˆä¸åŸé¡¹ç›®ä¸€è‡´ï¼‰
        self.gemini_generator = GeminiHardProblemGenerator(
            api_key=config.upgrade_api_key,
            model=config.upgrade_model,
            base_url=config.upgrade_base_url,
            max_tokens=config.gemini_max_tokens,
            max_output_tokens=config.gemini_max_output_tokens,
        )
        self.logger.info(f"âœ… Gemini Generator initialized: {config.upgrade_model}")
        self.base_sft_learning_rate = self.config.sft_learning_rate
    
    def _init_llm_components(self):
        """åˆå§‹åŒ– LLM Generator å’Œ Judgeï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼‰"""
        try:
            from adaptive_training.processors.llm_wrapper import LLMasGenerator, LLMasJudge
            
            self.logger.info("Initializing LLM Generator and Judge...")
            
            # åˆå§‹åŒ– LLM Generator
            if self.config.use_llm_generator:
                self.llm_generator = LLMasGenerator(self.config)
                self.logger.info(f"âœ… LLM Generator initialized: {self.config.llm_generator_model_path}")
            else:
                self.logger.warning("LLM Generator disabled")
            
            # åˆå§‹åŒ– LLM Judge
            if self.config.use_llm_judge:
                self.llm_judge = LLMasJudge(self.config)
                self.logger.info(f"âœ… LLM Judge initialized: {self.config.judge_model_path}")
            else:
                self.logger.warning("LLM Judge disabled")
            
            # åˆå§‹åŒ– Problem Validator
            self.problem_validator = ProblemValidator(
                config=self.config,
                llm_generator=self.llm_generator,
                llm_judge=self.llm_judge
            )
            self.logger.info("âœ… Problem Validator initialized")
            
            # åˆå§‹åŒ– Error Analyzerï¼ˆç”¨äºåˆ†æé”™é¢˜æ˜¯ caption é—®é¢˜è¿˜æ˜¯ reasoning é—®é¢˜ï¼‰
            if self.llm_generator:
                self.error_analyzer = ErrorAnalyzer(
                    config=self.config,
                    llm_generator=self.llm_generator
                )
                self.logger.info("âœ… Error Analyzer initialized")
            else:
                self.logger.warning("Error Analyzer disabled (LLM Generator not available)")
                
        except Exception as e:
            self.logger.error(f"Failed to initialize LLM components: {e}")
            raise
    
    def _get_max_id_from_initial_dataset(self) -> int:
        """
        ä»åˆå§‹æ•°æ®é›†ä¸­è¯»å–æœ€å¤§çš„ ID
        """
        max_id = 0
        
        try:
            # è¯»å–åˆå§‹æ•°æ®é›†
            initial_dataset_path = Path(self.config.initial_dataset)
            if not initial_dataset_path.exists():
                self.logger.warning(f"Initial dataset not found: {initial_dataset_path}")
                return max_id
            
            with open(initial_dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # éå†æ‰€æœ‰é—®é¢˜ï¼Œæ‰¾åˆ°æœ€å¤§çš„æ•°å­— ID
            for problem in data:
                # ä» id å­—æ®µè·å–
                problem_id = problem.get('id', 0)
                
                # å°è¯•è½¬æ¢ä¸ºæ•´æ•°
                try:
                    if isinstance(problem_id, int):
                        id_num = problem_id
                    elif isinstance(problem_id, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•æå–æ•°å­—éƒ¨åˆ†
                        # ä¾‹å¦‚: "123" -> 123, "123_upgraded_reasoning" -> 123
                        id_str = problem_id.split('_')[0]
                        id_num = int(id_str)
                    else:
                        continue
                    
                    if id_num > max_id:
                        max_id = id_num
                except (ValueError, AttributeError):
                    continue
            
            self.logger.info(f"ğŸ“Š Found max ID in initial dataset: {max_id} (from {len(data)} problems)")
        except Exception as e:
            self.logger.warning(f"Failed to read max ID from initial dataset: {e}")
        
        return max_id
    
    # =============================
    # éš¾åº¦åˆ»åº¦ç›¸å…³è¾…åŠ©å‡½æ•°
    # =============================
    def _build_reasoning_scale(self, custom_values: Optional[Any]) -> List[float]:
        """æ„å»ºæ¨ç†éš¾åº¦åˆ»åº¦ï¼Œä¼˜å…ˆä½¿ç”¨å¤–éƒ¨æä¾›çš„å€¼ï¼Œå¦åˆ™é€€å›é»˜è®¤è¡¨"""
        values: List[float] = []
        if isinstance(custom_values, (list, tuple)):
            candidates = custom_values
        elif custom_values is None:
            candidates = None
        else:
            candidates = [custom_values]
        if candidates:
            for raw in candidates:
                try:
                    val = float(raw)
                except (TypeError, ValueError):
                    continue
                values.append(val)
        if not values:
            values = list(DEFAULT_REASONING_LEVELS)
        filtered = sorted(
            {val for val in values if self.min_reasoning_level <= val <= self.max_reasoning_level}
        )
        if filtered:
            return filtered
        step = self.reasoning_level_step if self.reasoning_level_step > 0 else 0.5
        generated: List[float] = []
        current = self.min_reasoning_level
        while current <= self.max_reasoning_level + 1e-9:
            generated.append(round(current, 2))
            current += step
        return generated or [self.min_reasoning_level, self.max_reasoning_level]

    def _align_reasoning_level(self, value: float, direction: str = "nearest") -> float:
        """å°†éš¾åº¦å€¼æ˜ å°„åˆ°æœ€è¿‘/æœ€æ¥è¿‘çš„åˆæ³•åˆ»åº¦"""
        if not self.reasoning_scale:
            step = self.reasoning_level_step if self.reasoning_level_step > 0 else 0.5
            snapped = round(value / step) * step
            return float(min(self.max_reasoning_level, max(self.min_reasoning_level, snapped)))
        bounded = float(min(self.reasoning_scale[-1], max(self.reasoning_scale[0], value)))
        if direction == "up":
            for level in self.reasoning_scale:
                if level >= bounded - 1e-9:
                    return level
            return self.reasoning_scale[-1]
        if direction == "down":
            for level in reversed(self.reasoning_scale):
                if level <= bounded + 1e-9:
                    return level
            return self.reasoning_scale[0]
        return min(self.reasoning_scale, key=lambda lvl: (abs(lvl - bounded), lvl))

    def _clamp_reasoning_level(self, value: float) -> float:
        """å°†æ¨ç†éš¾åº¦é™å®šåˆ°åˆæ³•åŒºé—´"""
        return self._align_reasoning_level(value, direction="nearest")
    
    def _clamp_visual_level(self, value: int) -> int:
        """å°†è§†è§‰éš¾åº¦é™å®šåˆ°åˆæ³•åŒºé—´ï¼Œå¹¶å¯¹é½åˆ°æ­¥é•¿"""
        step = self.visual_level_step if self.visual_level_step > 0 else 1
        snapped = round(value / step) * step
        return int(min(self.max_visual_level, max(self.min_visual_level, snapped)))

    def _get_sft_learning_rate_for_round(self, round_num: int) -> float:
        """æ ¹æ®è½®æ¬¡é€‰æ‹© SFT å­¦ä¹ ç‡"""
        if round_num == 1 and self.config.sft_learning_rate_first_round is not None:
            return self.config.sft_learning_rate_first_round
        if round_num > 1 and self.config.sft_learning_rate_other_rounds is not None:
            return self.config.sft_learning_rate_other_rounds
        return self.base_sft_learning_rate
    
    def _normalize_reasoning_level(self, raw_value: Optional[Any]) -> float:
        """è§£æé¢˜ç›®çš„æ¨ç†éš¾åº¦æ ‡ç­¾ï¼Œç¼ºå¤±æ—¶ä½¿ç”¨é»˜è®¤å€¼"""
        try:
            value = float(raw_value)
        except (TypeError, ValueError):
            value = self.default_reasoning_level
        return self._clamp_reasoning_level(value)
    
    def _normalize_visual_level(self, raw_value: Optional[Any]) -> int:
        """è§£æé¢˜ç›®çš„è§†è§‰éš¾åº¦æ ‡ç­¾ï¼Œç¼ºå¤±æ—¶ä½¿ç”¨é»˜è®¤å€¼"""
        try:
            value = int(round(float(raw_value)))
        except (TypeError, ValueError):
            value = self.default_visual_level
        return self._clamp_visual_level(value)
    
    def _compute_target_levels(
        self,
        difficulty_aspect: str,
        current_reasoning: float,
        current_visual: int
    ) -> Tuple[float, int]:
        """
        æ ¹æ®å‡çº§æ–¹å‘è®¡ç®—ç›®æ ‡éš¾åº¦
        """
        target_reasoning = current_reasoning
        target_visual = current_visual
        
        if difficulty_aspect == "reasoning":
            if self.reasoning_level_increment > 0:
                delta = self.reasoning_level_increment
            elif len(self.reasoning_scale) >= 2:
                delta = self.reasoning_scale[1] - self.reasoning_scale[0]
            else:
                delta = self.reasoning_level_step if self.reasoning_level_step > 0 else 0.5
            candidate = current_reasoning + delta
            target_reasoning = self._align_reasoning_level(candidate, direction="up")
            if target_reasoning <= current_reasoning and len(self.reasoning_scale) > 1:
                target_reasoning = self._align_reasoning_level(current_reasoning + 1e-9, direction="up")
        elif difficulty_aspect == "visual":
            delta = self.visual_level_increment if self.visual_level_increment > 0 else 1
            target_visual = self._clamp_visual_level(current_visual + delta)
        # similar/general é»˜è®¤ä¿æŒåŸéš¾åº¦ï¼Œç”¨äºç”ŸæˆåŒçº§åˆ«é¢˜
        return target_reasoning, target_visual
    
    def run(self) -> None:
        """è¿è¡Œå¤šè½®è®­ç»ƒ"""
        pipeline_start_time = time.time()
        
        self.logger.info("=" * 80)
        self.logger.info("Starting Multi-Round Pipeline with SFT")
        self.logger.info("=" * 80)
        self.logger.info(f"Number of rounds: {self.config.num_rounds}")
        self.logger.info(f"SFT enabled: {self.config.enable_sft}")
        self.logger.info(f"Error analysis enabled: {self.config.enable_error_analysis}")
        self.logger.info("=" * 80)
        
        start_round = max(1, getattr(self.config, "start_round", 1))
        if start_round > self.config.num_rounds:
            self.logger.warning(
                "START_ROUND (%d) å¤§äºå¯æ‰§è¡Œçš„è½®æ•° (%d)ï¼Œå°†ä¸ä¼šè¿è¡Œä»»ä½•è½®æ¬¡ã€‚",
                start_round,
                self.config.num_rounds,
            )
            return
        
        if start_round == 1:
            test_data = self._load_initial_data()
        else:
            previous_round = start_round - 1
            self.logger.info(
                "æ£€æµ‹åˆ° START_ROUND=%dï¼Œå°è¯•ä» Round %d çš„è¾“å‡ºæ¢å¤æµ‹è¯•é›†å’Œæ¨¡å‹...",
                start_round,
                previous_round,
            )
            test_data = self._load_next_round_dataset(previous_round)
            self._use_model_from_round(previous_round)
            self._recover_sft_history(start_round)
        
        for round_num in range(start_round, self.config.num_rounds + 1):
            self.logger.info("\n" + "=" * 80)
            self.logger.info(f"Round {round_num} / {self.config.num_rounds}")
            self.logger.info("=" * 80)
            
            try:
                # æ‰§è¡Œä¸€è½®è®­ç»ƒ
                test_data = self._run_single_round(round_num, test_data)
                
                # ä¿å­˜è½®æ¬¡æ€»ç»“
                self._save_round_summary(round_num)
                
            except Exception as e:
                self.logger.error(f"Round {round_num} failed: {e}", exc_info=True)
                break
        
        pipeline_elapsed = time.time() - pipeline_start_time
        self.logger.info("=" * 80)
        self.logger.info("âœ… Multi-round training completed!")
        self.logger.info(f"â±ï¸  Total pipeline time: {pipeline_elapsed:.2f} seconds ({pipeline_elapsed/60:.2f} minutes / {pipeline_elapsed/3600:.2f} hours)")
        self.logger.info("=" * 80)
    
    def _load_initial_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½åˆå§‹æµ‹è¯•æ•°æ®"""
        self.logger.info(f"Loading initial test data from: {self.config.test_dataset}")
        
        with open(self.config.test_dataset, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        total_count = len(data)
        self.logger.info(f"Loaded {total_count} test problems")
        
        # æ•°æ®é‡‡æ ·ï¼šå¦‚æœæ•°æ®é‡å¤§äºé…ç½®çš„æ ·æœ¬æ•°ï¼Œéšæœºé‡‡æ ·
        # å¦‚æœ round_total_samples <= 0ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
        if self.config.round_total_samples > 0 and total_count > self.config.round_total_samples:
            import random
            random.seed(self.config.random_seed)
            data = random.sample(data, self.config.round_total_samples)
            self.logger.info(f"Sampled {len(data)} problems for this round (from {total_count} total)")
        elif self.config.round_total_samples > 0 and total_count <= self.config.round_total_samples:
            self.logger.info(f"Using all {total_count} problems (less than or equal to round_total_samples={self.config.round_total_samples})")
        else:
            self.logger.info(f"Using all {total_count} problems (round_total_samples={self.config.round_total_samples} <= 0, no sampling)")
        
        return data
    
    def _recover_sft_history(self, start_round: int) -> None:
        """åœ¨ä»ä¸­é€”æ¢å¤æ—¶ï¼ŒåŠ è½½å†å²è½®æ¬¡çš„ SFT æ•°æ®æ–‡ä»¶ï¼Œç¡®ä¿ç´¯ç§¯è®­ç»ƒæˆç«‹"""
        recovered = 0
        for past_round in range(1, start_round):
            sft_dir = self.config.output_dir / f"round_{past_round}" / "sft"
            if not sft_dir.exists():
                self.logger.warning(
                    "æ— æ³•æ‰¾åˆ° round_%d çš„ SFT ç›®å½• (%s)ï¼Œè·³è¿‡å†å²æ•°æ®æ¢å¤",
                    past_round,
                    sft_dir,
                )
                continue
            
            # ä¼˜å…ˆä½¿ç”¨æ¯è½®åŸå§‹ sft_data.jsonï¼Œé¿å…é‡å¤è®¡æ•° merged æ–‡ä»¶
            candidate = sft_dir / "sft_data.json"
            if not candidate.exists():
                # é€€è€Œæ±‚å…¶æ¬¡ï¼šå¦‚æœåªæœ‰ merged æ–‡ä»¶ï¼Œåˆ™ä½¿ç”¨ï¼Œä½†ç»™äºˆæç¤º
                candidate = sft_dir / "sft_data_merged.json"
                if candidate.exists():
                    self.logger.warning(
                        "Round %d ä»…å‘ç° sft_data_merged.jsonï¼Œå°†å…¶ç”¨äºå†å²æ¢å¤ï¼Œ"
                        "å¯èƒ½ä¼šåŒ…å«ä¹‹å‰è½®æ¬¡çš„é‡å¤æ•°æ®ï¼Œè¯·ç¡®è®¤ã€‚",
                        past_round,
                    )
            
            if candidate.exists():
                self.sft_data_files.append(candidate)
                recovered += 1
                self.logger.info(
                    "å·²æ¢å¤ Round %d çš„ SFT æ•°æ®ï¼š%s",
                    past_round,
                    candidate,
                )
            else:
                self.logger.warning(
                    "Round %d æœªæ‰¾åˆ°ä»»ä½• SFT æ•°æ®æ–‡ä»¶ (sft_data.json / sft_data_merged.json)",
                    past_round,
                )
        
        if recovered == 0:
            self.logger.warning("æœªæ¢å¤åˆ°ä»»ä½•å†å² SFT æ•°æ®ï¼ŒRound %d å°†ä»…ä½¿ç”¨å½“å‰è½®æ•°æ®", start_round)
    
    def _load_next_round_dataset(self, previous_round: int) -> List[Dict[str, Any]]:
        """ä»ä¸Šä¸€è½®çš„è¾“å‡ºä¸­åŠ è½½ next_round_dataset.json"""
        round_dir = self.config.output_dir / f"round_{previous_round}"
        next_round_file = round_dir / "next_round_dataset.json"
        
        if not next_round_file.exists():
            raise FileNotFoundError(
                f"æ‰¾ä¸åˆ°ä¸Šä¸€è½® (round {previous_round}) çš„ next_round_dataset: {next_round_file}"
            )
        
        self.logger.info("Loading next round dataset from: %s", next_round_file)
        with open(next_round_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.logger.info(
            "Loaded %d problems from round %d output as the starting test set.",
            len(data),
            previous_round,
        )
        return data
    
    def _use_model_from_round(self, previous_round: int) -> None:
        """å°†å½“å‰æ¨¡å‹è·¯å¾„åˆ‡æ¢ä¸ºä¸Šä¸€è½®è®­ç»ƒç”Ÿæˆçš„æ¨¡å‹"""
        # ä¼˜å…ˆä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„ RESUME_MODEL_PATH
        resume_model_path = getattr(self.config, "resume_model_path", "")
        if resume_model_path and Path(resume_model_path).exists():
            self.current_model_path = resume_model_path
            self.logger.info(
                "Using manually specified RESUME_MODEL_PATH: %s",
                self.current_model_path,
            )
            return
        
        # å¦åˆ™è‡ªåŠ¨æŸ¥æ‰¾ä¸Šä¸€è½®çš„æ¨¡å‹
        round_dir = self.config.output_dir / f"round_{previous_round}"
        candidate = round_dir / "sft" / f"sft_model_round{previous_round}"
        
        if candidate.exists():
            self.current_model_path = str(candidate)
            self.logger.info(
                "Resuming with model from round %d: %s",
                previous_round,
                self.current_model_path,
            )
        else:
            self.logger.warning(
                "æœªæ‰¾åˆ° round %d çš„ SFT æ¨¡å‹ç›®å½• (%s)ã€‚å°†ç»§ç»­ä½¿ç”¨å½“å‰æ¨¡å‹è·¯å¾„ï¼š%s",
                previous_round,
                candidate,
                self.current_model_path,
            )
    
    def _run_single_round(
        self,
        round_num: int,
        test_data: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå•è½®è®­ç»ƒ
        
        Returns:
            ä¸‹ä¸€è½®çš„æµ‹è¯•æ•°æ®
        """
        round_start_time = time.time()
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"Starting Round {round_num}")
        self.logger.info(f"{'='*80}")
        
        round_dir = self.config.output_dir / f"round_{round_num}"
        round_dir.mkdir(parents=True, exist_ok=True)
        
        resume_from_step = max(1, getattr(self.config, "resume_from_step", 1))
        if resume_from_step > 1:
            self.logger.warning(
                "Resume mode enabled for Round %d: starting from Step %d",
                round_num,
                resume_from_step,
            )
        
        # ===== 1. æµ‹è¯•æ¨¡å‹ =====
        eval_results: List[Dict[str, Any]] = []
        if resume_from_step <= 1:
            self.logger.info(f"\n[Step 1/{10 if self.config.enable_sft else 8}] Testing model...")
            step_start_time = time.time()
            eval_results = self._test_model(round_num, test_data, round_dir)
            step_elapsed = time.time() - step_start_time
            self.logger.info(f"â±ï¸  Step 1 completed in {step_elapsed:.2f} seconds ({step_elapsed/60:.2f} minutes)")
        else:
            self.logger.info(
                f"\n[Step 1/{10 if self.config.enable_sft else 8}] Testing model... (skipped, resume_from_step={resume_from_step})"
            )
            eval_results = self._load_eval_results(round_dir)
            if not eval_results:
                raise RuntimeError("Failed to load cached eval_results for resume mode.")
            self.logger.info("Loaded %d cached evaluation results.", len(eval_results))
        
        # ===== 2. åˆ†ç¦»é”™é¢˜å’Œå¯¹é¢˜ =====
        if resume_from_step <= 2:
            self.logger.info(f"\n[Step 2/{10 if self.config.enable_sft else 8}] Separating correct/incorrect problems...")
            step_start_time = time.time()
            wrong_problems, correct_problems = self._separate_problems(eval_results, test_data, round_dir)
            step_elapsed = time.time() - step_start_time
            self.logger.info(f"â±ï¸  Step 2 completed in {step_elapsed:.2f} seconds")
            
            self.logger.info(f"  - Wrong problems: {len(wrong_problems)}")
            self.logger.info(f"  - Correct problems: {len(correct_problems)}")
            
            # ä¿å­˜åˆ†ç¦»ç»“æœ
            self._save_separated_problems(round_dir, wrong_problems, correct_problems)
        else:
            self.logger.info(
                f"\n[Step 2/{10 if self.config.enable_sft else 8}] Separating correct/incorrect problems... "
                f"(skipped, resume_from_step={resume_from_step})"
            )
            wrong_problems, correct_problems = self._load_saved_separated_problems(round_dir)
            if not wrong_problems:
                raise RuntimeError("Failed to load cached wrong_problems for resume mode.")
            self.logger.info(
                "Loaded cached problems: wrong=%d, correct=%d",
                len(wrong_problems),
                len(correct_problems),
            )
        
        step_num = 3
        manual_skip_to_upgrade = self._should_skip_to_upgrade(round_num)
        
        # ===== 2.5 HES ç­›é€‰ï¼šå¯¹æ­£ç¡®é¢˜è®¡ç®—é«˜ç†µåˆ†æ•°ï¼Œç­›é€‰ä¸ç¡®å®šçš„æ­£ç¡®é¢˜ =====
        high_entropy_correct = []
        if getattr(self.config, "enable_hes_filtering", False) and correct_problems and not manual_skip_to_upgrade:
            if resume_from_step <= 2:  # HES å±äº Step 2 çš„å»¶ç»­
                self.logger.info(f"\n[Step 2.5] Computing HES scores for {len(correct_problems)} correct problems...")
                step_start_time = time.time()
                high_entropy_correct = self._filter_high_entropy_correct_problems(correct_problems, round_dir)
                step_elapsed = time.time() - step_start_time
                self.logger.info(f"â±ï¸  Step 2.5 (HES Filtering) completed in {step_elapsed:.2f} seconds ({step_elapsed/60:.2f} minutes)")
                self.logger.info(f"âœ… Selected {len(high_entropy_correct)} high-entropy correct problems (top {self.config.hes_top_ratio*100:.0f}%)")
                # ä¿å­˜é«˜ç†µæ­£ç¡®é¢˜
                self._save_high_entropy_correct(round_dir, high_entropy_correct)
            else:
                # ä»ç¼“å­˜åŠ è½½
                high_entropy_correct = self._load_high_entropy_correct(round_dir)
                if high_entropy_correct:
                    self.logger.info(f"Loaded {len(high_entropy_correct)} cached high-entropy correct problems")
        
        # åˆå¹¶ï¼šé”™é¢˜ + é«˜ç†µæ­£ç¡®é¢˜ -> éœ€è¦ç”Ÿæˆ CoT çš„é—®é¢˜
        problems_for_cot = wrong_problems.copy()
        if high_entropy_correct:
            for p in high_entropy_correct:
                p["is_high_entropy_correct"] = True  # æ ‡è®°æ¥æº
            problems_for_cot.extend(high_entropy_correct)
            self.logger.info(f"ğŸ“Š Total problems for CoT generation: {len(problems_for_cot)} (wrong={len(wrong_problems)}, high_entropy_correct={len(high_entropy_correct)})")
        
        # ===== 3. ç”Ÿæˆ CoT (é”™é¢˜ + é«˜ç†µæ­£ç¡®é¢˜) =====
        corrected_cots = {}
        if self.config.enable_sft and problems_for_cot and not manual_skip_to_upgrade:
            if resume_from_step <= 3:
                self.logger.info(f"\n[Step {step_num}/{10 if self.config.enable_sft else 8}] Generating corrected CoT for {len(problems_for_cot)} problems...")
                step_start_time = time.time()
                corrected_cots = self._generate_corrected_cots_for_wrong_problems(problems_for_cot, round_dir)
                step_elapsed = time.time() - step_start_time
                self.logger.info(f"â±ï¸  Step {step_num} (Corrected CoT Generation) completed in {step_elapsed:.2f} seconds ({step_elapsed/60:.2f} minutes)")
                self.logger.info(f"âœ… Generated corrected CoT for {len(corrected_cots)}/{len(problems_for_cot)} problems")
                self._save_corrected_cots(round_dir, corrected_cots)
            else:
                # resume_from_step > 3ï¼šä»ç¼“å­˜åŠ è½½ corrected_cots
                self.logger.info(
                    f"\n[Step {step_num}/{10 if self.config.enable_sft else 8}] Loading cached corrected CoT... (resume_from_step={resume_from_step})"
                )
                corrected_cots = self._load_corrected_cots(round_dir)
                if corrected_cots:
                    self.logger.info(f"âœ… Loaded {len(corrected_cots)} cached corrected CoTs")
                else:
                    self.logger.warning("No cached corrected CoTs found. Error analysis will be skipped.")
            step_num += 1
        elif manual_skip_to_upgrade:
            self.logger.info("Manual skip enabled - bypassing corrected CoT generation.")
            step_num += 1
        
        # ===== 4. åˆ†æé”™é¢˜çš„é”™è¯¯ç±»å‹ï¼ˆä½¿ç”¨ corrected CoT ä½œä¸º reference_answerï¼‰=====
        if wrong_problems:
            if manual_skip_to_upgrade:
                self.logger.info("Manual skip enabled - applying cached error analysis summary.")
                self._apply_saved_error_analysis(round_dir, wrong_problems)
                step_num += 1
            elif corrected_cots:
                if resume_from_step <= 4:
                    self.logger.info(f"\n[Step {step_num}/{10 if self.config.enable_sft else 8}] Analyzing error types using corrected CoT...")
                    step_start_time = time.time()
                    self._analyze_error_types_with_corrected_cot(wrong_problems, corrected_cots, round_dir)
                    step_elapsed = time.time() - step_start_time
                    self.logger.info(f"â±ï¸  Step {step_num} (Error Analysis) completed in {step_elapsed:.2f} seconds")
                    # ä¿å­˜å¸¦æœ‰ error_type çš„æœ€æ–°é”™é¢˜
                    self._save_separated_problems(round_dir, wrong_problems, correct_problems)
                else:
                    self.logger.info(
                        f"\n[Step {step_num}/{10 if self.config.enable_sft else 8}] Analyzing error types using corrected CoT... "
                        f"(skipped, resume_from_step={resume_from_step})"
                    )
                    self._apply_saved_error_analysis(round_dir, wrong_problems)
                step_num += 1
            else:
                self.logger.warning("No corrected CoT available for error analysis.")
                step_num += 1
        

        step_num = 6
        # ===== 6. SFT è®­ç»ƒ (å¦‚æœå¯ç”¨) =====
        # ä½¿ç”¨ problems_for_cot (é”™é¢˜ + é«˜ç†µæ­£ç¡®é¢˜) è¿›è¡Œè®­ç»ƒ
        if self.config.enable_sft and problems_for_cot and not manual_skip_to_upgrade:
            self.logger.info(f"\n[Step {step_num}/{10 if self.config.enable_sft else 8}] SFT Training on {len(problems_for_cot)} problems...")
            step_start_time = time.time()
            sft_success = self._run_sft_training(round_num, problems_for_cot, round_dir, corrected_cots)
            step_elapsed = time.time() - step_start_time
            self.logger.info(f"â±ï¸  Step {step_num} (SFT Training) completed in {step_elapsed:.2f} seconds ({step_elapsed/60:.2f} minutes)")
            
            if sft_success:
                self.current_model_path = str(round_dir / "sft" / f"sft_model_round{round_num}")
                self.logger.info(f"âœ… SFT training completed. Updated model path: {self.current_model_path}")
            else:
                self.logger.warning("âš ï¸ SFT training failed, continuing with current model")
                raise RuntimeError("SFT training failed")
            
            step_num += 1
        elif manual_skip_to_upgrade:
            manual_model_path = getattr(self.config, "manual_skip_model_path", "")
            if not manual_model_path:
                raise RuntimeError("Manual skip to upgrade requires MANUAL_SKIP_MODEL_PATH to be set.")
            self.current_model_path = manual_model_path
            self.logger.info("Manual skip enabled - using pre-trained model: %s", manual_model_path)
            
            # æ‰‹åŠ¨è·³è¿‡æ—¶ï¼Œå°è¯•å°†å·²æœ‰çš„ sft_data.json åŠ å…¥ç´¯ç§¯åˆ—è¡¨ï¼Œä»¥ä¾¿åç»­è½®æ¬¡ç´¯ç§¯è®­ç»ƒ
            existing_sft_data = round_dir / "sft" / "sft_data.json"
            if existing_sft_data.exists():
                self.sft_data_files.append(existing_sft_data)
                self.logger.info("ğŸ“Š Added existing sft_data.json to accumulation list: %s", existing_sft_data)
            else:
                self.logger.warning("âš ï¸ No existing sft_data.json found at %s for accumulation", existing_sft_data)
            
            step_num += 1
        else:
            if not wrong_problems:
                self.logger.info("â„¹ï¸ No wrong problems, skipping SFT training")
        
        # ===== 6. å‡†å¤‡å¾…å‡çº§çš„é¢˜ç›® =====
        # - é”™é¢˜ï¼šæ ¹æ®é”™è¯¯ç±»å‹å‡çº§ï¼ˆcaption é”™è¯¯ -> visual å‡çº§ï¼Œreasoning é”™è¯¯ -> reasoning å‡çº§ï¼‰
        # - é«˜ç†µæ­£ç¡®é¢˜ï¼šåŒæ—¶å‡çº§ reasoning å’Œ visualï¼ˆæ¨¡å‹ä¸ç¡®å®šçš„é¢˜ç›®ï¼‰
        self.logger.info(f"\n[Step {step_num}/{10 if self.config.enable_sft else 8}] Preparing problems for difficulty upgrade...")
        step_start_time = time.time()
        
        # è¿‡æ»¤æ‰æ²¡æœ‰ corrected CoT çš„é¢˜ç›®ï¼ˆä½¿ç”¨æ›´å¯é çš„åˆ¤æ–­ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ error_type ä¸”ä¸ä¸º skippedï¼‰
        problems_to_upgrade_wrong = [
            p for p in wrong_problems 
            if p.get('error_type') and p.get('error_type') != 'skipped'
        ]
        skipped_problems_count = len(wrong_problems) - len(problems_to_upgrade_wrong)
        
        # é«˜ç†µæ­£ç¡®é¢˜ï¼šä¹Ÿéœ€è¦å‡çº§ï¼ˆè¿™äº›é¢˜è™½ç„¶ç­”å¯¹äº†ï¼Œä½†æ¨¡å‹ä¸ç¡®å®šï¼‰
        problems_to_upgrade_hes = [
            p for p in high_entropy_correct 
            if str(p.get('id', '')) in corrected_cots  # åªå‡çº§æœ‰ corrected CoT çš„
        ] if high_entropy_correct else []
        
        self.logger.info(f"å‡çº§ç­–ç•¥:")
        self.logger.info(f"  - æ­£ç¡®é¢˜ç›®: {len(correct_problems)} ä¸ª")
        self.logger.info(f"    â€¢ é«˜ç†µæ­£ç¡®é¢˜ï¼ˆéœ€å‡çº§ï¼‰: {len(problems_to_upgrade_hes)} ä¸ª")
        self.logger.info(f"    â€¢ ä½ç†µæ­£ç¡®é¢˜ï¼ˆä¸å‡çº§ï¼‰: {len(correct_problems) - len(high_entropy_correct)} ä¸ª")
        self.logger.info(f"  - é”™é¢˜: {len(wrong_problems)} ä¸ª")
        if skipped_problems_count > 0:
            self.logger.info(f"    â€¢ è·³è¿‡ {skipped_problems_count} ä¸ªé”™é¢˜ (æ—  corrected CoT)")
        self.logger.info(f"  - å®é™…å‡çº§é”™é¢˜: {len(problems_to_upgrade_wrong)} ä¸ª (æ ¹æ®é”™è¯¯ç±»å‹å‡çº§)")
        self.logger.info(f"  - æ€»è®¡å‡çº§: {len(problems_to_upgrade_wrong) + len(problems_to_upgrade_hes)} ä¸ª")
        
        # ç»Ÿè®¡é”™é¢˜çš„é”™è¯¯ç±»å‹åˆ†å¸ƒ
        if problems_to_upgrade_wrong:
            caption_errors = sum(1 for p in problems_to_upgrade_wrong if p.get('error_type') == 'caption')
            reasoning_errors = sum(1 for p in problems_to_upgrade_wrong if p.get('error_type') == 'reasoning')
            unknown_errors = sum(1 for p in problems_to_upgrade_wrong if p.get('error_type') not in ['caption', 'reasoning', 'skipped'])
            self.logger.info(f"  é”™é¢˜åˆ†ç±»:")
            self.logger.info(f"    - Caption é”™è¯¯ (å‡çº§ visual éš¾åº¦): {caption_errors} ä¸ª")
            self.logger.info(f"    - Reasoning é”™è¯¯ (å‡çº§ reasoning éš¾åº¦): {reasoning_errors} ä¸ª")
            if unknown_errors > 0:
                self.logger.info(f"    - æœªçŸ¥é”™è¯¯ (å‡çº§ reasoning + visual + similar): {unknown_errors} ä¸ª")
        
        step_elapsed = time.time() - step_start_time
        self.logger.info(f"â±ï¸  Step {step_num} completed in {step_elapsed:.2f} seconds")
        step_num += 1
        
        # ===== 7. éš¾åº¦å‡çº§ï¼ˆé”™é¢˜ + é«˜ç†µæ­£ç¡®é¢˜ï¼‰=====
        self.logger.info(f"\n[Step {step_num}/{10 if self.config.enable_sft else 8}] Upgrading difficulty...")
        step_start_time = time.time()
        
        upgraded_problems = []
        
        # å‡çº§é”™é¢˜ï¼ˆæ ¹æ®é”™è¯¯ç±»å‹å‡çº§ï¼‰
        if problems_to_upgrade_wrong:
            self.logger.info(f"æ­£åœ¨å‡çº§ {len(problems_to_upgrade_wrong)} ä¸ªé”™é¢˜ï¼ˆåŸºäºé”™è¯¯ç±»å‹ï¼‰...")
            upgraded_from_wrong = self._upgrade_difficulty(
                round_num, 
                problems_to_upgrade_wrong, 
                round_dir, 
                upgrade_mode="error_based"
            )
            upgraded_problems.extend(upgraded_from_wrong)
            self.logger.info(f"âœ… ä»é”™é¢˜ç”Ÿæˆäº† {len(upgraded_from_wrong)} ä¸ªæ–°é¢˜")
        else:
            self.logger.info("â„¹ï¸  æ²¡æœ‰é”™é¢˜éœ€è¦å‡çº§")
        
        # å‡çº§é«˜ç†µæ­£ç¡®é¢˜ï¼ˆåŒæ—¶å‡çº§ reasoning å’Œ visualï¼‰
        if problems_to_upgrade_hes:
            self.logger.info(f"æ­£åœ¨å‡çº§ {len(problems_to_upgrade_hes)} ä¸ªé«˜ç†µæ­£ç¡®é¢˜ï¼ˆå‡çº§ reasoning + visualï¼‰...")
            # ä¸ºé«˜ç†µæ­£ç¡®é¢˜è®¾ç½® error_type ä¸º "high_entropy"ï¼Œä»¥ä¾¿åç»­è·Ÿè¸ª
            for p in problems_to_upgrade_hes:
                p["error_type"] = "high_entropy"
            upgraded_from_hes = self._upgrade_difficulty(
                round_num,
                problems_to_upgrade_hes,
                round_dir,
                upgrade_mode="both"  # åŒæ—¶å‡çº§ reasoning å’Œ visual
            )
            upgraded_problems.extend(upgraded_from_hes)
            self.logger.info(f"âœ… ä»é«˜ç†µæ­£ç¡®é¢˜ç”Ÿæˆäº† {len(upgraded_from_hes)} ä¸ªæ–°é¢˜")
        
        self.logger.info(f"ğŸ“Š æ€»è®¡ç”Ÿæˆ {len(upgraded_problems)} ä¸ªå‡çº§åçš„æ–°é¢˜")
        
        step_elapsed = time.time() - step_start_time
        self.logger.info(f"â±ï¸  Step {step_num} (éš¾åº¦å‡çº§) å®Œæˆï¼Œè€—æ—¶ {step_elapsed:.2f} ç§’ ({step_elapsed/60:.2f} åˆ†é’Ÿ)")
        
        step_num += 1
        
        # ===== 8. ç”Ÿæˆå›¾ç‰‡ï¼ˆæ‰§è¡Œ image_codeï¼‰=====
        self.logger.info(f"\n[Step {step_num}/{10 if self.config.enable_sft else 8}] Generating images from code...")
        step_start_time = time.time()
        upgraded_with_images = self._materialise_problem_images(round_num, upgraded_problems, round_dir)
        step_elapsed = time.time() - step_start_time
        self.logger.info(f"âœ… {len(upgraded_with_images)} problems have valid images")
        self.logger.info(f"â±ï¸  Step {step_num} (Image Generation) completed in {step_elapsed:.2f} seconds ({step_elapsed/60:.2f} minutes)")
        
        if len(upgraded_with_images) < len(upgraded_problems):
            discarded = len(upgraded_problems) - len(upgraded_with_images)
            self.logger.warning(f"âš ï¸  {discarded} problems discarded due to image generation failure")
        
        validated_problems = upgraded_with_images
        step_num += 1
        
        # ===== 10. å‡†å¤‡ä¸‹ä¸€è½®æ•°æ® =====
        self.logger.info(f"\n[Step {step_num}/{10 if self.config.enable_sft else 8}] Preparing problems for next round (using Gemini answers as ground truth)...")
        step_start_time = time.time()
        qualified_problems = validated_problems
        step_elapsed = time.time() - step_start_time
        self.logger.info(f"âœ… {len(qualified_problems)} problems ready for next round")
        self.logger.info(f"â±ï¸  Step {step_num} (Preparation) completed in {step_elapsed:.2f} seconds ({step_elapsed/60:.2f} minutes)")
        
        # ===== å‡†å¤‡ä¸‹ä¸€è½®æ•°æ®ï¼šåªåŒ…å«ä»é”™é¢˜å‡çº§å¾—åˆ°çš„æ–°é¢˜ =====
        self.logger.info(f"\n[å‡†å¤‡ä¸‹ä¸€è½®æ•°æ®é›†...]")

        next_round_data = qualified_problems
        
        # âš ï¸ ç»Ÿä¸€æ•°æ®æ ¼å¼ï¼Œä¸ Round 1 ä¿æŒä¸€è‡´
        for problem in next_round_data:
            # ç»Ÿä¸€ image_path æ ¼å¼ï¼šè½¬æ¢åˆ—è¡¨ä¸ºå­—ç¬¦ä¸²ï¼ˆä¸ Round 1 ä¸€è‡´ï¼‰
            if 'image_path' in problem and isinstance(problem['image_path'], list):
                # å¦‚æœæ˜¯åˆ—è¡¨ä¸”éç©ºï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                if problem['image_path']:
                    problem['image_path'] = problem['image_path'][0]
                else:
                    problem['image_path'] = ""
        
        self.logger.info(f"ä¸‹ä¸€è½®æ•°æ®é›†: {len(qualified_problems)} ä¸ªæ–°é¢˜ï¼ˆä»é”™é¢˜å‡çº§å¾—åˆ°ï¼‰")
        self.logger.info(f"  - æ–°é¢˜ï¼ˆä»é”™é¢˜å‡çº§ï¼Œä½œä¸ºä¸‹ä¸€è½®æµ‹è¯•é›†ï¼‰: {len(qualified_problems)}")
        self.logger.info(f"  - æœ¬è½®é”™é¢˜ï¼ˆå·²é€šè¿‡DPOè®­ç»ƒï¼Œä¸å†æµ‹è¯•ï¼‰: {len(wrong_problems)}")
        self.logger.info(f"  - æœ¬è½®æ­£ç¡®é¢˜ï¼ˆå·²æŒæ¡ï¼Œä¸å†æµ‹è¯•ï¼‰: {len(correct_problems)}")
        
        # ä¿å­˜ä¸‹ä¸€è½®æ•°æ®
        next_round_file = round_dir / "next_round_dataset.json"
        with open(next_round_file, 'w', encoding='utf-8') as f:
            json.dump(next_round_data, f, ensure_ascii=False, indent=2)
        self.logger.info(f"Next round dataset saved to: {next_round_file}")
        
        round_elapsed = time.time() - round_start_time
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"â±ï¸  Round {round_num} completed in {round_elapsed:.2f} seconds ({round_elapsed/60:.2f} minutes / {round_elapsed/3600:.2f} hours)")
        self.logger.info(f"{'='*80}\n")
        
        # ğŸ”§ æ¯è½®ç»“æŸæ—¶å¼ºåˆ¶æ¸…ç†å¯èƒ½æ®‹ç•™çš„ GPU è¿›ç¨‹ï¼ˆSFTã€vLLM æ¨ç†ã€HES è®¡ç®—ç­‰éƒ½å¯èƒ½ç•™ä¸‹ï¼‰
        self._cleanup_gpu_processes()
        
        # Resume æ¨¡å¼ä»…é’ˆå¯¹å½“å‰è½®æ¬¡ï¼Œå®Œæˆåå¤ä½
        self.config.resume_from_step = 1
        
        return next_round_data
    
    def _should_skip_to_upgrade(self, round_num: int) -> bool:
        """Whether to skip directly to upgrade stage for a given round."""
        if not getattr(self.config, "manual_skip_to_upgrade", False):
            return False
        target_round = int(getattr(self.config, "manual_skip_round", 1))
        return round_num == target_round
    
    def _filter_invalid_sft_samples(
        self,
        problems: List[Dict[str, Any]],
        round_dir: Path,
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """ç§»é™¤å›¾æ–‡ä¸åŒ¹é…çš„æ ·æœ¬ï¼š<image> token æ•°é‡éœ€ä¸ image_path æ•°é‡ä¸€è‡´"""
        valid_samples: List[Dict[str, Any]] = []
        removed_ids: List[str] = []
        invalid_records: List[Dict[str, Any]] = []
        
        def _normalize_image_list(image_field: Any) -> List[str]:
            if isinstance(image_field, list):
                return [str(p).strip() for p in image_field if str(p).strip()]
            if isinstance(image_field, str):
                image_field = image_field.strip()
                return [image_field] if image_field else []
            return []
        
        for sample in problems:
            problem_id = str(sample.get("id", ""))
            question = sample.get("problem") or ""
            image_token_count = question.count("<image>")
            images = _normalize_image_list(sample.get("image_path", []))
            image_count = len(images)
            
            is_valid = False
            if image_token_count == 0 and image_count == 0:
                is_valid = True
            elif image_token_count > 0 and image_token_count == image_count:
                is_valid = True
            
            if is_valid:
                valid_samples.append(sample)
            else:
                removed_ids.append(problem_id)
                invalid_records.append(
                    {
                        "id": problem_id,
                        "image_token_count": image_token_count,
                        "image_path_count": image_count,
                        "image_paths": images,
                    }
                )
        
        if invalid_records:
            invalid_file = round_dir / "invalid_sft_samples.json"
            try:
                with invalid_file.open("w", encoding="utf-8") as f:
                    json.dump(invalid_records, f, ensure_ascii=False, indent=2)
                self.logger.info("Invalid SFT samples logged to %s", invalid_file)
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("Failed to write invalid SFT sample log: %s", exc)
        
        return valid_samples, removed_ids
    
    def _load_json_file(self, path: Path) -> Optional[Any]:
        if not path.exists():
            return None
        try:
            with path.open("r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("Failed to load JSON file %s: %s", path, exc)
            return None
    
    def _load_eval_results(self, round_dir: Path) -> List[Dict[str, Any]]:
        eval_file = round_dir / "eval" / "eval_results.json"
        data = self._load_json_file(eval_file)
        return data or []
    
    def _load_saved_separated_problems(
        self,
        round_dir: Path,
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        eval_dir = round_dir / "eval"
        wrong = self._load_json_file(eval_dir / "wrong_problems.json") or []
        correct = self._load_json_file(eval_dir / "correct_problems.json") or []
        return wrong, correct
    
    def _save_corrected_cots(self, round_dir: Path, corrected_cots: Dict[str, str]) -> None:
        if not corrected_cots:
            return
        sft_dir = round_dir / "sft"
        sft_dir.mkdir(parents=True, exist_ok=True)
        file_path = sft_dir / "corrected_cots.json"
        serializable = {str(k): v for k, v in corrected_cots.items()}
        with file_path.open("w", encoding="utf-8") as f:
            json.dump(serializable, f, ensure_ascii=False, indent=2)
        self.logger.info("Corrected CoTs saved to %s", file_path)
    
    def _load_corrected_cots(self, round_dir: Path) -> Dict[str, str]:
        file_path = round_dir / "sft" / "corrected_cots.json"
        data = self._load_json_file(file_path) or {}
        # ç¡®ä¿é”®ä¸ºå­—ç¬¦ä¸²
        return {str(k): v for k, v in data.items()}
    
    def _apply_saved_error_analysis(self, round_dir: Path, wrong_problems: List[Dict[str, Any]]) -> None:
        summary_file = round_dir / "error_analysis" / "error_analysis_summary.json"
        summary = self._load_json_file(summary_file)
        if not summary:
            self.logger.warning(
                "No cached error analysis summary found at %s. Proceeding without error types.",
                summary_file,
            )
            return
        mapping = {str(item.get("id")): item for item in summary if item.get("id") is not None}
        for problem in wrong_problems:
            pid = str(problem.get("id", ""))
            if pid in mapping:
                problem["error_type"] = mapping[pid].get("error_type")
                problem["error_reason"] = mapping[pid].get("error_reason", "")
    
    def _cleanup_gpu_processes(self):
        """
        æ¸…ç†å¯èƒ½æ®‹ç•™çš„ GPU è¿›ç¨‹
        
        åœ¨ SFT/DPO è®­ç»ƒå®Œæˆåï¼ŒDeepSpeed å¯èƒ½ä¼šç•™ä¸‹å­è¿›ç¨‹å ç”¨ GPU å†…å­˜ã€‚
        è¿™ä¸ªå‡½æ•°ä¼šæŸ¥æ‰¾å¹¶æ€æ­»è¿™äº›æ®‹ç•™è¿›ç¨‹ï¼Œé¿å…åç»­ vLLM æ¨ç†æ—¶ OOMã€‚
        """
        self.logger.info("ğŸ§¹ Cleaning up residual GPU processes...")
        
        try:
            import subprocess
            current_pid = os.getpid()
            current_user = os.getenv("USER", "")
            
            # 1. è·å–å½“å‰å ç”¨ GPU çš„è¿›ç¨‹
            result = subprocess.run(
                ["nvidia-smi", "--query-compute-apps=pid,used_memory", "--format=csv,noheader,nounits"],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode != 0:
                self.logger.warning("Failed to query nvidia-smi")
                return
            
            gpu_pids = []
            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    parts = line.split(',')
                    if len(parts) >= 2:
                        try:
                            pid = int(parts[0].strip())
                            memory = int(parts[1].strip())
                            gpu_pids.append((pid, memory))
                        except ValueError:
                            continue
            
            if not gpu_pids:
                self.logger.info("No GPU processes found")
                return
            
            self.logger.info(f"Found {len(gpu_pids)} GPU processes")
            
            # 2. è¿‡æ»¤å¹¶æ€æ­»ä¸æ˜¯å½“å‰è¿›ç¨‹çš„æ®‹ç•™è¿›ç¨‹
            killed_count = 0
            for pid, memory in gpu_pids:
                if pid == current_pid:
                    self.logger.debug(f"Skipping current process {pid}")
                    continue
                
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å±äºå½“å‰ç”¨æˆ·
                try:
                    proc_result = subprocess.run(
                        ["ps", "-o", "user=", "-p", str(pid)],
                        capture_output=True,
                        text=True,
                        timeout=5
                    )
                    proc_user = proc_result.stdout.strip()
                    
                    if proc_user != current_user:
                        self.logger.debug(f"Skipping process {pid} (owned by {proc_user})")
                        continue
                    
                    # æ£€æŸ¥è¿›ç¨‹å‘½ä»¤è¡Œæ˜¯å¦åŒ…å« python/deepspeed ç­‰è®­ç»ƒç›¸å…³å…³é”®è¯
                    cmd_result = subprocess.run(
                        ["ps", "-o", "cmd=", "-p", str(pid)],
                        capture_output=True,
                        text=True,
                        timeout=5
                    )
                    cmd = cmd_result.stdout.strip()
                    
                    # åªæ€æ­»è®­ç»ƒç›¸å…³çš„è¿›ç¨‹ï¼Œé¿å…è¯¯æ€å…¶ä»–é‡è¦è¿›ç¨‹
                    training_keywords = ["deepspeed", "llamafactory", "torchrun", "accelerate", "sft"]
                    is_training_process = any(kw in cmd.lower() for kw in training_keywords)
                    
                    if is_training_process:
                        self.logger.info(f"Killing residual training process {pid} (using {memory} MiB): {cmd[:80]}...")
                        os.kill(pid, signal.SIGKILL)
                        killed_count += 1
                    else:
                        self.logger.debug(f"Skipping non-training process {pid}: {cmd[:50]}...")
                        
                except ProcessLookupError:
                    pass  # è¿›ç¨‹å·²ç»ç»“æŸ
                except Exception as e:
                    self.logger.debug(f"Error checking process {pid}: {e}")
            
            if killed_count > 0:
                self.logger.info(f"âœ… Killed {killed_count} residual training processes")
                time.sleep(5)  # ç­‰å¾…è¿›ç¨‹å®Œå…¨é€€å‡º
            else:
                self.logger.info("No residual training processes found")
            
            # 3. æ¸…ç† CUDA ç¼“å­˜
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
                    gc.collect()
                    self.logger.info("CUDA cache cleared")
            except Exception as e:
                self.logger.debug(f"Failed to clear CUDA cache: {e}")
            
            # 4. ç­‰å¾… GPU å†…å­˜å®Œå…¨é‡Šæ”¾
            time.sleep(10)
            self.logger.info("âœ… GPU cleanup completed")
            
        except Exception as e:
            self.logger.warning(f"Error during GPU cleanup: {e}")
            # å³ä½¿æ¸…ç†å¤±è´¥ä¹Ÿä¸è¦ä¸­æ–­æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ
    
    def _test_model(
        self,
        round_num: int,
        test_data: List[Dict[str, Any]],
        round_dir: Path
    ) -> List[Dict[str, Any]]:
        """æµ‹è¯•æ¨¡å‹å¹¶è¯„ä¼°"""
        eval_dir = round_dir / "eval"
        eval_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¸…ç† CUDA ç¼“å­˜
        try:
            import torch
            torch.cuda.empty_cache()
            self.logger.info("Cleared CUDA cache")
        except ImportError:
            self.logger.warning("torch not available, skipping CUDA cache clear")
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        test_file = eval_dir / "test_data.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"Testing model: {self.current_model_path}")
        self.logger.info(f"Test data: {len(test_data)} problems")
        
        # 1. ä½¿ç”¨ vLLM ç”Ÿæˆé¢„æµ‹
        predictions_file = eval_dir / "predictions.json"
        self.logger.info("Generating predictions with vLLM...")
        
        success = self.llm_generator.generate_predictions(
            test_file=test_file,
            output_file=predictions_file,
            model_path=self.current_model_path
        )
        
        if not success:
            self.logger.error("Failed to generate predictions")
            raise RuntimeError("Prediction generation failed")
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        if not predictions_file.exists():
            self.logger.error(f"Prediction file not found: {predictions_file}")
            raise RuntimeError("Prediction file not generated")
        
        self.logger.info(f"Predictions saved to: {predictions_file}")
        
        # === GPU å†…å­˜ç®¡ç†ï¼šè¯„ä¼°å®Œæˆåé‡Šæ”¾ GPU ===
        self.logger.info("Evaluation complete.")
        try:
            import torch
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            gc.collect()
            self.logger.info("GPU memory released. Waiting 10 seconds for GPU to fully release memory...")
            time.sleep(10)  
            torch.cuda.synchronize()  # ç¡®ä¿æ‰€æœ‰ CUDA æ“ä½œå®Œæˆ
            torch.cuda.empty_cache()  # å†æ¬¡æ¸…ç†
            
            # æ¸…ç†åˆ†å¸ƒå¼è¿›ç¨‹ç»„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if torch.distributed.is_initialized():
                self.logger.info("Cleaning up distributed process group...")
                torch.distributed.destroy_process_group()
                time.sleep(5)  # ç­‰å¾…ç«¯å£é‡Šæ”¾
            
            self.logger.info("Ready to start LLM Judge.")
        except ImportError:
            self.logger.warning("torch not available, skipping GPU memory management")
            self.logger.info("Waiting 30 seconds for GPU to fully release memory...")
            time.sleep(30)
        except Exception as e:
            self.logger.warning(f"Error during GPU cleanup: {e}")
            self.logger.info("Waiting 15 seconds for GPU to fully release memory...")
            time.sleep(15)
        
        # 2. ä½¿ç”¨ LLM Judge è¯„ä¼°
        self.logger.info("Evaluating predictions with LLM Judge...")
        
        # å‡†å¤‡è¯„ä¼°æ•°æ®ï¼ˆLLM Judge éœ€è¦çš„æ ¼å¼ï¼‰
        eval_records_file = eval_dir / "eval_records.json"
        eval_records = []
        for item in test_data:
            ground_truth_answer = item.get("answer", "") or item.get("model_answer", "")
            eval_records.append({
                "id": item.get("id"),
                "question": item.get("problem"),
                "answer": ground_truth_answer,  # ä¼˜å…ˆä½¿ç”¨ Gemini ç­”æ¡ˆ
                "category": item.get("category"),
                "category_name": item.get("category_name", "")
            })
        
        with open(eval_records_file, 'w', encoding='utf-8') as f:
            json.dump(eval_records, f, ensure_ascii=False, indent=2)
        
        # è°ƒç”¨ LLM Judge
        judge_results_file = eval_dir / "judge_results.json"
        success = self.llm_judge.evaluate(
            predictions_file=predictions_file,
            eval_records_file=eval_records_file,
            output_file=judge_results_file
        )
        
        if not success:
            self.logger.error("Failed to evaluate predictions")
            raise RuntimeError("Judge evaluation failed")
        
        self.logger.info(f"Judge results saved to: {judge_results_file}")
        
        # 3. åˆå¹¶ç»“æœ
        with open(predictions_file, 'r', encoding='utf-8') as f:
            predictions = json.load(f)
        
        with open(judge_results_file, 'r', encoding='utf-8') as f:
            judge_results_data = json.load(f)
        
        # å¤„ç† Judge è¾“å‡ºæ ¼å¼ï¼ˆä¸åŸé¡¹ç›®ä¸€è‡´ï¼‰
        if isinstance(judge_results_data, dict) and "eval_data" in judge_results_data:
            judge_results = judge_results_data["eval_data"]
            judge_stats = judge_results_data.get("judge_stats", {})
            
            # è¾“å‡º Judge ç»Ÿè®¡ï¼ˆä¸åŸé¡¹ç›®ä¸€è‡´ï¼‰
            self.logger.info("=" * 70)
            self.logger.info("LLM Judge Statistics:")
            self.logger.info("  Total: %d", judge_stats.get('total', 0))
            self.logger.info("  Correct: %d", judge_stats.get('correct', 0))
            self.logger.info("  Wrong: %d", judge_stats.get('wrong', 0))
            self.logger.info("  Unknown: %d", judge_stats.get('unknown', 0))
            self.logger.info("  Accuracy: %.2f%%", judge_stats.get('accuracy', 0.0) * 100)
            self.logger.info("=" * 70)
        else:
            judge_results = judge_results_data
        
        # åˆ›å»º id åˆ° judge ç»“æœçš„æ˜ å°„
        pid_to_judge = {}
        for judge_item in judge_results:
            pid = judge_item.get("id")
            if pid:
                pid_to_judge[str(pid)] = judge_item  # ç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸² key
        
        # åˆå¹¶è¯„ä¼°ç»“æœ
        eval_results = []
        for i, item in enumerate(test_data):
            # ğŸ”‘ å…³é”®ï¼šç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œä¸ Judge è¾“å‡ºçš„ problem_id åŒ¹é…
            pid = str(item.get("id"))
            pred_item = predictions[i] if i < len(predictions) else {}
            judge_item = pid_to_judge.get(pid, {})
            
            eval_results.append({
                **item,
                "model_prediction": pred_item.get("predict", ""),  # ä» 'predict' å­—æ®µè¯»å–ï¼ˆä¸åŸé¡¹ç›®ä¸€è‡´ï¼‰
                "matched": judge_item.get("matched", False),
                "judge_analysis": judge_item.get("match_analysis", ""),
                # ä¼˜å…ˆä½¿ç”¨ answerï¼ˆGemini ç­”æ¡ˆï¼‰ä½œä¸º reference_answer
                "reference_answer": item.get("answer", "") or item.get("model_answer", "")
            })
        
        # ä¿å­˜åˆå¹¶åçš„è¯„ä¼°ç»“æœ
        results_file = eval_dir / "eval_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(eval_results, f, ensure_ascii=False, indent=2)
        
        # ç»Ÿè®¡
        correct = sum(1 for r in eval_results if r.get("matched", False))
        total = len(eval_results)
        self.logger.info(f"Evaluation complete: {correct}/{total} correct ({correct/total*100:.1f}%)")
        
        return eval_results
    
    def _separate_problems(
        self,
        eval_results: List[Dict[str, Any]],
        original_data: List[Dict[str, Any]],
        round_dir: Path
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        åˆ†ç¦»é”™é¢˜å’Œå¯¹é¢˜ï¼ˆä¸è¿›è¡Œé”™è¯¯ç±»å‹åˆ†æï¼Œåˆ†æå°†åœ¨ç”Ÿæˆ corrected CoT åè¿›è¡Œï¼‰
        """
        wrong_problems = []
        correct_problems = []
        
        # æ”¶é›†æ‰€æœ‰é—®é¢˜æ•°æ®
        for result in eval_results:
            reference_answer = result.get("answer", "") or result.get("reference_answer", "") or result.get("model_answer", "")
            
            problem_data = {
                "problem": result.get("problem", ""),
                "reference_answer": reference_answer,  # ä¸´æ—¶ä½¿ç”¨ Gemini ç­”æ¡ˆï¼Œåç»­ä¼šæ›´æ–°ä¸º corrected CoT
                "model_prediction": result.get("model_prediction", ""),
                "image_path": result.get("image_path", ""),  # é»˜è®¤ç©ºå­—ç¬¦ä¸²ï¼Œä¸åŸå§‹æ•°æ®ä¸€è‡´
                "category": result.get("category", 0),
                "category_name": result.get("category_name", ""),
                "id": result.get("id", 0),
                "answer": result.get("answer", ""),  # ä¿ç•™ Gemini ç­”æ¡ˆï¼ˆä½œä¸º ground truthï¼‰
                "model_answer": result.get("model_answer", ""),  # ä¿ç•™ä¹‹å‰ç”Ÿæˆçš„ CoTï¼ˆå¦‚æœæœ‰ï¼Œç”¨äºæ—§æ•°æ®ï¼‰
                "reasoning_level": self._normalize_reasoning_level(result.get("reasoning_level")),
                "visual_level": self._normalize_visual_level(result.get("visual_level"))
            }
            
            if result.get("matched", False):
                correct_problems.append(problem_data)
            else:
                wrong_problems.append(problem_data)
        
        return wrong_problems, correct_problems
    
    def _save_separated_problems(
        self,
        round_dir: Path,
        wrong_problems: List[Dict[str, Any]],
        correct_problems: List[Dict[str, Any]]
    ) -> None:
        """ä¿å­˜åˆ†ç¦»çš„é¢˜ç›®"""
        eval_dir = round_dir / "eval"
        
        with open(eval_dir / "wrong_problems.json", 'w', encoding='utf-8') as f:
            json.dump(wrong_problems, f, ensure_ascii=False, indent=2)
        
        with open(eval_dir / "correct_problems.json", 'w', encoding='utf-8') as f:
            json.dump(correct_problems, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"Saved separated problems to {eval_dir}")
    
    def _filter_high_entropy_correct_problems(
        self,
        correct_problems: List[Dict[str, Any]],
        round_dir: Path
    ) -> List[Dict[str, Any]]:
        """
        å¯¹æ­£ç¡®é¢˜ç›®è®¡ç®— HES åˆ†æ•°ï¼Œç­›é€‰å‡ºå‰ top_ratio é«˜ç†µçš„æ­£ç¡®é¢˜ã€‚
        è¿™äº›é¢˜ç›®è™½ç„¶ç­”å¯¹äº†ï¼Œä½†æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸ç¡®å®šï¼Œå€¼å¾—åŠ å…¥è®­ç»ƒã€‚
        
        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        1. Apptainer æ¨¡å¼ï¼šä½¿ç”¨å®¹å™¨å†…çš„ vLLMï¼ˆè§£å†³ Qwen3-VL prompt_logprobs å…¼å®¹æ€§é—®é¢˜ï¼‰
        2. ç›´æ¥æ¨¡å¼ï¼šç›´æ¥è°ƒç”¨ HESScorerï¼ˆé€‚ç”¨äºå·²å®‰è£…å…¼å®¹ vLLM çš„ç¯å¢ƒï¼‰
        """
        if not correct_problems:
            return []
        
        hes_dir = round_dir / "hes"
        hes_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Apptainer å®¹å™¨
        use_apptainer = os.getenv("USE_APPTAINER_FOR_VLLM", "false").lower() in ("true", "1", "yes")
        apptainer_image = os.getenv("APPTAINER_IMAGE", "")
        
        all_hes_scores = []
        
        if use_apptainer and apptainer_image:
            # ============ Apptainer æ¨¡å¼ï¼šé€šè¿‡å­è¿›ç¨‹è°ƒç”¨å®¹å™¨å†…çš„ hes_scorer.py ============
            self.logger.info(f"Using Apptainer for HES calculation: {apptainer_image}")
            
            # 1. å‡†å¤‡è¾“å…¥ JSONL æ–‡ä»¶
            input_jsonl = hes_dir / "hes_input.jsonl"
            output_jsonl = hes_dir / "hes_output.jsonl"
            
            with open(input_jsonl, 'w', encoding='utf-8') as f:
                for p in correct_problems:
                    prompt = p.get("problem", "")
                    completion = p.get("model_prediction", "") or p.get("model_answer", "")
                    record = {
                        "id": p.get("id", ""),
                        "prompt": prompt,
                        "completion": completion,
                    }
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")
            
            self.logger.info(f"Prepared {len(correct_problems)} samples for HES calculation")
            
            # 2. æ„å»º Apptainer å‘½ä»¤
            script_path = Path(__file__).parent / "hes_scorer.py"
            project_root = Path(__file__).parent.parent.parent
            triton_cache_dir = f"/tmp/triton_cache_{os.getenv('USER', 'default')}_{os.getpid()}"
            
            tensor_parallel_size = getattr(self.config, "hes_tensor_parallel_size", 8)
            percentile_cutoff = getattr(self.config, "hes_percentile_cutoff", 0.005)
            batch_size = getattr(self.config, "hes_batch_size", 8)
            
            cmd = [
                "apptainer", "exec", "--nv",
                "--cleanenv",
                "--bind", "/share:/share,/mnt:/mnt",
                "--env", "HF_HUB_OFFLINE=1",
                "--env", "TRANSFORMERS_OFFLINE=1",
                "--env", "HF_DATASETS_OFFLINE=1",
                "--env", "PYTHONUNBUFFERED=1",
                "--env", f"CUDA_VISIBLE_DEVICES={os.getenv('CUDA_VISIBLE_DEVICES', '')}",
                "--env", "VLLM_LOGGING_LEVEL=WARNING",
                "--env", f"TRITON_CACHE_DIR={triton_cache_dir}",
                "--env", "PATH=/opt/py312/bin:/usr/local/cuda/bin:/usr/bin:/bin",
                "--env", "CC=/usr/bin/gcc",
                "--env", "CXX=/usr/bin/g++",
                "--env", "CUDAHOSTCXX=/usr/bin/g++",
                "--env", "LD_LIBRARY_PATH=/usr/local/cuda/lib64:/.singularity.d/libs",
                "--env", "CUDA_HOME=/usr/local/cuda",
                "--env", f"PYTHONPATH={project_root}",
                apptainer_image,
                "python", "-u", str(script_path),
                "--model_path", self.current_model_path,
                "--dataset_path", str(input_jsonl),
                "--output_path", str(output_jsonl),
                "--prompt_key", "prompt",
                "--completion_key", "completion",
                "--percentile_cutoff", str(percentile_cutoff),
                "--tensor_parallel_size", str(tensor_parallel_size),
                "--batch_size", str(batch_size),
                "--gpu_memory_utilization", "0.7",
                "--max_model_len", "16384",
            ]
            
            self.logger.info(f"Running HES scorer in Apptainer container...")
            self.logger.debug(f"Command: {' '.join(cmd)}")
            
            # 3. æ‰§è¡Œå‘½ä»¤
            try:
                import subprocess
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=3600,  # 1å°æ—¶è¶…æ—¶
                )
                
                if result.returncode != 0:
                    self.logger.error(f"HES scorer failed with exit code {result.returncode}")
                    self.logger.error(f"stderr: {result.stderr[-2000:]}")  # æœ€å2000å­—ç¬¦
                    return []
                
                self.logger.info("HES scorer completed successfully")
                if result.stdout:
                    # æ‰“å°æœ€åå‡ è¡Œè¾“å‡º
                    stdout_lines = result.stdout.strip().split('\n')[-10:]
                    for line in stdout_lines:
                        self.logger.info(f"[HES] {line}")
                
            except subprocess.TimeoutExpired:
                self.logger.error("HES scorer timed out after 1 hour")
                return []
            except Exception as e:
                self.logger.error(f"Failed to run HES scorer: {e}")
                return []
            
            # 4. è¯»å–è¾“å‡ºæ–‡ä»¶
            if not output_jsonl.exists():
                self.logger.error(f"HES output file not found: {output_jsonl}")
                return []
            
            hes_results = {}
            with open(output_jsonl, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        obj = json.loads(line)
                        hes_results[obj.get("id", "")] = obj.get("hes_score", 0.0)
            
            # 5. å°† HES åˆ†æ•°åˆå¹¶åˆ°åŸé—®é¢˜
            for p in correct_problems:
                p["hes_score"] = hes_results.get(p.get("id", ""), 0.0)
                all_hes_scores.append(p["hes_score"])
            
        else:
            # ============ ç›´æ¥æ¨¡å¼ï¼šä½¿ç”¨æœ¬åœ° HESScorer ============
            self.logger.info("Using local HESScorer (Apptainer disabled)")
            
            try:
                scorer = HESScorer(
                    model_path=self.current_model_path,
                    tensor_parallel_size=getattr(self.config, "hes_tensor_parallel_size", 8),
                    percentile_cutoff=getattr(self.config, "hes_percentile_cutoff", 0.005),
                )
            except Exception as e:
                self.logger.error(f"Failed to initialize HESScorer: {e}")
                self.logger.warning("Falling back to skipping HES filtering")
                return []
            
            # å‡†å¤‡ prompts å’Œ completions
            prompts = []
            completions = []
            for p in correct_problems:
                prompt = p.get("problem", "")
                completion = p.get("model_prediction", "") or p.get("model_answer", "")
                prompts.append(prompt)
                completions.append(completion)
            
            # æ‰¹é‡è®¡ç®— HES
            batch_size = getattr(self.config, "hes_batch_size", 8)
            
            for i in range(0, len(prompts), batch_size):
                batch_prompts = prompts[i:i+batch_size]
                batch_completions = completions[i:i+batch_size]
                try:
                    scores = scorer.calculate_hes_for_batch(batch_prompts, batch_completions)
                    all_hes_scores.extend(scores)
                except Exception as e:
                    self.logger.warning(f"HES calculation failed for batch {i//batch_size}: {e}")
                    all_hes_scores.extend([0.0] * len(batch_prompts))
            
            # é‡Šæ”¾ vLLM æ˜¾å­˜
            del scorer
            gc.collect()
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except:
                pass
            
            # ä¸ºæ¯ä¸ªé—®é¢˜æ·»åŠ  HES åˆ†æ•°
            for idx, p in enumerate(correct_problems):
                p["hes_score"] = all_hes_scores[idx] if idx < len(all_hes_scores) else 0.0
        
        # ============ é€šç”¨åå¤„ç† ============
        if not all_hes_scores:
            self.logger.warning("No HES scores calculated, returning empty list")
            return []
        
        # æŒ‰ HES åˆ†æ•°é™åºæ’åº
        sorted_problems = sorted(correct_problems, key=lambda x: x.get("hes_score", 0.0), reverse=True)
        
        # å–å‰ top_ratio
        top_ratio = getattr(self.config, "hes_top_ratio", 0.2)
        top_k = max(1, int(len(sorted_problems) * top_ratio))
        high_entropy_correct = sorted_problems[:top_k]
        
        # ä¿å­˜æ‰€æœ‰æ­£ç¡®é¢˜çš„ HES åˆ†æ•°ï¼ˆç”¨äºåˆ†æï¼‰
        with open(hes_dir / "correct_problems_with_hes.json", 'w', encoding='utf-8') as f:
            json.dump(sorted_problems, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"HES scores: min={min(all_hes_scores):.4f}, max={max(all_hes_scores):.4f}, mean={sum(all_hes_scores)/len(all_hes_scores):.4f}")
        self.logger.info(f"Selected top {top_k} high-entropy correct problems (threshold HES >= {high_entropy_correct[-1].get('hes_score', 0):.4f})")
        
        return high_entropy_correct
    
    def _save_high_entropy_correct(self, round_dir: Path, high_entropy_correct: List[Dict[str, Any]]) -> None:
        """ä¿å­˜é«˜ç†µæ­£ç¡®é¢˜"""
        hes_dir = round_dir / "hes"
        hes_dir.mkdir(parents=True, exist_ok=True)
        with open(hes_dir / "high_entropy_correct.json", 'w', encoding='utf-8') as f:
            json.dump(high_entropy_correct, f, ensure_ascii=False, indent=2)
        self.logger.info(f"Saved {len(high_entropy_correct)} high-entropy correct problems to {hes_dir}")
    
    def _load_high_entropy_correct(self, round_dir: Path) -> List[Dict[str, Any]]:
        """åŠ è½½é«˜ç†µæ­£ç¡®é¢˜"""
        file_path = round_dir / "hes" / "high_entropy_correct.json"
        return self._load_json_file(file_path) or []
    
    def _remove_think_content(self, text: str, problem_id: str) -> str:
        """
        æ¸…é™¤ Thinking æ¨¡å‹è¾“å‡ºçš„ </think> ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
        
        Args:
            text: æ¨¡å‹è¾“å‡ºçš„å®Œæ•´æ–‡æœ¬
            problem_id: é—®é¢˜IDï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return text
        
        # æŸ¥æ‰¾ </think> æ ‡ç­¾
        think_end_pattern = r'</think>\s*'
        match = re.search(think_end_pattern, text, re.IGNORECASE)
        
        if match:
            # æ‰¾åˆ°äº† </think>ï¼Œåˆ é™¤å®ƒä¹‹å‰çš„æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ ‡ç­¾æœ¬èº«ï¼‰
            cleaned_text = text[match.end():].strip()
            self.logger.info(f"Problem {problem_id}: å·²æ¸…é™¤ thinking å†…å®¹ï¼ˆåˆ é™¤äº† {match.end()} ä¸ªå­—ç¬¦ï¼‰")
            return cleaned_text
        else:
            # æ²¡æœ‰æ‰¾åˆ° </think>ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æœªé—­åˆçš„ <think> æˆ–å…¶ä»–æ€è€ƒæ¨¡å¼
            # å¦‚æœæ–‡æœ¬ä»¥ "Got it,", "Let me", "First," ç­‰å¼€å¤´ï¼Œä¸”åœ¨ç¬¬ä¸€ä¸ª <caption> ä¹‹å‰æœ‰å¤§é‡æ–‡æœ¬ï¼Œä¹Ÿæ¸…ç†
            caption_match = re.search(r'<caption>', text, re.IGNORECASE)
            if caption_match and caption_match.start() > 500:
                # <caption> ä¹‹å‰æœ‰è¶…è¿‡ 500 ä¸ªå­—ç¬¦ï¼Œå¯èƒ½æ˜¯æœªé—­åˆçš„æ€è€ƒè¿‡ç¨‹
                self.logger.warning(f"Problem {problem_id}: æ£€æµ‹åˆ°å¯èƒ½çš„æœªé—­åˆæ€è€ƒå†…å®¹ï¼ˆ<caption> ä¹‹å‰æœ‰ {caption_match.start()} ä¸ªå­—ç¬¦ï¼‰ï¼Œä¿ç•™åŸæ–‡")
            
            return text
    
    def _extract_boxed_answer(self, text: str, problem_id: Optional[str] = None) -> Optional[str]:
        """
        æå– \\boxed{} ä¸­çš„ç­”æ¡ˆï¼Œæ­£ç¡®å¤„ç†åµŒå¥—å¤§æ‹¬å·
        ä¼˜å…ˆä» <answer> æ ‡ç­¾ä¸­æå–ï¼Œå¦åˆ™ä»å…¨æ–‡ä¸­æå–æœ€åä¸€ä¸ª
        
        Args:
            text: åŒ…å« \\boxed{} çš„æ–‡æœ¬
            problem_id: é—®é¢˜IDï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
            
        Returns:
            æå–çš„ç­”æ¡ˆå­—ç¬¦ä¸²ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
        """
        if not text:
            return None
        
        # è¾…åŠ©å‡½æ•°ï¼šä»ç»™å®šæ–‡æœ¬ä¸­æå–æ‰€æœ‰ \boxed{}
        def extract_all_boxed(content: str) -> List[str]:
            results = []
            i = 0
            while i < len(content):
                start = content.find('\\boxed{', i)
                if start == -1:
                    break
                
                brace_start = start + 7  # len('\\boxed{') = 7
                brace_count = 1
                j = brace_start
                
                while j < len(content) and brace_count > 0:
                    if content[j] == '{':
                        brace_count += 1
                    elif content[j] == '}':
                        brace_count -= 1
                    j += 1
                
                if brace_count == 0:
                    answer = content[brace_start:j-1].strip()
                    if answer:  # åªä¿ç•™éç©ºç­”æ¡ˆ
                        results.append(answer)
                    i = j
                else:
                    i = brace_start
            return results
        
        # ä¼˜å…ˆä» <answer> æ ‡ç­¾ä¸­æå–
        import re
        answer_match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL | re.IGNORECASE)
        if answer_match:
            answer_content = answer_match.group(1).strip()
            boxed_in_answer = extract_all_boxed(answer_content)
            if boxed_in_answer:
                # å¦‚æœ <answer> ä¸­æœ‰å¤šä¸ªç­”æ¡ˆï¼Œè®°å½•è­¦å‘Š
                if len(boxed_in_answer) > 1:
                    logging.warning(f"Found {len(boxed_in_answer)} \\boxed{{}} in <answer> section, using the last one")
                return boxed_in_answer[-1]
            
            # å®¹é”™æœºåˆ¶ï¼šå¦‚æœ <answer> ä¸­æ²¡æœ‰ \boxed{}ï¼Œä½†æœ‰éç©ºå†…å®¹ï¼Œå°è¯•è‡ªåŠ¨åŒ…è£¹
            # 1. æ£€æŸ¥æ˜¯å¦æ˜¯ä¸å®Œæ•´çš„ \boxed{ï¼ˆç¼ºå°‘é—­åˆ}ï¼‰
            if '\\boxed{' in answer_content:
                # æœ‰ \boxed{ ä½†æ²¡æœ‰å®Œæ•´çš„ç­”æ¡ˆï¼Œè¯´æ˜æ ¼å¼é”™è¯¯ï¼Œä¸è‡ªåŠ¨åŒ…è£¹
                problem_prefix = f"Problem {problem_id}: " if problem_id else ""
                self.logger.warning(f"{problem_prefix}<answer> contains incomplete \\boxed{{}} (missing closing brace), skipping")
                # ç»§ç»­å°è¯•ä»å…¨æ–‡æå–
            elif answer_content and answer_content not in ['$$', '$', '{}', '()', '[]', '']:
                # 2. ç§»é™¤å¤–å±‚çš„ $ ç¬¦å·ï¼ˆå¦‚æœæœ‰ï¼‰
                cleaned_content = answer_content.strip('$').strip()
                # 3. ç¡®ä¿æ¸…ç†åçš„å†…å®¹æœ‰æ•ˆä¸”ä¸æ˜¯çº¯ç¬¦å·ï¼ˆå…è®¸å•å­—ç¬¦ç­”æ¡ˆå¦‚"A","B","D"ç­‰ï¼‰
                if cleaned_content and len(cleaned_content) >= 1 and cleaned_content not in ['$$', '{}', '()', '$']:
                    problem_prefix = f"Problem {problem_id}: " if problem_id else ""
                    self.logger.warning(f"{problem_prefix}<answer> section missing \\boxed{{}}, auto-wrapping content: {cleaned_content[:50]}...")
                    return cleaned_content
        
        # å¦åˆ™ä»å…¨æ–‡ä¸­æå–
        all_boxed = extract_all_boxed(text)
        if not all_boxed:
            return None
        
        # å¦‚æœæœ‰å¤šä¸ªä¸åŒçš„ç­”æ¡ˆï¼Œè®°å½•è­¦å‘Š
        unique_answers = set(all_boxed)
        if len(unique_answers) > 1:
            logging.warning(f"Found {len(all_boxed)} \\boxed{{}} with {len(unique_answers)} different answers in full text")
        
        return all_boxed[-1]
    
    # def _build_reflection_prompt(self, problem_text: str, wrong_answer: str) -> str:
    #     """
    #     æ„å»ºç”¨äºç”Ÿæˆ corrected CoT çš„æç¤ºï¼ˆç›´æ¥è®©æ¨¡å‹åšé¢˜ï¼Œä¸å±•ç¤ºé”™è¯¯ç­”æ¡ˆï¼‰
        
    #     Args:
    #         problem_text: é—®é¢˜æ–‡æœ¬
    #         wrong_answer: é”™è¯¯çš„ç­”æ¡ˆï¼ˆä¿ç•™å‚æ•°ä»¥å…¼å®¹ï¼Œä½†ä¸ä½¿ç”¨ï¼‰
            
    #     Returns:
    #         æ„å»ºå¥½çš„æç¤ºè¯
    #     """
    #     return problem_text
    
    def _normalize_image_paths(self, image_path_raw: Union[str, List[str]]) -> List[str]:
        """
        æ ‡å‡†åŒ–å›¾åƒè·¯å¾„ä¸ºåˆ—è¡¨æ ¼å¼
        
        Args:
            image_path_raw: å›¾åƒè·¯å¾„ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
            
        Returns:
            å›¾åƒè·¯å¾„åˆ—è¡¨
        """
        if isinstance(image_path_raw, list):
            return image_path_raw
        elif isinstance(image_path_raw, str) and image_path_raw:
            return [image_path_raw]
        else:
            return []
    
    def _get_problem_id(self, problem: Dict[str, Any]) -> str:
        """
        è·å–é—®é¢˜IDï¼Œç¡®ä¿ä¸ build_dpo_dataset ä¸€è‡´
        
        Args:
            problem: é—®é¢˜å­—å…¸
            
        Returns:
            é—®é¢˜IDå­—ç¬¦ä¸²
        """
        problem_id_val = problem.get('id')
        if problem_id_val is None or problem_id_val == 0:
            return str(hash(problem.get('problem', '')) % 100000)
        else:
            return str(problem_id_val)
    
    def _validate_corrected_cot_format(
        self,
        corrected_cot: str,
        problem_id: str,
        wrong_answer: str
    ) -> Tuple[bool, Optional[str]]:
        """
        éªŒè¯ corrected CoT çš„æ ¼å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        æ³¨æ„ï¼šåŸºæœ¬æ ¼å¼ï¼ˆä¸‰ä¸ªæ ‡ç­¾ã€é‡å¤æ£€æµ‹ï¼‰å·²åœ¨ API è°ƒç”¨åç«‹å³éªŒè¯
        è¿™é‡Œåªåšé¢å¤–çš„æ£€æŸ¥ï¼š
        1. èƒ½å¦æå–åˆ°æœ‰æ•ˆç­”æ¡ˆï¼ˆé€šè¿‡ _extract_boxed_answerï¼‰
        2. å¥å­çº§åˆ«çš„é‡å¤æ£€æµ‹ï¼ˆè¡¥å……å­—ç¬¦çº§åˆ«çš„æ£€æµ‹ï¼‰
        
        Args:
            corrected_cot: ç”Ÿæˆçš„ corrected CoTï¼ˆå·²é€šè¿‡åˆæ­¥éªŒè¯ï¼‰
            problem_id: é—®é¢˜ID
            wrong_answer: é”™è¯¯ç­”æ¡ˆï¼ˆç”¨äºå¯¹æ¯”ï¼Œå½“å‰æœªä½¿ç”¨ï¼‰
            
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, è·³è¿‡åŸå› )
        """
        # æ£€æŸ¥ 1: å¿…é¡»èƒ½æå–åˆ°ç­”æ¡ˆï¼ˆæ¯”ä»…æ£€æŸ¥æ ‡ç­¾å­˜åœ¨æ›´ä¸¥æ ¼ï¼‰
        corrected_box = self._extract_boxed_answer(corrected_cot, problem_id)
        if not corrected_box:
            return False, f"Cannot extract answer from <answer> section"
        
        # æ£€æŸ¥ 2: å¥å­çº§åˆ«çš„é‡å¤æ£€æµ‹ï¼ˆè¡¥å…… is_valid_cot çš„å­—ç¬¦çº§æ£€æµ‹ï¼‰
        sentences = corrected_cot.split('.')
        if len(sentences) > 10:
            for j in range(len(sentences) - 3):
                if sentences[j] == sentences[j+1] == sentences[j+2] == sentences[j+3]:
                    return False, "Contains repeated sentences"
        
        return True, None
    
    def _validate_corrected_cots_with_judge(
        self,
        temp_corrected_cots: Dict[str, str],
        wrong_problems: List[Dict[str, Any]],
        problem_ids: List[str],
        valid_inputs: List[Tuple[int, Any]],
        round_dir: Path = None
    ) -> Dict[str, str]:
        """
        ä½¿ç”¨LLM-as-JudgeéªŒè¯corrected CoTsçš„ç­”æ¡ˆæ­£ç¡®æ€§
        
        Args:
            temp_corrected_cots: æ ¼å¼éªŒè¯é€šè¿‡çš„corrected CoTs {problem_id: corrected_cot}
            wrong_problems: é”™é¢˜åˆ—è¡¨
            problem_ids: é—®é¢˜IDåˆ—è¡¨
            valid_inputs: æœ‰æ•ˆè¾“å…¥åˆ—è¡¨
            round_dir: è½®æ¬¡ç›®å½•ï¼ˆç”¨äºä¿å­˜ Judge çš„è¾“å…¥è¾“å‡ºï¼‰
            
        Returns:
            ç­”æ¡ˆæ­£ç¡®çš„corrected CoTså­—å…¸
        """
        # æ„å»ºpredictionså’Œeval_recordsç”¨äºjudge
        predictions = []
        eval_records = []
        id_to_cot = {}  # æ˜ å°„judgeç”¨çš„idåˆ°corrected_cot
        
        for problem_id, corrected_cot in temp_corrected_cots.items():
            # æ‰¾åˆ°å¯¹åº”çš„é—®é¢˜
            problem = None
            for wp in wrong_problems:
                if str(wp.get('id')) == str(problem_id):
                    problem = wp
                    break
            
            if not problem:
                self.logger.warning(f"Problem {problem_id} not found in wrong_problems")
                continue
            
            # æ„å»ºpredictionæ ¼å¼ï¼ˆjudgeæœŸæœ›çš„å­—æ®µåæ˜¯model_predictionï¼‰
            pred_item = {
                'id': problem_id,
                'model_prediction': corrected_cot  # JudgeæœŸæœ›è¿™ä¸ªå­—æ®µå
            }
            predictions.append(pred_item)
            
            # æ„å»ºeval_recordæ ¼å¼
            eval_item = {
                'pid': problem_id,
                'question': problem.get('problem', ''),
                'answer': problem.get('answer', ''),
                'image_path': problem.get('image_path', '')
            }
            eval_records.append(eval_item)
            
            id_to_cot[problem_id] = corrected_cot
        
        if not predictions:
            self.logger.warning("No predictions to validate with judge")
            return {}
        
        # å†³å®šä¿å­˜ç›®å½•
        # ä¼˜å…ˆä½¿ç”¨ round_dirï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å…¨å±€ä¸´æ—¶ç›®å½•
        if round_dir:
            judge_dir = round_dir / "sft" / "judge_validation"
            judge_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"ğŸ“ Saving Judge validation files to: {judge_dir}")
        else:
            judge_dir = Path(self.config.output_dir) / "temp_judge_validation"
            judge_dir.mkdir(parents=True, exist_ok=True)
            self.logger.warning(f"âš ï¸  round_dir not provided, using global temp dir: {judge_dir}")
        
        predictions_file = judge_dir / "corrected_predictions.json"
        eval_records_file = judge_dir / "corrected_eval_records.json"
        judge_results_file = judge_dir / "corrected_judge_results.json"
        
        with open(predictions_file, 'w', encoding='utf-8') as f:
            json.dump(predictions, f, ensure_ascii=False, indent=2)
        
        with open(eval_records_file, 'w', encoding='utf-8') as f:
            json.dump(eval_records, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“ Judge input files saved:")
        self.logger.info(f"   - Predictions: {predictions_file}")
        self.logger.info(f"   - Eval records: {eval_records_file}")
        
        # è°ƒç”¨LLM-as-Judge
        self.logger.info(f"Validating {len(predictions)} corrected CoTs with LLM-as-Judge...")
        success = self.llm_judge.evaluate(
            predictions_file=predictions_file,
            eval_records_file=eval_records_file,
            output_file=judge_results_file
        )
        
        if not success or not judge_results_file.exists():
            self.logger.error("LLM-as-Judge validation failed")
            return temp_corrected_cots  # å›é€€ï¼šå¦‚æœjudgeå¤±è´¥ï¼Œè¿”å›æ‰€æœ‰é€šè¿‡æ ¼å¼éªŒè¯çš„
        
        self.logger.info(f"âœ… Judge results saved to: {judge_results_file}")
        
        # è¯»å–judgeç»“æœï¼ˆå¤„ç†æ–°çš„JSONæ ¼å¼ï¼‰
        with open(judge_results_file, 'r', encoding='utf-8') as f:
            judge_data = json.load(f)
        
        # Judgeç»“æœå¯èƒ½æ˜¯ {"eval_data": [...]} æˆ–ç›´æ¥æ˜¯æ•°ç»„
        if isinstance(judge_data, dict) and 'eval_data' in judge_data:
            judge_results = judge_data['eval_data']
        elif isinstance(judge_data, list):
            judge_results = judge_data
        else:
            self.logger.error(f"Unexpected judge result format: {type(judge_data)}")
            return temp_corrected_cots
        
        # è¿‡æ»¤å‡ºç­”æ¡ˆæ­£ç¡®çš„corrected CoTs
        validated_cots = {}
        failed_details = []  # è®°å½•å¤±è´¥çš„è¯¦ç»†ä¿¡æ¯
        
        for result in judge_results:
            # ç¡®ä¿resultæ˜¯å­—å…¸
            if not isinstance(result, dict):
                self.logger.warning(f"Skipping non-dict result: {result}")
                continue
                
            problem_id = str(result.get('id'))
            matched = result.get('matched', False)
            
            if matched and problem_id in id_to_cot:
                validated_cots[problem_id] = id_to_cot[problem_id]
                self.logger.debug(f"Problem {problem_id}: corrected CoT validated by judge")
            elif problem_id in id_to_cot:
                self.logger.warning(f"Problem {problem_id}: corrected CoT answer is wrong according to judge")
                # è®°å½•å¤±è´¥è¯¦æƒ…
                failed_details.append({
                    'problem_id': problem_id,
                    'matched': matched,
                    'match_analysis': result.get('match_analysis', 'N/A')
                })
        
        # ä¿å­˜å¤±è´¥è¯¦æƒ…åˆ°å•ç‹¬çš„æ–‡ä»¶
        if failed_details and round_dir:
            failed_report_file = judge_dir / "failed_validation_report.json"
            with open(failed_report_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'total_validated': len(judge_results),
                    'passed': len(validated_cots),
                    'failed': len(failed_details),
                    'failed_problems': failed_details
                }, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ğŸ“‹ Failed validation report saved to: {failed_report_file}")
        
        return validated_cots
    
    def _prepare_batch_inputs(
        self,
        wrong_problems: List[Dict[str, Any]]
    ) -> Tuple[List[Optional[Dict[str, Any]]], List[str]]:
        """
        å‡†å¤‡æ‰¹é‡è¾“å…¥æ•°æ®
        
        Args:
            wrong_problems: é”™é¢˜åˆ—è¡¨
            
        Returns:
            (æ‰¹é‡è¾“å…¥åˆ—è¡¨, é—®é¢˜IDåˆ—è¡¨)
        """
        batch_inputs = []
        problem_ids = []
        
        for problem in wrong_problems:
            problem_id = self._get_problem_id(problem)
            problem_ids.append(problem_id)
            
            problem_text = problem.get('problem', '')
            wrong_answer = problem.get('model_prediction', '')
            image_paths = self._normalize_image_paths(problem.get('image_path', ''))
            
            if not problem_text or not wrong_answer:
                self.logger.warning(f"Skipping problem {problem_id}: missing problem or wrong_answer")
                batch_inputs.append(None)
                continue
            
            # ç¡®ä¿é—®é¢˜æ–‡æœ¬åŒ…å« <image> tokenï¼ˆå¦‚æœæœ‰å›¾åƒï¼‰
            if '<image>' not in problem_text and image_paths:
                problem_text = '<image> ' + problem_text
            
            # æ„å»ºåæ€æç¤º
            # reflection_prompt = self._build_reflection_prompt(problem_text, wrong_answer)
            
            batch_inputs.append({
                "problem": problem_text,
                "image_path": image_paths
            })
        
        return batch_inputs, problem_ids
    
    def _analyze_error_types_with_corrected_cot(
        self,
        wrong_problems: List[Dict[str, Any]],
        corrected_cots: Dict[str, str],
        round_dir: Path
    ) -> None:
        """
        ä½¿ç”¨ corrected CoT ä½œä¸ºå‚è€ƒç­”æ¡ˆè¿›è¡Œé”™è¯¯ç±»å‹åˆ†æ
        
        Args:
            wrong_problems: é”™é¢˜åˆ—è¡¨
            corrected_cots: ç”Ÿæˆçš„æ­£ç¡® CoT å­—å…¸ (problem_id -> corrected_cot)
            round_dir: è½®æ¬¡ç›®å½•
        """
        if not wrong_problems:
            return
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºé”™è¯¯åˆ†æ
        error_analysis_dir = round_dir / "error_analysis"
        error_analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨é”™è¯¯åˆ†æ
        enable_error_analysis = getattr(self.config, 'enable_error_analysis', True)
        
        if not enable_error_analysis or not self.error_analyzer:
            self.logger.warning("é”™è¯¯åˆ†ææœªå¯ç”¨æˆ–ä¸å¯ç”¨ï¼Œæ‰€æœ‰é”™é¢˜é»˜è®¤ä¸º reasoning é—®é¢˜")
            for problem_data in wrong_problems:
                problem_data["error_type"] = "reasoning"
                problem_data["error_reason"] = "Error analysis disabled or ErrorAnalyzer not available"
            return
        
        # æ²¡æœ‰ corrected CoT çš„é¢˜ç›®æ ‡è®°ä¸ºè·³è¿‡ï¼Œä¸å‚ä¸é”™è¯¯åˆ†æå’Œåç»­ç”Ÿæˆæ–°é¢˜
        problems_to_analyze = []
        skipped_count = 0
        
        for problem in wrong_problems:
            problem_id = str(problem.get("id", ""))
            if problem_id in corrected_cots:
                problem["reference_answer"] = corrected_cots[problem_id]
                problems_to_analyze.append(problem)
                self.logger.debug(f"Problem {problem_id}: ä½¿ç”¨ corrected CoT ä½œä¸º reference_answer")
            else:
                # æ²¡æœ‰ corrected CoTï¼Œæ ‡è®°ä¸ºè·³è¿‡
                problem["error_type"] = "skipped"
                problem["error_reason"] = "No valid corrected CoT available"
                skipped_count += 1
                self.logger.warning(f"Problem {problem_id}: æ²¡æœ‰ corrected CoTï¼Œè·³è¿‡é”™è¯¯åˆ†æå’Œæ–°é¢˜ç”Ÿæˆ")
        
        if skipped_count > 0:
            self.logger.warning(f"âš ï¸  {skipped_count} problems skipped (no corrected CoT): will not be analyzed or used for generating new problems")
        
        if not problems_to_analyze:
            self.logger.warning("æ²¡æœ‰å¯åˆ†æçš„é—®é¢˜ï¼ˆæ‰€æœ‰é—®é¢˜éƒ½æ²¡æœ‰ corrected CoTï¼‰")
            return
        
        # æ‰¹é‡åˆ†æé”™è¯¯ç±»å‹ï¼ˆåªåˆ†ææœ‰ corrected CoT çš„é¢˜ç›®ï¼‰
        try:
            self.logger.info(f"å¼€å§‹æ‰¹é‡åˆ†æ {len(problems_to_analyze)} ä¸ªé”™é¢˜ï¼ˆä½¿ç”¨ corrected CoT ä½œä¸ºå‚è€ƒï¼‰...")
            
            # ä½¿ç”¨æ‰¹é‡åˆ†ææ–¹æ³•ï¼ˆä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰é”™é¢˜ï¼ŒvLLM å¹¶è¡Œï¼‰
            error_results = self.error_analyzer.analyze_errors_batch(
                wrong_problems=problems_to_analyze,
                temp_dir=error_analysis_dir
            )
            
            # å°†åˆ†æç»“æœæ·»åŠ åˆ°æ¯ä¸ªé”™é¢˜ä¸­
            for i, (error_type, error_reason) in enumerate(error_results):
                if i < len(wrong_problems):
                    wrong_problems[i]["error_type"] = error_type
                    wrong_problems[i]["error_reason"] = error_reason
            
            self.logger.info(f"âœ… æ‰¹é‡é”™è¯¯åˆ†æå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡é”™è¯¯åˆ†æå¤±è´¥: {e}ï¼Œæ‰€æœ‰é”™é¢˜é»˜è®¤ä¸º reasoning é—®é¢˜")
            # é»˜è®¤æ‰€æœ‰é”™é¢˜ä¸º reasoning é—®é¢˜
            for problem_data in problems_to_analyze:
                problem_data["error_type"] = "reasoning"
                problem_data["error_reason"] = f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}"
        
        # ä¿å­˜é”™è¯¯åˆ†æç»“æœ
        if wrong_problems:
            analysis_summary_file = error_analysis_dir / "error_analysis_summary.json"
            error_summary = []
            for wp in wrong_problems:
                error_summary.append({
                    "id": wp.get("id"),
                    "error_type": wp.get("error_type"),
                    "error_reason": wp.get("error_reason", "")[:200],  # æˆªæ–­è¿‡é•¿çš„ç†ç”±
                    "problem_preview": wp.get("problem", "")[:100]
                })
            with open(analysis_summary_file, 'w', encoding='utf-8') as f:
                json.dump(error_summary, f, ensure_ascii=False, indent=2)
            self.logger.info(f"é”™è¯¯åˆ†ææ‘˜è¦ä¿å­˜åˆ°: {analysis_summary_file}")
    
    def _generate_corrected_cots_with_api(
        self,
        batch_input_file: Path,
        batch_output_file: Path,
        max_tokens: int
    ) -> bool:
        """
        ä½¿ç”¨APIç”Ÿæˆcorrected CoT (ä»£æ›¿æœ¬åœ°vLLMéƒ¨ç½²), ä½¿ç”¨å¹¶å‘åŠ é€Ÿ
        
        Args:
            batch_input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            batch_output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        import openai
        import base64
        import os
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # è¯»å–è¾“å…¥æ•°æ®
        with open(batch_input_file, 'r', encoding='utf-8') as f:
            inputs = json.load(f)
        
        # APIé…ç½®ï¼ˆä½¿ç”¨ä¸“é—¨çš„corrected CoTé…ç½®ï¼‰
        api_base = self.config.corrected_cot_base_url
        api_key = self.config.corrected_cot_api_key
        api_model = self.config.corrected_cot_model
        max_workers = min(self.config.corrected_cot_max_workers, len(inputs))
        
        self.logger.info(f"Corrected CoT APIé…ç½®: model={api_model}, base={api_base}")
        self.logger.info(f"Using concurrent generation with max_workers={max_workers}")
        
        client = openai.OpenAI(base_url=api_base, api_key=api_key)
        
        system_prompt = """
            You are an expert in science and visual reasoning with advanced capabilities in multimodal analysis.
            Your goal is to create a **perfect, highly detailed training example** for a new AI model.
            Do not summarize or abbreviate. Your reasoning must be **expansive, verbose, and pedagogical**.

            ### Core Principles
            1.  **Extreme Detail.** Prioritize depth over brevity. Explain the "why" and "how" behind every step, even simple ones.
            2.  **Visual Dependency.** Explicitly quote visual features (coordinates, colors, relative positions) in your reasoning to prove you are looking at the image.
            3.  **Atomic Reasoning.** Break down complex logic into small, atomic steps. Do not leap from premise to conclusion; build the bridge.
            4.  **Self-Correction.** If a step involves a common pitfall, explicitly mention it and explain how to avoid it.

            ### Response Structure & Detailed Instructions

            You **must** respond using **only** the three tags below in the exact order specified.

            #### 1. `<caption>` (Exhaustive Information Extraction)
            **Goal:** Create a text-based "digital twin" of the provided images and text.
            * **Visual Analysis:** Do not just list objects. Describe their relationships, exact values on axes, trend directions, specific colors used for specific data, and geometry properties.
            * **Text Analysis:** Copy out specific numbers and constraints verbatim before interpreting them.
            * **Synthesis:** Explicitly map text variables to image labels (e.g., "The variable $x$ in the text corresponds to the horizontal axis in Figure 1").
            * *Instruction:* Be verbose. Capture details that might seem minor, as they may be crucial later.

            #### 2. `<reasoning>` (Deep-Dive Solution Execution)
            **Goal:** A long-form, step-by-step derivation that reads like a detailed lecture.
            * **Step 1: Strategic Planning**
                * Restate the objective clearly.
                * List the specific formulas or scientific principles required.
                * Explain *why* these principles were chosen over others.
            * **Step 2: Execution (The "Thinking" Engine)**
                * **Expand Every Calculation:** Do not just show `$a + b = c$`. Instead, write: "Substitute $a=5$ and $b=3$ into the equation. This yields $5 + 3$, which equals $8$."
                * **Inner Monologue:** Explain the physical or logical meaning of intermediate results.
                * **Visual Check:** Continuously refer back to the `<caption>`. (e.g., "As seen in the graph, the curve peaks at $t=5$, which aligns with our calculated critical point.").
                * **Handling Complexity:** If a problem has multiple cases, analyze each one strictly and separately.
            * **Step 3: Verification**
                * Perform a sanity check on the magnitude and units of the result.
                * Does the answer physically make sense given the visual context?

            *Constraint:* **Do not omit algebra.** Show the manipulation of terms. If you are solving a system of equations, show the substitution or elimination steps explicitly.

            #### 3. `<answer>` (Final Conclusion)
            **Goal:** Provide the definitive result.
            * Format: `<answer>\\boxed{YOUR_ANSWER}</answer>`
            * For multiple choice, include the letter and the value.
            * Strictly no reasoning text inside this tag, only the final result.

            Analyze all provided materials carefully. **Write a lengthy, comprehensive, and meticulous response following the strictly defined format above.**
        """





        # å®šä¹‰å•ä¸ªé—®é¢˜çš„ç”Ÿæˆå‡½æ•°
        def generate_single_cot(idx_and_item):
            """ç”Ÿæˆå•ä¸ªé—®é¢˜çš„corrected CoTï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
            idx, item = idx_and_item
            problem = item.get('problem', '')
            image_paths = item.get('image_path', [])
            
            if not isinstance(image_paths, list):
                image_paths = [image_paths] if image_paths else []
            
            # æ£€æµ‹ç”Ÿæˆå¾ªç¯çš„è¾…åŠ©å‡½æ•°
            def detect_repetition(text, window=200, threshold=3):
                """æ£€æµ‹æ–‡æœ¬æ˜¯å¦æœ‰é‡å¤æ¨¡å¼"""
                if len(text) < window * 2:
                    return False
                last_window = text[-window:]
                # æ£€æŸ¥æœ€å window å¤§å°çš„å†…å®¹åœ¨ä¹‹å‰æ˜¯å¦é‡å¤å‡ºç°
                search_region = text[-window*5:-window]
                count = search_region.count(last_window[:50]) if len(last_window) >= 50 else 0
                return count >= threshold
            
            # éªŒè¯ç”Ÿæˆç»“æœçš„è¾…åŠ©å‡½æ•°
            def is_valid_cot(text):
                """éªŒè¯ CoT æ˜¯å¦æœ‰æ•ˆ"""
                if not text or len(text) == 0:
                    return False, "Empty"
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ç”Ÿæˆå¾ªç¯
                if detect_repetition(text):
                    return False, "Repetition detected"
                
                # æ£€æŸ¥å¿…éœ€çš„æ ‡ç­¾
                text_lower = text.lower()
                if '<caption>' not in text_lower:
                    return False, "Missing <caption> tag"
                if '<reasoning>' not in text_lower:
                    return False, "Missing <reasoning> tag"
                if '<answer>' not in text_lower:
                    return False, "Missing <answer> tag"
                
                return True, "Valid"
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": [{"type": "text", "text": problem}]}
            ]
            
            # æ·»åŠ å›¾åƒï¼ˆbase64ç¼–ç ï¼‰
            for img_path in image_paths:
                if os.path.exists(img_path):
                    try:
                        with open(img_path, 'rb') as img_file:
                            img_data = base64.b64encode(img_file.read()).decode('utf-8')
                        
                        img_ext = os.path.splitext(img_path)[1].lower()
                        mime_type = {
                            '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg',
                            '.png': 'image/png', '.gif': 'image/gif',
                            '.webp': 'image/webp'
                        }.get(img_ext, 'image/jpeg')
                        
                        messages[1]["content"].insert(0, {
                            "type": "image_url",
                            "image_url": {"url": f"data:{mime_type};base64,{img_data}"}
                        })
                    except Exception as e:
                        self.logger.warning(f"[Thread-{idx+1}] æ— æ³•è¯»å–å›¾åƒ {img_path}: {e}")
            
            # é‡è¯•æœºåˆ¶ï¼šæœ€å¤šå°è¯• 3 æ¬¡
            max_retries = 3
            last_error = None
            
            for attempt in range(max_retries):
                try:
                    # æ ¹æ®é‡è¯•æ¬¡æ•°è°ƒæ•´å‚æ•°
                    temperature = 0.7 + (attempt * 0.1)  # é€æ¸å¢åŠ éšæœºæ€§
                    frequency_penalty = 0.3 + (attempt * 0.2)  # é€æ¸å¢åŠ é‡å¤æƒ©ç½š
                    
                    response = client.chat.completions.create(
                        model=api_model,
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=min(temperature, 1.0),
                        frequency_penalty=min(frequency_penalty, 1.0),
                        presence_penalty=0.3
                    )
                    
                    message = response.choices[0].message
                    full_content = (getattr(message, "content", None) or "").strip()
                    reasoning_content = (getattr(message, "reasoning_content", None) or "").strip()

                    if reasoning_content:
                        import re

                        def _replace_reasoning(match):
                            return f"{match.group(1)}{reasoning_content}{match.group(3)}"

                        updated_content, replaced = re.subn(
                            r"(<reasoning>)(.*?)(</reasoning>)",
                            _replace_reasoning,
                            full_content,
                            count=1,
                            flags=re.IGNORECASE | re.DOTALL
                        )

                        if replaced > 0:
                            prediction = updated_content
                        else:
                            prediction = full_content
                            self.logger.warning("Reasoning content returned but <reasoning> tag not found; using original content.")
                    else:
                        prediction = full_content

                    # logging.info(f"prediction: {prediction}")
                    # éªŒè¯ç”Ÿæˆç»“æœ
                    is_valid, reason = is_valid_cot(prediction)
                    
                    if is_valid:
                        if attempt > 0:
                            self.logger.info(f"âœ… [Thread-{idx+1}] é‡è¯•æˆåŠŸï¼ˆç¬¬ {attempt+1} æ¬¡å°è¯•ï¼‰{idx+1}/{len(inputs)}")
                        else:
                            self.logger.info(f"âœ… [Thread-{idx+1}] å·²å®Œæˆ {idx+1}/{len(inputs)}")
                        return idx, {'predict': prediction}
                    else:
                        self.logger.warning(f"âš ï¸  [Thread-{idx+1}] éªŒè¯å¤±è´¥ï¼ˆ{reason}ï¼‰ï¼Œå°è¯•é‡è¯• {attempt+1}/{max_retries}")
                        last_error = reason
                        if attempt < max_retries - 1:
                            import time
                            time.sleep(2)  # çŸ­æš‚å»¶è¿Ÿåé‡è¯•
                        continue
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸  [Thread-{idx+1}] APIè°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt+1}/{max_retries}ï¼‰: {e}")
                    last_error = str(e)
                    if attempt < max_retries - 1:
                        import time
                        time.sleep(5)  # API é”™è¯¯åç­‰å¾…æ›´é•¿æ—¶é—´
                        continue
            
            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
            self.logger.error(f"âŒ [Thread-{idx+1}] æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}")
            return idx, {'predict': ''}
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œ
        results_dict = {}  # ä½¿ç”¨å­—å…¸æ¥ä¿æŒé¡ºåº
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = {
                executor.submit(generate_single_cot, (idx, item)): idx
                for idx, item in enumerate(inputs)
            }
            
            # æŒ‰å®Œæˆé¡ºåºå¤„ç†ç»“æœ
            completed_count = 0
            for future in as_completed(futures):
                try:
                    idx, result = future.result()
                    results_dict[idx] = result
                    completed_count += 1
                    
                    if completed_count % 10 == 0:
                        self.logger.info(f"ğŸ“Š è¿›åº¦: {completed_count}/{len(inputs)} ä¸ªcorrected CoTå·²å®Œæˆ")
                        
                except Exception as e:
                    self.logger.error(f"âŒ å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
        
        # æŒ‰åŸå§‹é¡ºåºæ’åˆ—ç»“æœ
        results = [results_dict[i] for i in range(len(inputs))]
        
        # ä¿å­˜ç»“æœ
        with open(batch_output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"âœ… APIå¹¶å‘ç”Ÿæˆå®Œæˆ: {len(results)}/{len(inputs)}")
        return True
    
    def _generate_corrected_cots_for_wrong_problems(
        self,
        wrong_problems: List[Dict[str, Any]],
        round_dir: Path
    ) -> Dict[str, str]:
        """
        æ‰¹é‡ç”Ÿæˆé”™é¢˜çš„ä¿®æ­£ CoT
        
        Args:
            wrong_problems: é”™é¢˜åˆ—è¡¨
            round_dir: è½®æ¬¡ç›®å½•
            
        Returns:
            å­—å…¸: problem_id -> corrected_cot
        """
        if not self.llm_generator:
            self.logger.warning("LLM Generator not available, skipping corrected CoT generation")
            return {}
        
        total = len(wrong_problems)
        self.logger.info(f"Generating corrected CoT for {total} wrong problems...")
        
        # å‡†å¤‡æ‰¹é‡è¾“å…¥
        batch_inputs, problem_ids = self._prepare_batch_inputs(wrong_problems)
        
        # è¿‡æ»¤æœ‰æ•ˆè¾“å…¥
        valid_inputs = [(idx, inp) for idx, inp in enumerate(batch_inputs) if inp is not None]
        if not valid_inputs:
            self.logger.warning("No valid inputs for corrected CoT generation")
            return {}
        
        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        sft_dir = round_dir / "sft"
        temp_dir = sft_dir / "sft_temp"
        temp_dir.mkdir(parents=True, exist_ok=True)
        batch_input_file = temp_dir / "batch_reflection_input.json"
        batch_output_file = temp_dir / "batch_reflection_output.json"
        
        # ä¿å­˜æ‰¹é‡è¾“å…¥
        valid_inputs_list = [inp for _, inp in valid_inputs]
        with open(batch_input_file, 'w', encoding='utf-8') as f:
            json.dump(valid_inputs_list, f, ensure_ascii=False, indent=2)
        
        # æ‰¹é‡ç”Ÿæˆ
        self.logger.info(f"ğŸ”„ Generating corrected CoT for {len(valid_inputs_list)} problems (batch mode)...")
        self.logger.info(f"âš ï¸  Using API for corrected CoT generation (not local model)")
        import time
        start_time = time.time()
        
        corrected_cot_max_tokens = self.config.corrected_cot_max_tokens
        self.logger.info(f"Using max_tokens={corrected_cot_max_tokens} for corrected CoT generation")

        # ============ æ–¹å¼1: è°ƒç”¨APIç”Ÿæˆ (å½“å‰ä½¿ç”¨) ============
        self.logger.info("ğŸŒ Using API to generate corrected CoT...")
        success = self._generate_corrected_cots_with_api(
            batch_input_file=batch_input_file,
            batch_output_file=batch_output_file,
            max_tokens=corrected_cot_max_tokens
        )
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"â±ï¸  Batch corrected CoT generation took {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)")
        
        if not success or not batch_output_file.exists():
            self.logger.warning("Failed to generate corrected CoT batch")
            return {}
        
        # è¯»å–ç»“æœå¹¶éªŒè¯
        with open(batch_output_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ ¼å¼éªŒè¯å’Œæ¸…æ´—
        temp_corrected_cots = {}
        initial_skipped = 0
        failed_reasons = {}  # è®°å½•å¤±è´¥åŸå› 
        
        for i, (original_idx, _) in enumerate(valid_inputs):
            if i >= len(results):
                problem_id = problem_ids[original_idx]
                self.logger.warning(f"No result for problem {problem_id}")
                failed_reasons[problem_id] = "No result from API"
                initial_skipped += 1
                continue
            
            corrected_cot = results[i].get('predict', '').strip()
            problem_id = problem_ids[original_idx]
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºï¼ˆé‡è¯•å¤±è´¥ä¼šè¿”å›ç©ºå­—ç¬¦ä¸²ï¼‰
            if not corrected_cot:
                self.logger.warning(f"Empty corrected CoT for problem {problem_id} (all retries failed)")
                failed_reasons[problem_id] = "Empty corrected CoT (retries exhausted)"
                initial_skipped += 1
                continue
            
            # æ¸…é™¤ Thinking æ¨¡å‹è¾“å‡ºçš„ </think> ä¹‹å‰çš„å†…å®¹
            # corrected_cot = self._remove_think_content(corrected_cot, problem_id)
            
            problem = wrong_problems[original_idx]
            wrong_answer = problem.get('model_prediction', '')
            
            # é¢å¤–çš„æ ¼å¼éªŒè¯ï¼ˆç­”æ¡ˆæå–ã€å¥å­é‡å¤æ£€æµ‹ï¼‰
            # æ³¨æ„ï¼šåŸºæœ¬æ ¼å¼ï¼ˆä¸‰æ ‡ç­¾ã€å­—ç¬¦é‡å¤ï¼‰å·²åœ¨ API è°ƒç”¨åéªŒè¯
            is_valid, skip_reason = self._validate_corrected_cot_format(
                corrected_cot, problem_id, wrong_answer
            )
            
            if not is_valid:
                self.logger.warning(f"Problem {problem_id}: {skip_reason}, skipping")
                failed_reasons[problem_id] = skip_reason
                initial_skipped += 1
                continue
            
            # æ ¼å¼éªŒè¯é€šè¿‡ï¼Œä¿å­˜
            temp_corrected_cots[problem_id] = corrected_cot
        
        self.logger.info(f"Format validation: {len(temp_corrected_cots)}/{len(valid_inputs)} passed")
        if initial_skipped > 0:
            self.logger.warning(f"âš ï¸  {initial_skipped} problems failed format validation")
            # æ±‡æ€»æ˜¾ç¤ºå¤±è´¥åŸå› 
            reason_counts = {}
            for problem_id, reason in failed_reasons.items():
                reason_counts[reason] = reason_counts.get(reason, 0) + 1
            
            self.logger.warning("ğŸ“‹ Format validation failure summary:")
            for reason, count in sorted(reason_counts.items(), key=lambda x: -x[1]):
                self.logger.warning(f"   - {reason}: {count} problem(s)")
            
            # æ˜¾ç¤ºæ¯ä¸ªé¢˜ç›®çš„å…·ä½“å¤±è´¥åŸå› 
            if len(failed_reasons) <= 10:  # å¦‚æœä¸è¶…è¿‡10ä¸ªï¼Œæ˜¾ç¤ºè¯¦æƒ…
                self.logger.warning("   Details:")
                for problem_id, reason in failed_reasons.items():
                    self.logger.warning(f"     â€¢ Problem {problem_id}: {reason}")
        
        if not temp_corrected_cots:
            return {}
        
        # ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨LLM-as-Judgeæ‰¹é‡éªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§
        self.logger.info("Starting LLM-as-Judge validation for corrected CoTs...")
        corrected_cots = self._validate_corrected_cots_with_judge(
            temp_corrected_cots, wrong_problems, problem_ids, valid_inputs, round_dir
        )
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        judge_skipped = len(temp_corrected_cots) - len(corrected_cots)
        self.logger.info(f"âœ… Successfully validated corrected CoT for {len(corrected_cots)}/{len(valid_inputs)} problems")
        self.logger.info(f"   Format validation passed: {len(temp_corrected_cots)}")
        self.logger.info(f"   Judge validation passed: {len(corrected_cots)}")
        if judge_skipped > 0:
            self.logger.warning(f"âš ï¸  {judge_skipped} problems failed judge validation (wrong answers)")
            # æ˜¾ç¤ºè¢«Judgeæ‹’ç»çš„é¢˜ç›®ID
            failed_judge_ids = set(temp_corrected_cots.keys()) - set(corrected_cots.keys())
            if failed_judge_ids and len(failed_judge_ids) <= 10:
                self.logger.warning("   Failed judge validation:")
                for problem_id in sorted(failed_judge_ids):
                    self.logger.warning(f"     â€¢ Problem {problem_id}: Answer is incorrect")
        
        return corrected_cots
    
    def _run_sft_training(
        self,
        round_num: int,
        wrong_problems: List[Dict[str, Any]],
        round_dir: Path,
        corrected_cots: Optional[Dict[str, str]] = None
    ) -> bool:
        """è¿è¡Œ SFT è®­ç»ƒï¼ˆç´¯ç§¯è®­ç»ƒæ¨¡å¼ï¼šæ¯è½®éƒ½ç”¨æ‰€æœ‰å†å²æ•°æ®è®­ç»ƒåˆå§‹æ¨¡å‹ï¼‰"""
        sft_dir = round_dir / "sft"
        sft_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. æ„å»ºæœ¬è½® SFT æ•°æ®
        sft_data_file = sft_dir / "sft_data.json"
        num_sft_points, num_wrong = self.sft_builder.build_sft_dataset(
            wrong_problems, 
            sft_data_file,
            corrected_cots=corrected_cots
        )
        
        if num_sft_points == 0:
            self.logger.warning("No SFT data generated, skipping SFT training")
            return False
        
        self.logger.info(f"Created {num_sft_points} SFT data points from {num_wrong} wrong problems")
        
        # 2. ä¿å­˜æœ¬è½®æ•°æ®æ–‡ä»¶è·¯å¾„åˆ°å†å²è®°å½•
        self.sft_data_files.append(sft_data_file)
        self.logger.info(f"ğŸ“Š Accumulated SFT data files: {len(self.sft_data_files)} rounds")
        
        # 3. åˆå¹¶æ‰€æœ‰å†å²æ•°æ®ï¼ˆç´¯ç§¯è®­ç»ƒï¼‰
        if len(self.sft_data_files) > 1:
            self.logger.info(f"ğŸ”„ Merging SFT data from {len(self.sft_data_files)} rounds for cumulative training...")
            merged_data = []
            total_points = 0
            
            for i, data_file in enumerate(self.sft_data_files, 1):
                try:
                    with open(data_file, 'r', encoding='utf-8') as f:
                        round_data = json.load(f)
                    merged_data.extend(round_data)
                    total_points += len(round_data)
                    self.logger.info(f"   Round {i}: {len(round_data)} data points")
                except Exception as e:
                    self.logger.error(f"Failed to load SFT data from {data_file}: {e}")
                    return False
            
            # è¿‡æ»¤æ‰å›¾åƒä¸åŒ¹é…çš„æ•°æ®ï¼ˆ<image> tokenæ•°é‡ != imagesæ•°ç»„é•¿åº¦ï¼‰
            filtered_data = []
            removed_count = 0
            for item in merged_data:
                # ç»Ÿè®¡<image> tokenæ•°é‡
                image_token_count = 0
                for conv in item.get('conversations', []):
                    if conv.get('from') == 'user':
                        image_token_count = conv.get('value', '').count('<image>')
                
                # è·å–imagesæ•°ç»„é•¿åº¦
                images_count = len(item.get('images', []))
                
                # åªä¿ç•™åŒ¹é…çš„æ•°æ®
                if image_token_count == images_count:
                    filtered_data.append(item)
                else:
                    removed_count += 1
            
            self.logger.info(f"ğŸ” è¿‡æ»¤å›¾åƒä¸åŒ¹é…çš„æ•°æ®: ç§»é™¤ {removed_count} æ¡, ä¿ç•™ {len(filtered_data)} æ¡")
            
            # ä¿å­˜è¿‡æ»¤åçš„åˆå¹¶æ•°æ®
            merged_data_file = sft_dir / "sft_data_merged.json"
            with open(merged_data_file, 'w', encoding='utf-8') as f:
                json.dump(filtered_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… Merged {len(filtered_data)} total SFT data points from {len(self.sft_data_files)} rounds (filtered {removed_count} mismatched)")
            training_data_file = merged_data_file
        else:
            self.logger.info("ğŸ“ Round 1: Using single round data (no merging needed)")
            training_data_file = sft_data_file
        
        # 4. GPU æ˜¾å­˜ç®¡ç†ï¼šç­‰å¾…æ˜¾å­˜é‡Šæ”¾ï¼ˆSFTè®­ç»ƒéœ€è¦å¤§é‡æ˜¾å­˜ï¼‰
        wait_time = self.config.dpo_memory_wait_time
        self.logger.info(f"Waiting {wait_time} seconds for GPU memory to fully release before SFT training...")
        time.sleep(wait_time)
        
        self.logger.info("Ready to start SFT training.")
        
        # 5. æ‰§è¡Œ SFT è®­ç»ƒï¼ˆä½¿ç”¨åˆå§‹æ¨¡å‹ + æ‰€æœ‰ç´¯ç§¯æ•°æ®ï¼‰
        dataset_name = f"sft_round_{round_num}_cumulative"
        model_path = Path(self.initial_model_path)  # ä½¿ç”¨åˆå§‹æ¨¡å‹ï¼Œè€Œä¸æ˜¯ä¸Šä¸€è½®çš„æ¨¡å‹
        output_dir = sft_dir / f"sft_model_round{round_num}"
        
        self.logger.info(f"ğŸ¯ Training strategy: Cumulative training")
        self.logger.info(f"   - Base model: {self.initial_model_path}")
        self.logger.info(f"   - Training data: {len(self.sft_data_files)} rounds merged")
        self.logger.info(f"   - Output model: {output_dir}")
        
        success = self.sft_trainer.train(
            dataset_name=dataset_name,
            sft_data_file=training_data_file,
            model_path=model_path,
            output_dir=output_dir,
            round_num=round_num,
            learning_rate=self._get_sft_learning_rate_for_round(round_num),
        )
        
        if not success:
            return False
        
        # è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ç›´æ¥ä¿å­˜
        trained_model_path = output_dir
        self.logger.info(f"âœ… SFT training completed. Model saved to: {trained_model_path}")
        
        return True
    
    def _upgrade_difficulty(
        self,
        round_num: int,
        problems: List[Dict[str, Any]],
        round_dir: Path,
        upgrade_mode: str = "both"
    ) -> List[Dict[str, Any]]:
        """
        å‡çº§éš¾åº¦ (å¹¶è¡Œå¤„ç†)
        
        Args:
            round_num: è½®æ¬¡
            problems: éœ€è¦å‡çº§çš„é¢˜ç›®åˆ—è¡¨
            round_dir: è½®æ¬¡ç›®å½•
            upgrade_mode: å‡çº§æ¨¡å¼
                - "both": ç”Ÿæˆ reasoning å’Œ visual ä¸¤ä¸ªå˜ä½“ (é»˜è®¤, ç”¨äºæ­£ç¡®é¢˜ç›®)
                - "error_based": æ ¹æ®é”™è¯¯ç±»å‹å‡çº§ (ç”¨äºé”™é¢˜)
        
        Returns:
            å‡çº§åçš„é¢˜ç›®åˆ—è¡¨
        """
        upgraded_dir = round_dir / "upgraded"
        upgraded_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"Upgrading difficulty for {len(problems)} problems (mode: {upgrade_mode})...")
        self.logger.info(f"Using concurrent generation with max_workers={min(100, len(problems))}")
        
        upgraded_problems = []
        
        # å®šä¹‰å•ä¸ªé¢˜ç›®çš„å‡çº§å‡½æ•°
        def upgrade_single_problem(idx_and_problem):
            """å‡çº§å•ä¸ªé¢˜ç›®"""
            idx, problem = idx_and_problem
            problem_id = problem.get('id', '')
            error_type = problem.get('error_type', None)  # é”™è¯¯ç±»å‹ï¼ˆå¦‚æœæœ‰ï¼‰
            
            self.logger.info(f"ğŸ¤– [Thread-{idx+1}] Upgrading problem {idx+1}/{len(problems)}: ID {problem_id}")
            
            # å‡†å¤‡é¢˜ç›®ä¿¡æ¯
            problem_text = problem.get("problem", "")
            answer_text = problem.get("reference_answer", "") or problem.get("answer", "")
            category_id = problem.get("category", 0)
            category_name = problem.get("category_name", "")
            image_path = problem.get("image_path")
            if isinstance(image_path, list) and image_path:
                image_path = image_path[0]
            elif not image_path:
                image_path = ""
            
            results = []
            current_reasoning_level = self._normalize_reasoning_level(problem.get("reasoning_level"))
            current_visual_level = self._normalize_visual_level(problem.get("visual_level"))
            
            # æ ¹æ®æ¨¡å¼å†³å®šå‡çº§å“ªäº›éš¾åº¦
            if upgrade_mode == "error_based" and error_type:
                # æ ¹æ®é”™è¯¯ç±»å‹å‡çº§ï¼šcaption é”™è¯¯ -> å‡çº§ visualï¼Œreasoning é”™è¯¯ -> å‡çº§ reasoning
                # åŒæ—¶ï¼Œé¢å¤–ç”Ÿæˆä¸€é“ç›¸ä¼¼éš¾åº¦çš„é¢˜ç›®ï¼ˆä¿è¯å¤šæ ·æ€§ï¼‰
                if error_type == "caption":
                    difficulty_aspects = ["visual", "similar"]
                    self.logger.info(f"  [Thread-{idx+1}] Caption error detected, upgrading VISUAL difficulty + generating SIMILAR problem for diversity")
                elif error_type == "reasoning":
                    difficulty_aspects = ["reasoning", "similar"]
                    self.logger.info(f"  [Thread-{idx+1}] Reasoning error detected, upgrading REASONING difficulty + generating SIMILAR problem for diversity")
                else:
                    # æœªçŸ¥é”™è¯¯ç±»å‹ï¼ŒåŒæ—¶å‡çº§ reasoningã€visual å’Œ similarï¼ˆä¿å®ˆç­–ç•¥ï¼Œå…¨é¢æå‡ï¼‰
                    difficulty_aspects = ["reasoning", "visual", "similar"]
                    self.logger.warning(f"  [Thread-{idx+1}] Unknown error type '{error_type}', upgrading reasoning + visual + similar (comprehensive improvement)")
            else:
                # é»˜è®¤æ¨¡å¼ï¼šå‡çº§ä¸¤ä¸ªéš¾åº¦
                difficulty_aspects = ["reasoning", "visual"]
            
            # å‡çº§æŒ‡å®šçš„éš¾åº¦
            for difficulty_aspect in difficulty_aspects:
                try:
                    target_reasoning_level, target_visual_level = self._compute_target_levels(
                        difficulty_aspect,
                        current_reasoning_level,
                        current_visual_level
                    )
                    self.logger.debug(
                        "  [Thread-%d] Levels for %s (aspect=%s): reasoning %.1fâ†’%.1f, visual %dâ†’%d",
                        idx + 1,
                        problem_id,
                        difficulty_aspect,
                        current_reasoning_level,
                        target_reasoning_level,
                        current_visual_level,
                        target_visual_level,
                    )
                    # è°ƒç”¨ Gemini å‡çº§éš¾åº¦
                    upgraded = self.gemini_generator.upgrade_problem_difficulty(
                        problem=problem_text,
                        answer=answer_text,
                        image_path=image_path,
                        category_id=category_id,
                        category_name=category_name,
                        difficulty_aspect=difficulty_aspect,
                        current_reasoning_level=current_reasoning_level,
                        current_visual_level=current_visual_level,
                        target_reasoning_level=target_reasoning_level,
                        target_visual_level=target_visual_level,
                    )
                    
                    if upgraded:
                        # åˆ†é…æ–°çš„å”¯ä¸€IDï¼ˆçº¯æ•°å­—ï¼Œä¸åŸé¡¹ç›®ä¸€è‡´ï¼‰
                        generated_id = self.next_generated_id
                        self.next_generated_id += 1
                        
                        # è·å–é—®é¢˜æ–‡æœ¬å¹¶ç¡®ä¿ä»¥ <image> å¼€å¤´
                        new_problem_text = upgraded.get("question", "").strip()
                        if not new_problem_text.startswith("<image>"):
                            new_problem_text = "<image> " + new_problem_text
                        
                        new_problem = {
                            "id": generated_id,  # çº¯æ•°å­— ID
                            "problem": new_problem_text,  # ç¡®ä¿æœ‰ <image> token
                            "answer": upgraded.get("answer", "").strip(),
                            "category": category_id,  # ç±»åˆ« ID
                            "category_name": category_name,
                            "image_path": [],
                            "image_code": upgraded.get("image_code", ""),
                            "difficulty_type": difficulty_aspect,
                            "original_problem_id": problem_id,
                            "reasoning_level": target_reasoning_level,
                            "visual_level": target_visual_level,
                        }
                        
                        # å¦‚æœæ˜¯é”™é¢˜å‡çº§ï¼Œè®°å½•åŸå§‹é”™è¯¯ç±»å‹
                        if error_type:
                            new_problem["source_error_type"] = error_type
                        
                        results.append(new_problem)
                    else:
                        self.logger.warning(f"  [Thread-{idx+1}] Failed to upgrade {difficulty_aspect} for {problem_id}")
                
                except Exception as e:
                    self.logger.error(f"  [Thread-{idx+1}] Error upgrading {difficulty_aspect} for {problem_id}: {e}")
            
            self.logger.info(f"âœ… [Thread-{idx+1}] Generated {len(results)} upgraded problems for {problem_id}")
            return results
        
        # è¾¹ç•Œæ£€æŸ¥ï¼šå¦‚æœæ²¡æœ‰é¢˜ç›®éœ€è¦å‡çº§ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        if not problems:
            self.logger.warning("No problems to upgrade, returning empty list")
            upgraded_file = upgraded_dir / "upgraded_problems.json"
            with open(upgraded_file, 'w', encoding='utf-8') as f:
                json.dump([], f, ensure_ascii=False, indent=2)
            return []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œ
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        max_workers = min(50, len(problems))
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = {
                executor.submit(upgrade_single_problem, (idx, problem)): idx
                for idx, problem in enumerate(problems)
            }
            
            # æŒ‰å®Œæˆé¡ºåºå¤„ç†ç»“æœ
            for future in as_completed(futures):
                try:
                    results = future.result()
                    upgraded_problems.extend(results)
                except Exception as e:
                    self.logger.error(f"âŒ Failed to process problem: {e}")
        
        self.logger.info(f"âœ… Successfully generated {len(upgraded_problems)} upgraded problems")
        
        # ä¿å­˜å‡çº§åçš„é¢˜ç›®
        upgraded_file = upgraded_dir / "upgraded_problems.json"
        with open(upgraded_file, 'w', encoding='utf-8') as f:
            json.dump(upgraded_problems, f, ensure_ascii=False, indent=2)
        
        return upgraded_problems
    


    def _materialise_problem_images(
        self,
        round_num: int,
        problems: List[Dict[str, Any]],
        round_dir: Path
    ) -> List[Dict[str, Any]]:
        """æ‰§è¡Œ image_code ç”Ÿæˆå›¾ç‰‡ï¼Œæœ€å¤šé‡è¯•3æ¬¡ï¼Œå¤±è´¥åˆ™ä¸¢å¼ƒé¢˜ç›®ï¼ˆä¸åŸé¡¹ç›®ä¸€è‡´ï¼‰"""
        upgraded_dir = round_dir / "upgraded"
        
        # è¿‡æ»¤å‡ºéœ€è¦ç”Ÿæˆå›¾ç‰‡çš„é¢˜ç›®
        problems_with_code = [p for p in problems if p.get("image_code")]
        problems_without_code = [p for p in problems if not p.get("image_code")]
        
        if not problems_with_code:
            self.logger.info("No problems require image generation")
            return problems
        
        self.logger.info(f"Generating images for {len(problems_with_code)} problems...")
        
        # ç”¨äºç»Ÿè®¡å›¾åƒç”Ÿæˆæ—¶é—´
        image_gen_times = []
        image_gen_start = time.time()
        
        try:
            import matplotlib
            import matplotlib.pyplot as plt
            from matplotlib.figure import Figure
            import numpy as np
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']
            matplotlib.rcParams['axes.unicode_minus'] = False
        except Exception as exc:
            self.logger.error(f"matplotlib/numpy not available: {exc}")
            self.logger.warning("Discarding all problems that require images")
            return problems_without_code
        
        # å°†å›¾ç‰‡è½åœ°åˆ°å½“å‰è½®æ¬¡ç›®å½•ï¼Œé¿å…è·¨è½®è¦†ç›–/åˆ é™¤
        image_dir = round_dir / "generated_images"
        image_dir.mkdir(parents=True, exist_ok=True)
        
        max_retries = 0  # æœ€å¤šé‡è¯•2æ¬¡ï¼ˆæ€»å…±3æ¬¡å°è¯•ï¼‰
        successful_problems = []
        failed_problems = []
        
        for problem in problems_with_code:
            problem_start_time = time.time()
            problem_id = problem.get("id", "")
            image_code = problem.get("image_code", "")
            
            # ä½¿ç”¨ id ä½œä¸ºå›¾ç‰‡åç§°
            output_path = image_dir / f"{problem_id}.png"

            # å¦‚æœå·²ç»å­˜åœ¨æœ‰æ•ˆå›¾ç‰‡ä¸”é¢˜ç›®è®°å½•äº† image_pathï¼Œåˆ™è·³è¿‡é‡æ–°ç”Ÿæˆ
            existing_paths = problem.get("image_path") or []
            if output_path.exists() and any(str(output_path) == p for p in existing_paths):
                # self.logger.debug(f"Image already exists for {problem_id}, skipping regeneration")
                successful_problems.append(problem)
                continue
            
            current_code = image_code
            success = False
            error_history = []
            
            for attempt in range(max_retries + 1):
                namespace = {"plt": plt, "np": np}
                original_savefig = plt.savefig
                original_fig_savefig = Figure.savefig
                
                def plt_savefig_override(path, *args, **kwargs):
                    return original_savefig(output_path, *args, **kwargs)
                
                def fig_savefig_override(self, path, *args, **kwargs):
                    return original_fig_savefig(self, output_path, *args, **kwargs)
                
                plt.savefig = plt_savefig_override
                Figure.savefig = fig_savefig_override
                
                try:
                    # è®¾ç½®è¶…æ—¶ï¼ˆ30ç§’ï¼‰
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(30)
                    
                    # ä¿®å¤ï¼šä¸æä¾›å±€éƒ¨å‘½åç©ºé—´ï¼Œè®©æ‰€æœ‰å˜é‡éƒ½åœ¨åŒä¸€ä¸ªå‘½åç©ºé—´ä¸­
                    # è¿™æ ·å‡½æ•°å†…éƒ¨å¯ä»¥æ­£ç¡®è®¿é—®å¤–éƒ¨å®šä¹‰çš„å˜é‡
                    exec(current_code, namespace)
                    
                    # å–æ¶ˆè¶…æ—¶
                    signal.alarm(0)
                    
                    if output_path.exists():
                        # æ›´æ–° image_path ä¸ºç»å¯¹è·¯å¾„ï¼Œç¡®ä¿åç»­è½®æ¬¡èƒ½è®¿é—®
                        problem["image_path"] = [str(output_path)]
                        self.logger.debug(f"âœ“ Generated image for {problem_id}")
                        success = True
                    else:
                        error_msg = "Code executed but no image file created"
                        error_history.append({
                            "attempt": attempt + 1,
                            "error": error_msg,
                            "code": current_code  # è®°å½•å®Œæ•´ä»£ç 
                        })
                        self.logger.warning(f"âœ— Image generation failed for {problem_id} (attempt {attempt+1}): {error_msg}")
                
                except Exception as exc:
                    error_message = f"{type(exc).__name__}: {str(exc)}"
                    error_history.append({
                        "attempt": attempt + 1,
                        "error": error_message,
                        "code": current_code  # è®°å½•å®Œæ•´ä»£ç 
                    })
                    self.logger.warning(f"âœ— Image generation failed for {problem_id} (attempt {attempt+1}): {error_message}")
                    
                    if output_path.exists():
                        output_path.unlink(missing_ok=True)
                    
                    # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œè°ƒç”¨ Gemini ä¿®å¤ä»£ç 
                    if attempt < max_retries:
                        self.logger.info(f"  Asking Gemini to fix code for {problem_id}...")
                        try:
                            fixed_code = self.gemini_generator.fix_image_code(
                                question=problem.get("problem", ""),
                                answer=problem.get("answer", ""),
                                original_code=current_code,
                                error_message=error_message,
                            )
                            
                            if fixed_code:
                                self.logger.info(f"  Got fixed code from Gemini, retrying...")
                                current_code = fixed_code
                                problem["image_code"] = fixed_code  # æ›´æ–°ä»£ç 
                            else:
                                self.logger.warning(f"  Gemini could not fix code, giving up")
                                break
                        except Exception as fix_exc:
                            self.logger.error(f"  Error calling Gemini to fix code: {fix_exc}")
                            break
                
                finally:
                    # å–æ¶ˆè¶…æ—¶ï¼ˆé˜²æ­¢å¼‚å¸¸æ—¶æœªå–æ¶ˆï¼‰
                    signal.alarm(0)
                    plt.savefig = original_savefig
                    Figure.savefig = original_fig_savefig
                    plt.close("all")
                
                if success:
                    break
            
            # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
            if success:
                problem_elapsed = time.time() - problem_start_time
                image_gen_times.append(problem_elapsed)
                successful_problems.append(problem)
            else:
                self.logger.error(f"âœ— All attempts failed for {problem_id}, discarding this problem")
                # è®°å½•å®Œæ•´çš„å¤±è´¥ä¿¡æ¯ï¼ˆä¸åŸé¡¹ç›®ä¸€è‡´ï¼‰
                failed_problems.append({
                    "id": problem_id,  # ä½¿ç”¨ id å­—æ®µ
                    "category": problem.get("category"),
                    "category_name": problem.get("category_name"),
                    "problem": problem.get("problem"),
                    "answer": problem.get("answer"),
                    "original_problem_id": problem.get("original_problem_id"),
                    "difficulty_type": problem.get("difficulty_type"),
                    "original_image_code": image_code,  # åŸå§‹çš„ä»£ç 
                    "error_history": error_history,
                    "total_attempts": len(error_history)
                })
        
        # ä¿å­˜å¤±è´¥è®°å½•
        if failed_problems:
            failed_file = upgraded_dir / "failed_image_generation.json"
            with open(failed_file, 'w', encoding='utf-8') as f:
                json.dump(failed_problems, f, ensure_ascii=False, indent=2)
            self.logger.warning(f"Saved {len(failed_problems)} failed image generation records to {failed_file}")
        
        # åˆå¹¶æˆåŠŸçš„é¢˜ç›®å’Œä¸éœ€è¦å›¾ç‰‡çš„é¢˜ç›®
        all_successful = successful_problems + problems_without_code
        
        # è®¡ç®—å›¾åƒç”Ÿæˆç»Ÿè®¡
        image_gen_total_time = time.time() - image_gen_start
        
        self.logger.info(f"Image generation completed:")
        self.logger.info(f"  Success: {len(successful_problems)}")
        self.logger.info(f"  Failed (discarded): {len(failed_problems)}")
        self.logger.info(f"  No image required: {len(problems_without_code)}")
        self.logger.info(f"  Total remaining: {len(all_successful)}")
        
        if image_gen_times:
            avg_time = sum(image_gen_times) / len(image_gen_times)
            max_time = max(image_gen_times)
            min_time = min(image_gen_times)
            self.logger.info(f"â±ï¸  Image generation stats:")
            self.logger.info(f"    Total time: {image_gen_total_time:.2f}s ({image_gen_total_time/60:.2f}m)")
            self.logger.info(f"    Avg per image: {avg_time:.2f}s")
            self.logger.info(f"    Min/Max: {min_time:.2f}s / {max_time:.2f}s")
        
        return all_successful
    
    def _save_round_summary(self, round_num: int) -> None:
        """ä¿å­˜è½®æ¬¡æ€»ç»“"""
        round_dir = self.config.output_dir / f"round_{round_num}"
        summary_file = round_dir / "round_summary.json"
        
        summary = {
            "round": round_num,
            "model_path": self.current_model_path,
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.round_history.append(summary)

