"""
DPO è®­ç»ƒå™¨ - ç›´æ¥è°ƒç”¨ LLaMA-Factory

åŠŸèƒ½ï¼š
1. å‡†å¤‡ LLaMA-Factory æ•°æ®é›†é…ç½®
2. ç”Ÿæˆ DPO è®­ç»ƒé…ç½®æ–‡ä»¶
3. è°ƒç”¨ LLaMA-Factory CLI è¿›è¡Œè®­ç»ƒ
"""

import json
import logging
import os
import subprocess
import yaml  # ä½¿ç”¨ yaml åº“ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼ˆä¸åŸé¡¹ç›®ä¸€è‡´ï¼‰
from pathlib import Path
from typing import Optional, Dict, Any


logger = logging.getLogger(__name__)


class DPOTrainer:
    """DPO è®­ç»ƒå™¨ - ä½¿ç”¨ LLaMA-Factory"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # LLaMA-Factory è·¯å¾„
        self.llamafactory_path = Path("/mnt/petrelfs/shangxiaoran/math_generation6/LLaMA-Factory")
        if not self.llamafactory_path.exists():
            self.logger.warning(f"LLaMA-Factory not found at {self.llamafactory_path}")
    
    def prepare_dataset_info(
        self,
        dataset_name: str,
        dataset_file: Path,
        dataset_info_path: Path
    ) -> None:
        """
        å‡†å¤‡ LLaMA-Factory çš„ dataset_info.json
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            dataset_file: DPO æ•°æ®æ–‡ä»¶è·¯å¾„
            dataset_info_path: dataset_info.json ä¿å­˜è·¯å¾„
        """
        # LLaMA-Factory æ•°æ®é›†é…ç½®æ ¼å¼
        # ğŸ”§ ä½¿ç”¨ sharegpt æ ¼å¼ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ•°æ®ä½¿ç”¨ conversations
        dataset_info = {
            dataset_name: {
                "file_name": str(dataset_file),
                "formatting": "sharegpt",  # ä½¿ç”¨ ShareGPT æ ¼å¼
                "ranking": True,  # å¯ç”¨ DPO
                "columns": {
                    "messages": "conversations",
                    "chosen": "chosen",
                    "rejected": "rejected",
                    "images": "images",
                    "system": "system"  # æ·»åŠ  system prompt æ˜ å°„
                },
                # ğŸ”§ æ·»åŠ role tagsä»¥åŒ¹é…æˆ‘ä»¬çš„æ•°æ®æ ¼å¼
                "tags": {
                    "role_tag": "from",
                    "content_tag": "value",
                    "user_tag": "user",
                    "assistant_tag": "assistant"
                }
            }
        }
        
        # å¦‚æœå·²å­˜åœ¨ dataset_info.jsonï¼Œåˆ™æ›´æ–°å®ƒ
        if dataset_info_path.exists():
            try:
                with open(dataset_info_path, 'r', encoding='utf-8') as f:
                    existing_info = json.load(f)
                existing_info.update(dataset_info)
                dataset_info = existing_info
            except Exception as e:
                self.logger.warning(f"Failed to load existing dataset_info.json: {e}, will create new one")
        
        # ä¿å­˜
        dataset_info_path.parent.mkdir(parents=True, exist_ok=True)
        with open(dataset_info_path, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"Dataset info updated/created: {dataset_info_path}")
        self.logger.info(f"  Added/Updated dataset: {dataset_name}")
    
    def create_training_config(
        self,
        dataset_name: str,
        model_path: Path,
        output_dir: Path,
        config_path: Path
    ) -> Path:
        """
        åˆ›å»º LLaMA-Factory è®­ç»ƒé…ç½®æ–‡ä»¶ï¼ˆYAML æ ¼å¼ï¼‰
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            model_path: æ¨¡å‹è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            config_path: é…ç½®æ–‡ä»¶ä¿å­˜è·¯å¾„
        
        Returns:
            é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # ä»ç¯å¢ƒå˜é‡è·å– GPU é…ç½®ï¼ˆæ”¯æŒæµ‹è¯•è„šæœ¬ï¼‰
        import os
        num_gpus = int(os.environ.get('DPO_NUM_GPUS', self.config.dpo_num_gpus))
        batch_size = int(os.environ.get('DPO_BATCH_SIZE', self.config.dpo_batch_size))
        gradient_accumulation = int(os.environ.get('DPO_GRADIENT_ACCUMULATION', self.config.dpo_gradient_accumulation))
        num_epochs = int(os.environ.get('DPO_NUM_EPOCHS', self.config.dpo_epochs))
        learning_rate = float(os.environ.get('DPO_LEARNING_RATE', self.config.dpo_learning_rate))
        
        self.logger.info(f"DPO Training Config: GPUs={num_gpus}, batch_size={batch_size}, "
                        f"grad_accum={gradient_accumulation}, epochs={num_epochs}, lr={learning_rate}")
        
        # æ„å»ºè®­ç»ƒå‚æ•°ï¼ˆå‚è€ƒ qwen2_5vl_full_sft.yaml + qwen2_5vl_lora_dpo.yamlï¼‰
        training_args = {
            # === model ===
            "model_name_or_path": str(model_path),
            "image_max_pixels": 262144,
            "video_max_pixels": 16384,
            "trust_remote_code": True,
            "model_revision": "main",  # æœ¬åœ°æ¨¡å‹ä½¿ç”¨mainåˆ†æ”¯
            
            # === method ===
            "stage": "dpo",
            "do_train": True,
            "finetuning_type": "full",  
            "freeze_vision_tower": self.config.dpo_freeze_vision_tower,  # æ˜¯å¦å†»ç»“è§†è§‰ç¼–ç å™¨
            "freeze_multi_modal_projector": self.config.dpo_freeze_projector,  # æ˜¯å¦å†»ç»“æŠ•å½±å±‚
            "freeze_language_model": self.config.dpo_freeze_llm,  # æ˜¯å¦å†»ç»“è¯­è¨€æ¨¡å‹
            "pref_beta": self.config.dpo_beta,
            "pref_loss": "sigmoid", 
            "deepspeed": "/mnt/petrelfs/shangxiaoran/math_generation6/LLaMA-Factory/examples/deepspeed/ds_z3_config.json",  
            
            # === dataset ===
            "dataset": dataset_name,
            "template": self.config.dpo_template,
            "cutoff_len": self.config.dpo_max_length,
            "overwrite_cache": True,
            "preprocessing_num_workers": 16, 
            "dataloader_num_workers": 4,
            
            # === output ===
            "output_dir": str(output_dir),
            "logging_steps": 10,
            "save_steps": 500,
            "plot_loss": True,
            "overwrite_output_dir": True,
            "save_only_model": False,
            "report_to": "none",  
            
            # === train ===
            "per_device_train_batch_size": batch_size,
            "gradient_accumulation_steps": gradient_accumulation,
            "learning_rate": learning_rate,
            "num_train_epochs": num_epochs,
            "lr_scheduler_type": "cosine",
            "warmup_ratio": 0.1,
            "bf16": True,
            "gradient_checkpointing": True,  
            "ddp_timeout": 180000000,
            "resume_from_checkpoint": None,
        }
        

        
        
        # ä¿å­˜ä¸º YAML æ ¼å¼ï¼ˆä½¿ç”¨ yaml.dumpï¼Œä¸åŸé¡¹ç›®ä¸€è‡´ï¼‰
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(training_args, f, allow_unicode=True, default_flow_style=False)
        
        self.logger.info(f"Training config saved to: {config_path}")
        
        return config_path
    
    def train(
        self,
        dataset_name: str,
        dpo_data_file: Path,
        model_path: Path,
        output_dir: Path,
        round_num: int
    ) -> bool:
        """
        æ‰§è¡Œ DPO è®­ç»ƒ
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            dpo_data_file: DPO æ•°æ®æ–‡ä»¶è·¯å¾„
            model_path: æ¨¡å‹è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            round_num: å½“å‰è½®æ¬¡
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info(f"=" * 80)
            self.logger.info(f"Starting DPO training for Round {round_num}")
            self.logger.info(f"=" * 80)
            
            # 1. å‡†å¤‡ dataset_info.json
            dataset_info_path = self.llamafactory_path / "data" / "dataset_info.json"
            self.prepare_dataset_info(dataset_name, dpo_data_file, dataset_info_path)
            
            # 2. åˆ›å»ºè®­ç»ƒé…ç½®
            config_dir = output_dir / "dpo_configs"
            config_path = config_dir / f"dpo_config_round_{round_num}.yaml"
            self.create_training_config(dataset_name, model_path, output_dir, config_path)
            
            # 3. è°ƒç”¨ LLaMA-Factory è®­ç»ƒ
            self.logger.info("Calling LLaMA-Factory for DPO training...")
            
            # æ„å»ºå‘½ä»¤ - ç›´æ¥è°ƒç”¨æœ¬åœ°LLaMA-Factoryçš„Pythonæ¨¡å—ï¼Œé¿å…ä½¿ç”¨å…¨å±€çš„llamafactory-cli
            # è¿™æ ·ç¡®ä¿ä½¿ç”¨çš„æ˜¯math_generation6ç›®å½•ä¸‹çš„LLaMA-Factoryï¼Œè€Œä¸æ˜¯system-wideå®‰è£…çš„ç‰ˆæœ¬
            cmd = [
                "python3", "-m", "llamafactory.cli", "train",
                str(config_path)
            ]
            
            self.logger.info(f"Command: {' '.join(cmd)}")
            self.logger.info(f"Working directory: {self.llamafactory_path}")
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            env['HF_HUB_OFFLINE'] = '1'  # å®Œå…¨ç¦»çº¿æ¨¡å¼
            env['TRANSFORMERS_OFFLINE'] = '1'  # Transformersç¦»çº¿æ¨¡å¼
            env['HF_DATASETS_OFFLINE'] = '1'  # Datasetsç¦»çº¿æ¨¡å¼
            env['DATASETS_VERBOSITY'] = 'error'  # å‡å°‘datasetsæ—¥å¿—
            # ğŸ”§ ç¦ç”¨datasetsç¼“å­˜ä»¥é¿å…å¤šGPUç«æ€æ¡ä»¶
            env['HF_DATASETS_CACHE'] = '/tmp/hf_datasets_cache_dpo'  # ä½¿ç”¨ä¸´æ—¶ç¼“å­˜ç›®å½•
            os.makedirs('/tmp/hf_datasets_cache_dpo', exist_ok=True)
            
            # ğŸ”§ è®¾ç½®PYTHONPATHç¡®ä¿ä½¿ç”¨æœ¬åœ°çš„LLaMA-Factory
            llamafactory_src = str(self.llamafactory_path / "src")
            if 'PYTHONPATH' in env:
                env['PYTHONPATH'] = f"{llamafactory_src}:{env['PYTHONPATH']}"
            else:
                env['PYTHONPATH'] = llamafactory_src
            self.logger.info(f"PYTHONPATH: {env['PYTHONPATH']}")
            
            # æ‰§è¡Œè®­ç»ƒ
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                cwd=str(self.llamafactory_path),
                env=env
            )
            
            # å®æ—¶è¾“å‡ºæ—¥å¿—
            for line in process.stdout:
                line = line.rstrip()
                if line:
                    self.logger.info(line)
            
            # ç­‰å¾…å®Œæˆ
            return_code = process.wait()
            
            if return_code == 0:
                self.logger.info("âœ… DPO training completed successfully")
                return True
            else:
                self.logger.error(f"âŒ DPO training failed with code {return_code}")
                return False
        
        except Exception as e:
            self.logger.error(f"Error during DPO training: {e}", exc_info=True)
            return False


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from config import get_default_config
    
    config = get_default_config()
    trainer = DPOTrainer(config)
    
    # æµ‹è¯•æ•°æ®é›†é…ç½®
    dataset_name = "test_dpo_round_1"
    dataset_file = Path("./test_dpo_data.json")
    dataset_info_path = Path("./test_dataset_info.json")
    
    trainer.prepare_dataset_info(dataset_name, dataset_file, dataset_info_path)
    
    print("âœ… DPO Trainer test completed")

