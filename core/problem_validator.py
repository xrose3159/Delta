"""
é—®é¢˜éªŒè¯å™¨ï¼šä½¿ç”¨æŒ‡å®šæ¨¡å‹éªŒè¯æ–°é¢˜çš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
"""

import json
import logging
import re
from pathlib import Path
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)


class ProblemValidator:
    """é—®é¢˜éªŒè¯å™¨ç±»"""
    
    def __init__(self, config, llm_generator, llm_judge):
        """
        åˆå§‹åŒ–é—®é¢˜éªŒè¯å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
            llm_generator: LLM Generator å®ä¾‹
            llm_judge: LLM Judge å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.config = config
        self.llm_generator = llm_generator
        self.llm_judge = llm_judge
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def validate_new_problems(
        self,
        round_num: int,
        problems: List[Dict[str, Any]],
        round_dir: Path
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹éªŒè¯æ–°é¢˜çš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
        
        å°†é¢˜ç›®å’Œ Gemini ç­”æ¡ˆä¸€èµ·æä¾›ç»™æ¨¡å‹ï¼Œè®©æ¨¡å‹åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
        
        Args:
            round_num: è½®æ¬¡ç¼–å·
            problems: å¾…éªŒè¯çš„æ–°é¢˜åˆ—è¡¨
            round_dir: è½®æ¬¡ç›®å½•
            
        Returns:
            éªŒè¯é€šè¿‡çš„é¢˜ç›®åˆ—è¡¨
        """
        # ä»é…ç½®ä¸­è·å–éªŒè¯æ¨¡å‹è·¯å¾„
        validation_model_path = self.config.llm_generator_model_path
        if not problems:
            self.logger.info("No problems to validate")
            return []
        
        if not self.llm_generator:
            self.logger.warning("LLM Generator not available, skipping validation")
            return problems
        
        validation_dir = round_dir / "validation"
        validation_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"Validating {len(problems)} new problems with model: {validation_model_path}")
        
        # 1. å‡†å¤‡éªŒè¯è¾“å…¥æ•°æ®ï¼ˆå°†é¢˜ç›®å’Œ Gemini ç­”æ¡ˆä¸€èµ·æä¾›ç»™æ¨¡å‹ï¼‰
        validation_inputs = []
        problem_ids = []
        
        for problem in problems:
            problem_id = problem.get('id')
            if problem_id is None:
                problem_id = hash(problem.get('problem', '')) % 100000
            problem_ids.append(problem_id)
            
            problem_text = problem.get('problem', '')
            gemini_answer = problem.get('answer', '').strip()
            
            # image_path å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼Œç»Ÿä¸€è½¬æ¢ä¸ºåˆ—è¡¨
            image_path_raw = problem.get('image_path', '')
            if isinstance(image_path_raw, list):
                image_paths = image_path_raw
            elif isinstance(image_path_raw, str) and image_path_raw:
                image_paths = [image_path_raw]
            else:
                image_paths = []
            
            # æ„å»ºéªŒè¯ promptï¼šè®©æ¨¡å‹åˆ¤æ–­ç»™å®šçš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
            validation_prompt = self._create_validation_prompt(problem_text, gemini_answer)
            
            validation_inputs.append({
                "id": problem_id,
                "problem": validation_prompt,
                "image_path": image_paths
            })
        
        # 2. ä¿å­˜éªŒè¯è¾“å…¥æ–‡ä»¶
        validation_input_file = validation_dir / "validation_input.json"
        with open(validation_input_file, 'w', encoding='utf-8') as f:
            json.dump(validation_inputs, f, ensure_ascii=False, indent=2)
        
        # 3. ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆåˆ¤æ–­ç»“æœ
        validation_output_file = validation_dir / "validation_output.json"
        self.logger.info("Asking model to judge if the answer is correct...")
        success = self.llm_generator.generate_predictions(
            test_file=validation_input_file,
            output_file=validation_output_file,
            model_path=validation_model_path,
            max_tokens=30000,  # å…è®¸æ¨¡å‹å®Œæ•´æ€è€ƒè¿‡ç¨‹
            disable_thinking=False,  # å…è®¸æ¨¡å‹æ€è€ƒï¼Œç„¶åä»è¾“å‡ºä¸­æå–æœ€åä¸€ä¸ªåˆ¤æ–­è¯
            is_validation=True  # ä½¿ç”¨ç­”æ¡ˆéªŒè¯é…ç½®ï¼ˆä¸é”™è¯¯åˆ†æç›¸åŒï¼‰
        )
        
        if not success or not validation_output_file.exists():
            self.logger.error("Failed to generate validation judgments, keeping all problems")
            return problems
        
        # 4. è¯»å–ç”Ÿæˆçš„åˆ¤æ–­ç»“æœ
        with open(validation_output_file, 'r', encoding='utf-8') as f:
            validation_results = json.load(f)
        
        # 5. è§£æåˆ¤æ–­ç»“æœï¼Œç­›é€‰éªŒè¯é€šè¿‡çš„é¢˜ç›®
        validated_problems = []
        for i, problem in enumerate(problems):
            problem_id = problem_ids[i]
            judgment_text = ""
            
            # æ‰¾åˆ°å¯¹åº”çš„ç”Ÿæˆç»“æœ
            for result in validation_results:
                if result.get('id') == problem_id:
                    judgment_text = result.get('predict', '').strip()
                    break
            
            if not judgment_text:
                self.logger.warning(f"No judgment found for problem {problem_id}, will be discarded")
                continue
            
            # è§£æåˆ¤æ–­ç»“æœ
            is_correct = self._parse_judgment(judgment_text)
            
            if is_correct:
                validated_problems.append(problem)
                self.logger.debug(f"Problem {problem_id} passed validation (answer is correct)")
            else:
                self.logger.debug(f"Problem {problem_id} failed validation (answer is incorrect)")
        
        return validated_problems
    
    def _create_validation_prompt(self, problem_text: str, answer: str) -> str:
        """
        åˆ›å»ºéªŒè¯ promptï¼šè®©æ¨¡å‹åˆ¤æ–­ç»™å®šçš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
        
        æ³¨æ„ï¼šè¾“å‡ºè¦æ±‚å·²ç»åœ¨ System prompt ä¸­å®šä¹‰ï¼Œè¿™é‡Œåªæä¾›é¢˜ç›®å’Œç­”æ¡ˆ
        
        Args:
            problem_text: é¢˜ç›®æ–‡æœ¬
            answer: Gemini ç”Ÿæˆçš„ç­”æ¡ˆ
            
        Returns:
            éªŒè¯ promptï¼ˆåªåŒ…å«é¢˜ç›®å’Œç­”æ¡ˆï¼Œä¸åŒ…å«é‡å¤çš„è¾“å‡ºè¦æ±‚ï¼‰
        """
        prompt = f"""## PROBLEM
{problem_text}

## GIVEN ANSWER
{answer}

Analyze the problem and the given answer step by step. Determine if the given answer is mathematically correct and solves the problem correctly. After your analysis, end your response with exactly one of these tags: <correct> or <incorrect>."""
        return prompt
    
    def _parse_judgment(self, judgment_text: str) -> bool:
        """
        è§£æåˆ¤æ–­ç»“æœï¼Œæå– <correct> æˆ– <incorrect> æ ‡ç­¾
        
        Args:
            judgment_text: æ¨¡å‹ç”Ÿæˆçš„åˆ¤æ–­æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å« thinking å†…å®¹ï¼‰
            
        Returns:
            True å¦‚æœåˆ¤æ–­ä¸ºæ­£ç¡®ï¼ŒFalse å¦‚æœåˆ¤æ–­ä¸ºé”™è¯¯
        """
        import re
        
        # å¤„ç† <think> å‰ç¼€ï¼šç§»é™¤å‰ç¼€ï¼Œåªä¿ç•™å®é™…è¾“å‡º
        text_to_parse = judgment_text
        if text_to_parse.startswith("<think>"):
            text_to_parse = text_to_parse[len("<think>"):].strip()
        
        # å¤„ç† <think>...</think> æ ‡ç­¾ï¼šæå–æ ‡ç­¾å¤–çš„å†…å®¹
        pattern_redacted = r'<think>(.*?)</think>'
        matches_redacted = re.findall(pattern_redacted, text_to_parse, re.DOTALL)
        
        if matches_redacted:
            # å¦‚æœæ‰¾åˆ°æ ‡ç­¾ï¼Œæå–æ ‡ç­¾å¤–çš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
            text_without_tags = re.sub(pattern_redacted, '', text_to_parse, flags=re.DOTALL).strip()
            if text_without_tags:
                # å¦‚æœæ ‡ç­¾å¤–æœ‰å†…å®¹ï¼Œä¼˜å…ˆä½¿ç”¨æ ‡ç­¾å¤–çš„å†…å®¹
                text_to_parse = text_without_tags
            else:
                # å¦‚æœæ•´ä¸ªè¾“å‡ºéƒ½åœ¨æ ‡ç­¾å†…ï¼Œä½¿ç”¨æ ‡ç­¾å†…çš„æœ€åä¸€éƒ¨åˆ†
                text_to_parse = matches_redacted[-1] if matches_redacted else text_to_parse
        
        # ğŸ”‘ å…³é”®ï¼šæŸ¥æ‰¾ <correct> æˆ– <incorrect> æ ‡ç­¾
        # ä¼˜å…ˆæŸ¥æ‰¾æ ‡ç­¾æ ¼å¼
        correct_tag_pattern = r'<correct>'
        incorrect_tag_pattern = r'<incorrect>'
        
        correct_tag_matches = list(re.finditer(correct_tag_pattern, text_to_parse, re.IGNORECASE))
        incorrect_tag_matches = list(re.finditer(incorrect_tag_pattern, text_to_parse, re.IGNORECASE))
        
        if correct_tag_matches or incorrect_tag_matches:
            # æ‰¾åˆ°æœ€åä¸€ä¸ªæ ‡ç­¾çš„ä½ç½®
            last_correct_pos = max([m.start() for m in correct_tag_matches]) if correct_tag_matches else -1
            last_incorrect_pos = max([m.start() for m in incorrect_tag_matches]) if incorrect_tag_matches else -1
            
            # æ¯”è¾ƒæœ€åä¸€ä¸ªæ ‡ç­¾çš„ä½ç½®
            if last_correct_pos > last_incorrect_pos:
                return True
            elif last_incorrect_pos > last_correct_pos:
                return False
            elif last_correct_pos != -1:  # åªæœ‰ <correct>ï¼Œæ²¡æœ‰ <incorrect>
                return True
            elif last_incorrect_pos != -1:  # åªæœ‰ <incorrect>ï¼Œæ²¡æœ‰ <correct>
                return False
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œå‘åå…¼å®¹ï¼šæŸ¥æ‰¾æœ€åä¸€ä¸ª "correct" æˆ– "incorrect" å•è¯
        judgment_text_lower = text_to_parse.lower().strip()
        
        correct_positions = []
        incorrect_positions = []
        
        # æŸ¥æ‰¾æ‰€æœ‰ "correct" çš„ä½ç½®ï¼ˆä½¿ç”¨å•è¯è¾¹ç•Œï¼Œé¿å…åŒ¹é…åˆ° "incorrect" ä¸­çš„ "correct"ï¼‰
        for match in re.finditer(r'\bcorrect\b', judgment_text_lower):
            correct_positions.append(match.start())
        
        # æŸ¥æ‰¾æ‰€æœ‰ "incorrect" çš„ä½ç½®
        for match in re.finditer(r'\bincorrect\b', judgment_text_lower):
            incorrect_positions.append(match.start())
        
        # æ‰¾åˆ°æœ€åä¸€ä¸ªåŒ¹é…çš„ä½ç½®
        last_correct_pos = max(correct_positions) if correct_positions else -1
        last_incorrect_pos = max(incorrect_positions) if incorrect_positions else -1
        
        # æ¯”è¾ƒæœ€åä¸€ä¸ª "correct" å’Œæœ€åä¸€ä¸ª "incorrect" çš„ä½ç½®
        if last_correct_pos > last_incorrect_pos:
            return True
        elif last_incorrect_pos > last_correct_pos:
            return False
        elif last_correct_pos != -1:  # åªæœ‰ correctï¼Œæ²¡æœ‰ incorrect
            return True
        elif last_incorrect_pos != -1:  # åªæœ‰ incorrectï¼Œæ²¡æœ‰ correct
            return False
        
        # é»˜è®¤ï¼šå¦‚æœæ²¡æœ‰æ˜ç¡®åˆ¤æ–­ï¼Œè®¤ä¸ºä¸æ­£ç¡®ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        self.logger.warning(f"Could not parse judgment (no <correct> or <incorrect> tag found): {judgment_text[-200:] if len(judgment_text) > 200 else judgment_text}")
        return False
    

