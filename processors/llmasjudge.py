import json
import os
import re
import argparse
from typing import List, Dict, Any, Tuple
from tqdm import tqdm
from vllm import LLM, SamplingParams
from collections import defaultdict
from transformers import AutoTokenizer

def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½ JSON æ•°ç»„æ ¼å¼çš„æ–‡ä»¶ã€‚
    æ³¨æ„ï¼šå‡½æ•°åä¿ç•™ä¸º load_jsonl ä»¥ä¿æŒå…¼å®¹æ€§ï¼Œä½†å®é™…åªæ”¯æŒ JSON æ•°ç»„æ ¼å¼ã€‚
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_jsonl(data: List[Dict[str, Any]], file_path: str):
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def save_json(data: List[Dict[str, Any]], file_path: str):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_json(file_path: str) -> List[Dict[str, Any]]:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    if isinstance(data, list):
        return data

    if isinstance(data, dict):
        if 'data' in data and isinstance(data['data'], list):
            return data['data']
        raise ValueError(f"Expected a list at the root of {file_path}, got dict without 'data' list.")

    raise ValueError(f"Unsupported JSON structure in {file_path}: expected list or dict containing 'data'.")

def load_data(file_path: str) -> List[Dict[str, Any]]:
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()

    if ext == '.jsonl':
        return load_jsonl(file_path)
    if ext == '.json':
        return load_json(file_path)

    raise ValueError(f"Unsupported input file extension '{ext}'. Expected '.jsonl' or '.json'.")

def save_data(data: List[Dict[str, Any]], file_path: str):
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()

    if ext == '.jsonl':
        save_jsonl(data, file_path)
    elif ext == '.json':
        save_json(data, file_path)
    else:
        raise ValueError(f"Unsupported output file extension '{ext}'. Expected '.jsonl' or '.json'.")

def create_comparison_prompt(question:str, solution: str, response: str) -> str:
    prompt = f"""You are an expert evaluator. Compare the reference and generated answers only for semantic correctness and factual agreement.

## TASK
Determine whether the two answers express the same correct solution. Focus on meaning, correctness, and final results rather than wording or format.

## EVALUATION GUIDELINES
- **EQUIVALENT**: same conclusion or final answer, no substantive factual differences.
- **DIFFERENT**: conflicting conclusions, missing required reasoning, or any factual mistake in the generated answer.

## INPUT QUESTION
{question}

## REFERENCE ANSWER
{solution}

## GENERATED ANSWER
{response}

## OUTPUT INSTRUCTIONS
Respond in the following two-line format (no extra text):
Analysis: <concise reasoning>
JUDGMENT: <EQUIVALENT or DIFFERENT>
"""

    return prompt

JUDGMENT_PATTERN = re.compile(r"^judgment\s*:\s*(equivalent|different)\s*$", re.IGNORECASE)

# ä¿®å¤ï¼šä½¿ç”¨æ›´æ™ºèƒ½çš„æ–¹æ³•åŒ¹é… \boxed{} ä¸­çš„å†…å®¹ï¼Œæ”¯æŒåµŒå¥—æ‹¬å·ï¼ˆå¦‚ \frac{1}{18}ï¼‰
def _extract_boxed_content(text: str) -> list:
    """æå–æ‰€æœ‰ \\boxed{} ä¸­çš„å†…å®¹ï¼Œæ­£ç¡®å¤„ç†åµŒå¥—æ‹¬å·"""
    results = []
    i = 0
    while i < len(text):
        # æŸ¥æ‰¾ \boxed{
        start = text.find('\\boxed{', i)
        if start == -1:
            break
        # ä» { å¼€å§‹è®¡æ•°æ‹¬å·
        brace_start = start + 7  # len('\\boxed{') = 7
        brace_count = 1
        j = brace_start
        while j < len(text) and brace_count > 0:
            if text[j] == '{':
                brace_count += 1
            elif text[j] == '}':
                brace_count -= 1
            j += 1
        if brace_count == 0:
            # æˆåŠŸæ‰¾åˆ°åŒ¹é…çš„ }
            results.append(text[brace_start:j-1])
            i = j
        else:
            # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ }ï¼Œè·³è¿‡è¿™ä¸ª \boxed{
            i = brace_start
    return results

ANSWER_TAG_PATTERN = re.compile(r"<answer>(.*?)</answer>", re.IGNORECASE | re.DOTALL)
ANSWER_PHRASE_PATTERNS = [
    re.compile(r"\banswer\s*:\s*([^\n]+)", re.IGNORECASE),
    re.compile(r"\bthe answer is\s*:\s*([^\n]+)", re.IGNORECASE),
    re.compile(r"\bthe answer is\s+([^\n]+)", re.IGNORECASE),
]


def _normalize_extracted_text(text: str) -> str:
    return text.strip().rstrip(". ").strip()


def extract_solution_text(solution: Any) -> str:
    if not isinstance(solution, str):
        return str(solution)

    # ä½¿ç”¨æ–°çš„åµŒå¥—æ‹¬å·åŒ¹é…å‡½æ•°
    boxed_matches = _extract_boxed_content(solution)
    if boxed_matches:
        return _normalize_extracted_text(boxed_matches[-1])

    answer_tag_match = ANSWER_TAG_PATTERN.search(solution)
    if answer_tag_match:
        return _normalize_extracted_text(answer_tag_match.group(1))

    for pattern in ANSWER_PHRASE_PATTERNS:
        phrase_match = pattern.search(solution)
        if phrase_match:
            return _normalize_extracted_text(phrase_match.group(1))

    return solution.strip()


def extract_response_text(response: Any) -> str:
    if not isinstance(response, str):
        return str(response)

    trimmed = response.strip()
    if not trimmed:
        return trimmed

    extracted = extract_solution_text(trimmed)
    return extracted or trimmed


def extract_judgment_batch(responses: List[str]) -> List[str]:
    judgments: List[str] = []
    for response in responses:
        parsed = 'unknown'
        for line in reversed(response.splitlines()):
            stripped = line.strip()
            if not stripped:
                continue
            match = JUDGMENT_PATTERN.match(stripped)
            if match:
                parsed = match.group(1).lower()
                break
        judgments.append(parsed)

    return judgments

def prepare_batch_prompts(
    data: List[Dict[str, Any]],
    tokenizer: AutoTokenizer
) -> Tuple[List[str], List[Tuple[int, int]]]:
    all_prompts = []
    all_indices = []

    for data_idx, item in enumerate(data):
        question = item.get('problem', '')
        answer = item.get('answer', '')
        if isinstance(answer, str) and answer.strip():
            solution = answer.strip()
        else:
            solution = extract_solution_text(item.get('solution', ''))
        
        # æ”¯æŒå¤šç§å­—æ®µåï¼špredictï¼ˆvLLMè¾“å‡ºï¼‰æˆ– model_predictionï¼ˆæ—§æ ¼å¼ï¼‰
        raw_response = item.get('predict', item.get('model_prediction', ''))

        response = extract_response_text(raw_response)
        if response and solution: 
            prompt = create_comparison_prompt(question, solution, response)
            messages = [{"role": "user", "content": prompt}]
            prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
            all_prompts.append(prompt)
            all_indices.append(data_idx)

    return all_prompts, all_indices

def batch_evaluate(
    data: List[Dict[str, Any]],
    llm: LLM,
    sampling_params: SamplingParams,
    tokenizer: AutoTokenizer
) -> List[Dict[str, Any]]:
    all_prompts, all_indices = prepare_batch_prompts(data, tokenizer)

    print(f"ğŸ”¥ Starting batch generation for {len(all_prompts)} prompts...", flush=True)
    outputs = llm.generate(all_prompts, sampling_params)

    print(f"âœ… Generation completed, processing outputs...", flush=True)
    
    # ğŸ”‘ å…³é”®ä¿®å¤ï¼šç«‹å³å°† outputs è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œé¿å…æ‡’åŠ è½½å¯¼è‡´çš„é˜»å¡
    # vLLM çš„ outputs å¯èƒ½æ˜¯ç”Ÿæˆå™¨æˆ–åŒ…å«æœªå®Œæˆçš„å¼‚æ­¥æ“ä½œ
    outputs_list = list(outputs)
    print(f"âœ… Converted {len(outputs_list)} outputs to list", flush=True)
    
    # æå–å“åº”æ–‡æœ¬
    responses = []
    for i, output in enumerate(outputs_list):
        try:
            text = output.outputs[0].text
            responses.append(text)
        except (IndexError, AttributeError) as e:
            print(f"âš ï¸  Warning: Failed to extract text from output {i}: {e}", flush=True)
            responses.append("")  # ä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºå ä½ç¬¦
    
    print(f"âœ… Extracted {len(responses)} responses, parsing judgments...", flush=True)
    judgments = extract_judgment_batch(responses)

    for data_idx, response, judgment in zip(all_indices, responses, judgments):
        if judgment == 'equivalent':
            score = True
        elif judgment == 'different':
            score = False
        else:
            score = None
        data[data_idx]['match_analysis'] = response
        data[data_idx]['matched'] = score

    return data


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument('--input_file', type=str, default='math_with_rollouts.jsonl')
    parser.add_argument('--output_file', type=str, default='math_with_failscores.jsonl')

    parser.add_argument('--model', type=str, default='/mnt/dhwfile/raise/user/zhuyun/Qwen3-4B-Instruct-2507') # /share/wulijun/panzhuoshi/huggingface/hub/models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5
    parser.add_argument('--tensor_parallel_size', type=int, default=1)
    parser.add_argument('--gpu_memory_utilization', type=float, default=0.8)

    parser.add_argument('--generation_size', type=int, default=8192)
    parser.add_argument('--temperature', type=float, default=0.6)
    parser.add_argument('--max_model_len', type=int, default=70000, help='Maximum context length for Judge model')

    args = parser.parse_args()

    data = load_data(args.input_file)

    llm = LLM(
        model=args.model,
        tensor_parallel_size=args.tensor_parallel_size,
        gpu_memory_utilization=args.gpu_memory_utilization,
        trust_remote_code=True,
        max_model_len=args.max_model_len  # ä½¿ç”¨å¯é…ç½®çš„ä¸Šä¸‹æ–‡é•¿åº¦
    )

    tokenizer = AutoTokenizer.from_pretrained(args.model)

    sampling_params = SamplingParams(
        temperature=args.temperature,
        max_tokens=args.generation_size,
    )

    data = batch_evaluate(
        data,
        llm,
        sampling_params,
        tokenizer
    )

    import os
    output_dir = os.path.dirname(args.output_file)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    save_data(data, args.output_file)

def evaluate_predictions_with_judge(
    predictions_path: str,
    eval_records: List[Dict[str, Any]],
    model_path: str,
    tensor_parallel_size: int = 1,
    gpu_memory_utilization: float = 0.8,
    temperature: float = 0.6,
    max_tokens: int = 8192,
    max_model_len: int = 70000,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """ä½¿ç”¨ LLM-as-Judge è¯„ä¼°é¢„æµ‹ç»“æœ
    
    Args:
        predictions_path: LLaMA-Factory ç”Ÿæˆçš„ predictions.jsonl è·¯å¾„
        eval_records: è¯„ä¼°æ•°æ®é›†è®°å½•åˆ—è¡¨ï¼ˆProblemRecord è½¬æ¢çš„å­—å…¸ï¼‰
        model_path: Judge æ¨¡å‹è·¯å¾„
        tensor_parallel_size: å¼ é‡å¹¶è¡Œå¤§å°
        gpu_memory_utilization: GPU å†…å­˜åˆ©ç”¨ç‡
        temperature: é‡‡æ ·æ¸©åº¦
        max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        max_model_len: æ¨¡å‹å…è®¸çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆtoken æ•°ï¼‰
    
    Returns:
        (è¯„ä¼°åçš„æ•°æ®, ç»Ÿè®¡ä¿¡æ¯å­—å…¸)
    """
    # 1. åŠ è½½é¢„æµ‹ç»“æœ
    predictions = load_jsonl(predictions_path)
    
    # 2. æ„å»º pid/id åˆ° record çš„æ˜ å°„ï¼ˆæ”¯æŒè´¨é‡æ£€æµ‹å’Œæ­£å¼è¯„ä¼°ä¸¤ç§åœºæ™¯ï¼‰
    pid_to_record = {}
    for record in eval_records:
        # ä¼˜å…ˆä½¿ç”¨ pidï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ idï¼ˆè½¬ä¸ºå­—ç¬¦ä¸²ï¼‰
        key = record.get('pid') or str(record.get('id', ''))
        if key:
            pid_to_record[key] = record
    
    # 3. æ„å»ºè¯„ä¼°æ•°æ®æ ¼å¼
    eval_data = []
    for pred in predictions:
        # ä»é¢„æµ‹ä¸­è·å– pidï¼ˆæ”¯æŒä¸‰ç§æƒ…å†µï¼‰
        # æƒ…å†µ1ï¼šé¢„æµ‹ä¸­æœ‰ _metadata.original_pid å­—æ®µï¼ˆè´¨é‡æ£€æµ‹æ—¶ï¼‰
        # æƒ…å†µ2ï¼šé¢„æµ‹ä¸­ç›´æ¥æœ‰ pid å­—æ®µ
        # æƒ…å†µ3ï¼šé¢„æµ‹ä¸­æœ‰ id å­—æ®µï¼ˆæ­£å¼è¯„ä¼°æ—¶ï¼‰
        if '_metadata' in pred and 'original_pid' in pred['_metadata']:
            original_pid = pred['_metadata']['original_pid']
            attempt = pred['_metadata'].get('attempt', 1)
        elif 'pid' in pred:
            original_pid = pred['pid']
            attempt = 1
        elif 'id' in pred:
            original_pid = str(pred['id'])
            attempt = 1
        else:
            # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•ä» eval_records åŒ¹é…ï¼ˆæŒ‰é¡ºåºï¼‰
            if len(eval_data) < len(eval_records):
                original_pid = eval_records[len(eval_data)].get('pid', '')
                attempt = 1
            else:
                print(f"âš ï¸  Warning: Cannot determine pid for prediction {len(eval_data)}")
                continue
        
        # è·å–å¯¹åº”çš„ record
        if original_pid not in pid_to_record:
            print(f"âš ï¸  Warning: PID {original_pid} not found in eval_records")
            continue
        
        record = pid_to_record[original_pid]
        
        eval_item = {
            'problem': record.get('question', ''),
            'answer': record.get('answer', ''),
            'model_prediction': pred.get('predict', pred.get('model_prediction', '')),  # æ”¯æŒä¸¤ç§å­—æ®µå
            'id': original_pid,  # ğŸ”‘ ç»Ÿä¸€ä½¿ç”¨ 'id' å­—æ®µï¼Œä¸è¾“å…¥æ•°æ®ä¸€è‡´
            'attempt': attempt,  # ä¿ç•™ attempt ç¼–å·
            'category_id': record.get('category_id', 0),
            'category_name': record.get('category_name', 'Unknown'),
        }
        eval_data.append(eval_item)
    
    # 3. åˆå§‹åŒ– LLM Judge ä¹‹å‰æ¸…ç† GPU æ˜¾å­˜
    print(f"ğŸ¤– Initializing LLM Judge: {model_path}")
    print(f"ğŸ’¾ GPU Memory Utilization: {gpu_memory_utilization}")
    # import torch
    # # ç¦ç”¨ torch.compile ä»¥é¿å… Triton ç¼–è¯‘é”™è¯¯
    # torch._dynamo.config.disable = True
    # torch._dynamo.config.suppress_errors = True
    # print("âœ… Disabled torch.compile to avoid Triton compilation errors")
    
    # # å¼ºåˆ¶æ¸…ç† GPU æ˜¾å­˜
    # try:
    #     import gc
    #     if torch.cuda.is_available():
    #         torch.cuda.empty_cache()
    #         torch.cuda.ipc_collect()
    #         gc.collect()
    #         print("âœ… GPU memory cleared before loading Judge model")
    # except Exception as e:
    #     print(f"âš ï¸  Could not clear GPU memory: {e}")
    
    # # æ˜¾å¼è®¾ç½®è®¾å¤‡ä¸º CUDAï¼Œé¿å…è‡ªåŠ¨æ£€æµ‹å¤±è´¥
    # import os
    # if 'CUDA_VISIBLE_DEVICES' not in os.environ:
    #     print("âš ï¸  CUDA_VISIBLE_DEVICES not set, setting to '0'")
    #     os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    
    print(f'ğŸ› ï¸ Requested max_model_len: {max_model_len}', flush=True)

    llm = LLM(
        model=model_path,
        tensor_parallel_size=tensor_parallel_size,
        gpu_memory_utilization=gpu_memory_utilization,
        trust_remote_code=True,
        max_model_len=max_model_len,
        enforce_eager=False  # å¯ç”¨ CUDA Graph ä¼˜åŒ–
    )

    try:
        actual_len = None
        if hasattr(llm, 'llm_engine') and hasattr(llm.llm_engine, 'model_config'):
            actual_len = llm.llm_engine.model_config.max_model_len
        elif hasattr(llm, 'model_config'):
            actual_len = getattr(llm.model_config, 'max_model_len', None)

        if actual_len is not None:
            print(f'âœ… vLLM initialized with max_model_len={actual_len}', flush=True)
            if actual_len < max_model_len:
                print(f'âš ï¸  WARNING: effective max_model_len ({actual_len}) < requested ({max_model_len})', flush=True)
        else:
            print('âš ï¸  Could not verify vLLM max_model_len from engine config', flush=True)
    except Exception as verify_error:
        print(f'âš ï¸  Failed to verify max_model_len: {verify_error}', flush=True)

    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    sampling_params = SamplingParams(
        temperature=temperature,
        max_tokens=max_tokens,
    )
    
    # 4. æ‰¹é‡è¯„ä¼°
    print(f"ğŸ“Š Evaluating {len(eval_data)} predictions with LLM Judge...")
    
    # è¯„ä¼°å®Œæˆåï¼Œä¸è¿›è¡Œæ˜¾å¼æ¸…ç†
    # åŸå› ï¼šåœ¨å¤šGPUç¯å¢ƒä¸‹ï¼ŒvLLM çš„èµ„æºæ¸…ç†ï¼ˆç‰¹åˆ«æ˜¯ del llmï¼‰å¯èƒ½ä¼šå¡ä½
    # è§£å†³æ–¹æ¡ˆï¼šè®©è¿›ç¨‹è‡ªç„¶é€€å‡ºï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ¸…ç†æ‰€æœ‰èµ„æºï¼ˆGPUå†…å­˜ã€CUDAä¸Šä¸‹æ–‡ã€IPCç­‰ï¼‰
    # è¿™æ˜¯æœ€å®‰å…¨å¯é çš„æ–¹å¼ï¼Œé¿å…äº†å¤šGPUåŒæ­¥é—®é¢˜
    eval_data = batch_evaluate(eval_data, llm, sampling_params, tokenizer)
    
    # 5. ç»Ÿè®¡ç»“æœ
    total = len(eval_data)
    matched = sum(1 for item in eval_data if item.get('matched') == True)
    different = sum(1 for item in eval_data if item.get('matched') == False)
    unknown = sum(1 for item in eval_data if item.get('matched') is None)
    
    stats = {
        'total': total,
        'correct': matched,
        'wrong': different,
        'unknown': unknown,
        'accuracy': matched / total if total > 0 else 0.0,
    }
    
    print(f"âœ… Evaluation complete: {matched}/{total} correct ({stats['accuracy']*100:.2f}%)")
    if unknown > 0:
        print(f"âš ï¸  {unknown} predictions could not be judged")
    
    return eval_data, stats


if __name__ == "__main__":
    main()