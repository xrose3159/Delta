#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„ LLM Judge è¿è¡Œè„šæœ¬

æ­¤è„šæœ¬ç”¨äºåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œ LLM-as-Judgeï¼Œä»¥é¿å…ä¸ä¸»è¿›ç¨‹çš„ CUDA ä¸Šä¸‹æ–‡å†²çªã€‚
æ³¨æ„ï¼šå¿…é¡»ä½¿ç”¨ if __name__ == '__main__' ä¿æŠ¤ï¼Œå› ä¸º vLLM ä¼šä½¿ç”¨ spawn æ¨¡å¼åˆ›å»ºå­è¿›ç¨‹ã€‚

ä½¿ç”¨æ–¹å¼:
    python run_llm_judge.py <eval_records_file> <predictions_path> <output_file> <judge_config_json>

å‚æ•°:
    eval_records_file: åŒ…å«è¯„ä¼°è®°å½•çš„ JSON æ–‡ä»¶è·¯å¾„
    predictions_path: æ¨¡å‹é¢„æµ‹ç»“æœæ–‡ä»¶è·¯å¾„
    output_file: è¾“å‡ºç»“æœçš„ JSON æ–‡ä»¶è·¯å¾„
    judge_config_json: Judge é…ç½®å‚æ•°çš„ JSON å­—ç¬¦ä¸²
"""

import json
import sys
import argparse
from pathlib import Path

# ç¡®ä¿å¯ä»¥å¯¼å…¥é¡¹ç›®æ¨¡å—
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from adaptive_training.processors.llmasjudge import evaluate_predictions_with_judge


def main():
    parser = argparse.ArgumentParser(description='Run LLM Judge evaluation')
    parser.add_argument('eval_records_file', type=str, help='Path to eval records JSON file')
    parser.add_argument('predictions_path', type=str, help='Path to predictions file')
    parser.add_argument('output_file', type=str, help='Path to output JSON file')
    parser.add_argument('judge_config', type=str, help='Judge configuration as JSON string')
    
    args = parser.parse_args()
    
    try:
        print("ğŸš€ LLM Judge subprocess started", flush=True)
        print(f"ğŸ“ Eval records: {args.eval_records_file}", flush=True)
        print(f"ğŸ“ Predictions: {args.predictions_path}", flush=True)
        print(f"ğŸ“ Output: {args.output_file}", flush=True)

        # è¯»å–è¯„ä¼°è®°å½•
        print("ğŸ“– Reading eval records...", flush=True)
        with open(args.eval_records_file, 'r') as f:
            eval_records = json.load(f)
        print(f"âœ… Loaded {len(eval_records)} eval records", flush=True)
        
        # è§£æ Judge é…ç½®
        print("âš™ï¸  Parsing judge config...", flush=True)
        judge_config = json.loads(args.judge_config)
        print(f"ğŸ“Š Judge config: {judge_config}", flush=True)

        configured_max_len = judge_config.get('max_model_len', 70000)
        
        # è¿è¡Œ Judge
        print("ğŸ”¥ Starting LLM Judge evaluation...", flush=True)
        eval_data, judge_stats = evaluate_predictions_with_judge(
            predictions_path=args.predictions_path,
            eval_records=eval_records,
            model_path=judge_config['model_path'],
            tensor_parallel_size=judge_config['tensor_parallel_size'],
            gpu_memory_utilization=judge_config['gpu_memory_utilization'],
            temperature=judge_config['temperature'],
            max_tokens=judge_config['max_tokens'],
            max_model_len=configured_max_len,
        )
        print("âœ… LLM Judge evaluation completed", flush=True)
        
        # ä¿å­˜ç»“æœ
        print(f"ğŸ’¾ Saving results to {args.output_file}...", flush=True)
        with open(args.output_file, 'w') as f:
            json.dump({'eval_data': eval_data, 'judge_stats': judge_stats}, f, ensure_ascii=False)
        
        print("âœ… LLM Judge completed successfully", flush=True)
        
    except Exception as e:
        print(f"âŒ Fatal error in LLM Judge subprocess: {e}", flush=True)
        import traceback
        print("ğŸ“‹ Full traceback:", flush=True)
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()


