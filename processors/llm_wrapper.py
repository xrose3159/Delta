"""
LLM Generator å’Œ Judge çš„åŒ…è£…ç±»
ç”¨äºå°†è„šæœ¬è°ƒç”¨å°è£…æˆç±»æ¥å£
"""

import json
import logging
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import os


logger = logging.getLogger(__name__)


class LLMasGenerator:
    """LLM Generator åŒ…è£…ç±»"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def generate_predictions(
        self,
        test_file: Path,
        output_file: Path,
        model_path: Optional[str] = None,
        max_tokens: Optional[int] = None,
        disable_thinking: bool = False,
        is_error_analysis: bool = False,
        is_correction: bool = False,
        is_validation: bool = False
    ) -> bool:
        """
        ä½¿ç”¨ vLLM ç”Ÿæˆé¢„æµ‹
        
        Args:
            test_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ eval_max_tokensï¼‰
            disable_thinking: æ˜¯å¦ç¦ç”¨æ€è€ƒæ¨¡å¼
            is_error_analysis: æ˜¯å¦ä¸ºé”™è¯¯åˆ†æä»»åŠ¡ï¼ˆä½¿ç”¨ä¸“é—¨çš„ system promptï¼‰
            is_correction: æ˜¯å¦ä¸ºç”Ÿæˆ corrected CoT ä»»åŠ¡ï¼ˆä½¿ç”¨ä¸“é—¨çš„ system promptï¼‰
            is_validation: æ˜¯å¦ä¸ºç­”æ¡ˆéªŒè¯ä»»åŠ¡ï¼ˆä½¿ç”¨ä¸é”™è¯¯åˆ†æç›¸åŒçš„é…ç½®ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if model_path is None:
            model_path = self.config.model_path
        if max_tokens is None:
            max_tokens = self.config.eval_max_tokens
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©é…ç½®å‚æ•°
        if is_error_analysis or is_validation:
            # é”™è¯¯åˆ†æå’Œç­”æ¡ˆéªŒè¯ä½¿ç”¨ç›¸åŒçš„ä¸“ç”¨é…ç½®ï¼ˆéƒ½æ˜¯ä½¿ç”¨ Qwen3-VL-30Bï¼‰
            tensor_parallel_size = self.config.error_analyzer_tensor_parallel_size
            gpu_memory_utilization = self.config.error_analyzer_gpu_memory_utilization
            max_model_len = self.config.error_analyzer_max_model_len
            if max_tokens is None or max_tokens == self.config.eval_max_tokens:
                max_tokens = self.config.error_analyzer_max_tokens
        else:
            # é»˜è®¤ä½¿ç”¨ LLM Generator é…ç½®
            tensor_parallel_size = self.config.llm_generator_tensor_parallel_size
            gpu_memory_utilization = self.config.llm_generator_gpu_memory_utilization
            max_model_len = self.config.llm_generator_max_model_len
        
        script_path = Path(__file__).parent / "llmasgenerator.py"
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Apptainer
        use_apptainer = os.getenv("USE_APPTAINER_FOR_VLLM", "").lower() in ("true", "1", "yes")
        apptainer_image = os.getenv("APPTAINER_IMAGE", "")
        
        if use_apptainer and apptainer_image:
            self.logger.info("Using Apptainer for vLLM generation")
            
            # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆç”¨äº PYTHONPATHï¼‰
            project_root = Path(__file__).parent.parent.parent
            
            # ä½¿ç”¨ç”¨æˆ·ä¸“å±çš„ Triton ç¼“å­˜ç›®å½•ï¼Œé¿å…å¤šè¿›ç¨‹/å¤šç”¨æˆ·æƒé™å†²çª
            triton_cache_dir = f"/tmp/triton_cache_{os.getenv('USER', 'default')}_{os.getpid()}"
            
            cmd = [
                "apptainer", "exec", "--nv",
                "--cleanenv",
                "--bind", "/share:/share,/mnt:/mnt",
                # HuggingFace ç¦»çº¿æ¨¡å¼ï¼ˆå¿…é¡»è®¾ç½®ï¼Œé¿å…è·¯å¾„éªŒè¯é”™è¯¯ï¼‰
                "--env", "HF_HUB_OFFLINE=1",
                "--env", "TRANSFORMERS_OFFLINE=1",
                "--env", "HF_DATASETS_OFFLINE=1",
                # Python è¾“å‡ºä¸ç¼“å†²ï¼ˆå…³é”®ä¿®å¤ï¼‰
                "--env", "PYTHONUNBUFFERED=1",
                # CUDA è®¾å¤‡é…ç½®ï¼ˆå…³é”®ï¼šä¼ é€’ Slurm åˆ†é…çš„ GPUï¼‰
                "--env", f"CUDA_VISIBLE_DEVICES={os.getenv('CUDA_VISIBLE_DEVICES', '')}",
                # vLLM æ—¥å¿—æ§åˆ¶ï¼šåªæ˜¾ç¤º WARNING åŠä»¥ä¸Šçº§åˆ«
                "--env", "VLLM_LOGGING_LEVEL=WARNING",
                # Triton å’Œç¼–è¯‘ç¯å¢ƒï¼šä½¿ç”¨ç”¨æˆ·+è¿›ç¨‹ä¸“å±ç¼“å­˜ï¼Œé¿å…æƒé™å†²çª
                "--env", f"TRITON_CACHE_DIR={triton_cache_dir}",
                "--env", "PATH=/opt/py312/bin:/usr/local/cuda/bin:/usr/bin:/bin",
                # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ä½¿ç”¨å®¹å™¨å†…çš„ GCCï¼Œé¿å…å®¿ä¸»æœº GCC ä¾èµ–é—®é¢˜
                "--env", "CC=/usr/bin/gcc",              # C ç¼–è¯‘å™¨
                "--env", "CXX=/usr/bin/g++",             # C++ ç¼–è¯‘å™¨
                "--env", "CUDAHOSTCXX=/usr/bin/g++",     # CUDA ä¸»æœºç¼–è¯‘å™¨ï¼ˆæœ€å…³é”®ï¼ï¼‰
                "--env", "LD_LIBRARY_PATH=/usr/local/cuda/lib64:/.singularity.d/libs",
                "--env", "CUDA_HOME=/usr/local/cuda",
                "--env", f"PYTHONPATH={project_root}",  # æ·»åŠ  PYTHONPATH
                apptainer_image,
                "python", "-u", str(script_path),  # -u å‚æ•°å¼ºåˆ¶ä¸ç¼“å†²è¾“å‡º
                "--model-path", model_path,
                "--input", str(test_file),
                "--output", str(output_file),
                "--question-key", "problem",      # ä¸åŸé¡¹ç›®ä¸€è‡´
                "--answer-key", "predict",        # ä¸åŸé¡¹ç›®ä¸€è‡´ï¼šè¾“å‡ºåˆ° predict å­—æ®µ
                "--image-key", "image_path",      # ä¸åŸé¡¹ç›®ä¸€è‡´
                "--temperature", str(self.config.eval_temperature),
                "--top-p", str(self.config.eval_top_p),
                "--frequency-penalty", str(self.config.eval_frequency_penalty),
                "--max-tokens", str(max_tokens),
                "--tensor-parallel-size", str(tensor_parallel_size),
                "--gpu-memory-utilization", str(gpu_memory_utilization),
                "--max-model-len", str(max_model_len),
            ]
            # æ·»åŠ å¯é€‰å‚æ•°
            if disable_thinking:
                cmd.append("--disable-thinking")
            if is_error_analysis:
                cmd.append("--is-error-analysis")
            if is_correction:
                cmd.append("--is-correction")

        else:
            self.logger.info("Using local Python for vLLM generation")
            cmd = [
                "python", "-u", str(script_path),  # -u å‚æ•°å¼ºåˆ¶ä¸ç¼“å†²è¾“å‡º
                "--model-path", model_path,
                "--input", str(test_file),
                "--output", str(output_file),
                "--question-key", "problem",      # ä¸åŸé¡¹ç›®ä¸€è‡´
                "--answer-key", "predict",        # ä¸åŸé¡¹ç›®ä¸€è‡´ï¼šè¾“å‡ºåˆ° predict å­—æ®µ
                "--image-key", "image_path",      # ä¸åŸé¡¹ç›®ä¸€è‡´
                "--temperature", str(self.config.eval_temperature),
                "--top-p", str(self.config.eval_top_p),
                "--frequency-penalty", str(self.config.eval_frequency_penalty),
                "--max-tokens", str(max_tokens),
                "--tensor-parallel-size", str(tensor_parallel_size),
                "--gpu-memory-utilization", str(gpu_memory_utilization),
                "--max-model-len", str(max_model_len),
            ]
            # æ·»åŠ å¯é€‰å‚æ•°
            if disable_thinking:
                cmd.append("--disable-thinking")
            if is_error_analysis:
                cmd.append("--is-error-analysis")
            if is_correction:
                cmd.append("--is-correction")
        
        self.logger.info(f"Running LLM Generator: {' '.join(map(str, cmd[:6]))}...")
        
        try:
            # ä½¿ç”¨ Popen å®ç°å®æ—¶è¾“å‡ºï¼ˆä¸ Judge ä¸€è‡´ï¼‰
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1
            )
            
            # å®æ—¶è¾“å‡ºæ—¥å¿—ï¼ˆè¿‡æ»¤æ‰è¯¦ç»†çš„vLLMå†…éƒ¨æ—¥å¿—ï¼‰
            self.logger.info("=== LLM Generator subprocess output ===")
            
            # éœ€è¦æ‰“å°çš„å…³é”®è¯ï¼ˆé”™è¯¯ã€è­¦å‘Šã€é‡è¦è¿›åº¦ï¼‰
            important_keywords = [
                'ERROR', 'WARNING', 'CRITICAL', 'Failed', 'Exception',
                'Traceback', 'âŒ', 'âœ…', 'completed', 'Starting', 'Finished'
            ]
            
            # éœ€è¦è·³è¿‡çš„è¯¦ç»†æ—¥å¿—æ¨¡å¼
            skip_patterns = [
                'Found nccl', 'is using nccl', 'rank', 'TP rank', 'DP rank',
                'Loading safetensors checkpoint shards:', 'Completed |',
                'Using FlashInfer', 'Using Flash Attention', 'Starting to load model',
                'Capturing CUDA graphs',  # CUDA graphs æ•è·è¿›åº¦
                'it/s]',  # è¿›åº¦æ¡ï¼ˆé€šç”¨ï¼‰
                '% Completed',  # ç™¾åˆ†æ¯”è¿›åº¦
                'Model loading took',  # æ¨¡å‹åŠ è½½æ—¶é—´
                'Loading weights took',  # æƒé‡åŠ è½½æ—¶é—´
                'torch.compile',  # torch.compile ç›¸å…³æ—¥å¿—
                'Dynamo bytecode',  # Dynamo ç¼–è¯‘æ—¥å¿—
                'FutureWarning',  # Python è­¦å‘Š
                'pynvml package is deprecated',  # pynvml è­¦å‘Š
                'TORCH_CUDA_ARCH_LIST',  # CUDA æ¶æ„åˆ—è¡¨è­¦å‘Š
                'SymmMemCommunicator',  # å¯¹ç§°å†…å­˜é€šä¿¡å™¨è­¦å‘Š
                'Reducing Torch parallelism',  # Torch å¹¶è¡Œåº¦è°ƒæ•´
                'OMP_NUM_THREADS',  # OpenMP çº¿ç¨‹æ•°
                'torch/utils/cpp_extension.py',  # C++ æ‰©å±•ç¼–è¯‘è­¦å‘Š
                '[1;36m(EngineCore_DP0',  # åªæœ‰è¿›ç¨‹ä¿¡æ¯çš„ç©ºè¡Œ
                '[1;36m(Worker_TP'  # åªæœ‰ Worker ä¿¡æ¯çš„ç©ºè¡Œ
            ]
            
            import re  # ç”¨äºç§»é™¤ ANSI é¢œè‰²ä»£ç 
            for line in process.stdout:
                line_stripped = line.rstrip()
                
                # ç§»é™¤ ANSI é¢œè‰²ä»£ç åæ£€æŸ¥æ˜¯å¦ä¸ºç©º
                line_clean = re.sub(r'\x1b\[[0-9;]*m', '', line_stripped)  # ç§»é™¤ ANSI ä»£ç 
                if not line_clean.strip():  # å¦‚æœç§»é™¤é¢œè‰²ä»£ç åä¸ºç©ºï¼Œè·³è¿‡
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦å…³é”®è¯
                should_print = any(keyword in line_stripped for keyword in important_keywords)
                # æˆ–è€…ä¸åŒ…å«è·³è¿‡æ¨¡å¼
                should_skip = any(pattern in line_stripped for pattern in skip_patterns)
                
                if should_print or not should_skip:
                    self.logger.info(f"  [vLLM] {line_stripped}")
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            return_code = process.wait()
            self.logger.info(f"=== LLM Generator subprocess finished with code {return_code} ===")
            
            if return_code != 0:
                self.logger.error(f"âŒ LLM Generator failed with exit code {return_code}")
                return False
            
            self.logger.info("âœ… LLM Generator completed successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ LLM Generator failed: {e}")
            return False


class LLMasJudge:
    """LLM Judge åŒ…è£…ç±»"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def evaluate(
        self,
        predictions_file: Path,
        eval_records_file: Path,
        output_file: Path
    ) -> bool:
        """
        ä½¿ç”¨ LLM Judge è¯„ä¼°é¢„æµ‹
        
        Args:
            predictions_file: é¢„æµ‹æ–‡ä»¶è·¯å¾„
            eval_records_file: è¯„ä¼°è®°å½•æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        script_path = Path(__file__).parent / "run_llm_judge.py"
        
        # å‡†å¤‡ judge é…ç½®ï¼ˆä¸åŸé¡¹ç›®ä¸€è‡´ï¼‰
        judge_config = {
            "model_path": self.config.judge_model_path,
            "tensor_parallel_size": self.config.judge_tensor_parallel_size,
            "gpu_memory_utilization": self.config.judge_gpu_memory_utilization,
            "temperature": self.config.judge_temperature,
            "max_tokens": self.config.judge_max_tokens,
            "max_model_len": self.config.judge_max_model_len,  # æ·»åŠ  max_model_len é…ç½®
        }
        judge_config_json = json.dumps(judge_config)
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Apptainer å®¹å™¨
        use_apptainer = os.getenv("USE_APPTAINER_FOR_VLLM", "").lower() in ("true", "1", "yes")
        apptainer_image = os.getenv("APPTAINER_IMAGE", "")
        
        if use_apptainer and apptainer_image:
            self.logger.info(f"Using Apptainer for Judge: {apptainer_image}")
            # ä½¿ç”¨ Apptainer å®¹å™¨è¿è¡Œ Judge
            cmd = [
                "apptainer", "exec", "--nv",
                "--bind", "/mnt:/mnt",
                "--bind", "/tmp:/tmp",
                "--env", f"CUDA_VISIBLE_DEVICES={os.getenv('CUDA_VISIBLE_DEVICES', '')}",
                "--env", "PYTHONPATH=/mnt/petrelfs/shangxiaoran",
                "--env", f"HF_HOME={os.getenv('HF_HOME', '/tmp/huggingface')}",
                "--env", "LD_LIBRARY_PATH=/usr/local/cuda/lib64:/.singularity.d/libs",
                # vLLM æ—¥å¿—æ§åˆ¶ï¼šåªæ˜¾ç¤º WARNING åŠä»¥ä¸Šçº§åˆ«
                "--env", "VLLM_LOGGING_LEVEL=WARNING",
                # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ä½¿ç”¨å®¹å™¨å†…çš„ GCCï¼Œé¿å…å®¿ä¸»æœº GCC ä¾èµ–é—®é¢˜
                "--env", "CC=/usr/bin/gcc",              # C ç¼–è¯‘å™¨
                "--env", "CXX=/usr/bin/g++",             # C++ ç¼–è¯‘å™¨
                "--env", "CUDAHOSTCXX=/usr/bin/g++",     # CUDA ä¸»æœºç¼–è¯‘å™¨ï¼ˆæœ€å…³é”®ï¼ï¼‰
                "--env", "CUDA_HOME=/usr/local/cuda",
                "--pwd", str(Path.cwd()),
                apptainer_image,
                "python3", str(script_path),
                str(eval_records_file),
                str(predictions_file),
                str(output_file),
                judge_config_json
            ]
        else:
            # ç›´æ¥ä½¿ç”¨ Python è¿è¡Œï¼ˆå®¿ä¸»æœºç¯å¢ƒï¼‰
            self.logger.info("Using host Python for Judge (Apptainer disabled)")
            cmd = [
                sys.executable, str(script_path),
                str(eval_records_file),
                str(predictions_file),
                str(output_file),
                judge_config_json  # JSON å­—ç¬¦ä¸²ï¼Œä¸æ˜¯æ–‡ä»¶
            ]
        
        self.logger.info(f"Running LLM Judge: {' '.join(map(str, cmd[:4]))}...")
        
        try:
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1
            )
            
            # å®æ—¶è¾“å‡ºæ—¥å¿—ï¼ˆè¿‡æ»¤æ‰è¯¦ç»†çš„vLLMå†…éƒ¨æ—¥å¿—ï¼‰
            self.logger.info("=== LLM Judge subprocess output ===")
            
            # éœ€è¦æ‰“å°çš„å…³é”®è¯ï¼ˆé”™è¯¯ã€è­¦å‘Šã€é‡è¦è¿›åº¦ï¼‰
            important_keywords = [
                'ERROR', 'WARNING', 'CRITICAL', 'Failed', 'Exception',
                'Traceback', 'âŒ', 'âœ…', 'completed', 'Starting', 'Finished',
                'Evaluating', 'Total problems'
            ]
            
            # éœ€è¦è·³è¿‡çš„è¯¦ç»†æ—¥å¿—æ¨¡å¼
            skip_patterns = [
                'Found nccl', 'is using nccl', 'rank', 'TP rank', 'DP rank',
                'Loading safetensors checkpoint shards:', 'Completed |',
                'Using FlashInfer', 'Using Flash Attention', 'Starting to load model',
                'Loading model from scratch', 'Using cache directory',
                'Capturing CUDA graphs',  # CUDA graphs æ•è·è¿›åº¦
                'it/s]',  # è¿›åº¦æ¡ï¼ˆé€šç”¨ï¼‰
                '% Completed',  # ç™¾åˆ†æ¯”è¿›åº¦
                'Model loading took',  # æ¨¡å‹åŠ è½½æ—¶é—´
                'Loading weights took',  # æƒé‡åŠ è½½æ—¶é—´
                'torch.compile',  # torch.compile ç›¸å…³æ—¥å¿—
                'Dynamo bytecode',  # Dynamo ç¼–è¯‘æ—¥å¿—
                'FutureWarning',  # Python è­¦å‘Š
                'pynvml package is deprecated',  # pynvml è­¦å‘Š
                'TORCH_CUDA_ARCH_LIST',  # CUDA æ¶æ„åˆ—è¡¨è­¦å‘Š
                'SymmMemCommunicator',  # å¯¹ç§°å†…å­˜é€šä¿¡å™¨è­¦å‘Š
                'Reducing Torch parallelism',  # Torch å¹¶è¡Œåº¦è°ƒæ•´
                'OMP_NUM_THREADS',  # OpenMP çº¿ç¨‹æ•°
                'torch/utils/cpp_extension.py',  # C++ æ‰©å±•ç¼–è¯‘è­¦å‘Š
                '[1;36m(EngineCore_DP0',  # åªæœ‰è¿›ç¨‹ä¿¡æ¯çš„ç©ºè¡Œ
                '[1;36m(Worker_TP'  # åªæœ‰ Worker ä¿¡æ¯çš„ç©ºè¡Œ
            ]
            
            # ğŸ”‘ å…³é”®ä¿®å¤ï¼šæ·»åŠ è¶…æ—¶æœºåˆ¶å’Œéé˜»å¡è¯»å–
            import select
            import time
            import re  # ç”¨äºç§»é™¤ ANSI é¢œè‰²ä»£ç 
            
            # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆ20åˆ†é’Ÿæ— è¾“å‡ºåˆ™è®¤ä¸ºå¡ä½ï¼‰
            # æ³¨æ„ï¼švLLM èµ„æºæ¸…ç†ï¼ˆç‰¹åˆ«æ˜¯å¤šGPUç¯å¢ƒä¸‹ï¼‰å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
            timeout_seconds = 1200
            last_output_time = time.time()
            
            while True:
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦ç»“æŸ
                if process.poll() is not None:
                    # è¿›ç¨‹å·²ç»“æŸï¼Œè¯»å–å‰©ä½™è¾“å‡º
                    for line in process.stdout:
                        line_stripped = line.rstrip()
                        # ç§»é™¤ ANSI é¢œè‰²ä»£ç åæ£€æŸ¥æ˜¯å¦ä¸ºç©º
                        line_clean = re.sub(r'\x1b\[[0-9;]*m', '', line_stripped)
                        if not line_clean.strip():
                            continue
                        should_print = any(keyword in line_stripped for keyword in important_keywords)
                        should_skip = any(pattern in line_stripped for pattern in skip_patterns)
                        if should_print or not should_skip:
                            self.logger.info(f"  [Judge] {line_stripped}")
                    break
                
                # éé˜»å¡è¯»å–ï¼ˆä½¿ç”¨ selectï¼Œä»… Unixï¼‰
                # å¯¹äº Windowsï¼Œéœ€è¦ä½¿ç”¨ä¸åŒçš„æ–¹æ³•
                try:
                    if sys.platform != 'win32':
                        # Unix ç³»ç»Ÿï¼šä½¿ç”¨ select
                        readable, _, _ = select.select([process.stdout], [], [], 1.0)
                        if readable:
                            line = process.stdout.readline()
                            if line:
                                line_stripped = line.rstrip()
                                # ç§»é™¤ ANSI é¢œè‰²ä»£ç åæ£€æŸ¥æ˜¯å¦ä¸ºç©º
                                line_clean = re.sub(r'\x1b\[[0-9;]*m', '', line_stripped)
                                if line_clean.strip():  # åªå¤„ç†éç©ºè¡Œ
                                    should_print = any(keyword in line_stripped for keyword in important_keywords)
                                    should_skip = any(pattern in line_stripped for pattern in skip_patterns)
                                    if should_print or not should_skip:
                                        self.logger.info(f"  [Judge] {line_stripped}")
                                last_output_time = time.time()
                    else:
                        # Windows ç³»ç»Ÿï¼šä½¿ç”¨é˜»å¡è¯»å–ï¼Œä½†æ£€æŸ¥è¶…æ—¶
                        line = process.stdout.readline()
                        if line:
                            line_stripped = line.rstrip()
                            # ç§»é™¤ ANSI é¢œè‰²ä»£ç åæ£€æŸ¥æ˜¯å¦ä¸ºç©º
                            line_clean = re.sub(r'\x1b\[[0-9;]*m', '', line_stripped)
                            if line_clean.strip():  # åªå¤„ç†éç©ºè¡Œ
                                should_print = any(keyword in line_stripped for keyword in important_keywords)
                                should_skip = any(pattern in line_stripped for pattern in skip_patterns)
                                if should_print or not should_skip:
                                    self.logger.info(f"  [Judge] {line_stripped}")
                            last_output_time = time.time()
                except Exception as read_error:
                    self.logger.warning(f"Error reading subprocess output: {read_error}")
                
                # æ£€æŸ¥è¶…æ—¶
                if time.time() - last_output_time > timeout_seconds:
                    self.logger.error(f"âŒ Subprocess timeout: no output for {timeout_seconds} seconds")
                    process.kill()
                    process.wait()
                    return False
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            return_code = process.wait()
            self.logger.info(f"=== LLM Judge subprocess finished with code {return_code} ===")
            
            if return_code != 0:
                self.logger.error(f"âŒ LLM Judge failed with exit code {return_code}")
                return False
            
            # éªŒè¯è¾“å‡ºæ–‡ä»¶
            if not output_file.exists():
                self.logger.error(f"âŒ Output file not found: {output_file}")
                return False
            
            self.logger.info("âœ… LLM Judge completed successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ LLM Judge failed: {e}")
            # ç¡®ä¿æ¸…ç†å­è¿›ç¨‹
            try:
                if 'process' in locals() and process.poll() is None:
                    process.kill()
                    process.wait()
            except:
                pass
            return False


